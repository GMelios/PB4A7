[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PB4A7 - Quantitative Applications for Behavioural Science",
    "section": "",
    "text": "📑 Course Brief\nDescription: In this course, students will immerse themselves in the world of causal inference methodologies, a cornerstone in behavioural science research. The curriculum guides learners through the essential processes of cleaning, analysing, and visualising secondary data, equipping them with the skills to conduct robust and insightful research. Through practical examples, students will learn to adeptly navigate various research designs, and develop the proficiency to communicate their findings effectively, fostering a deeper understanding and application of best practices in behavioural science.\nFocus: Understand the fundamentals of causal inference and its applications in Behavioural Science. Master statistical tools used by psychologists, political scientists, and economists. Recognize and address contemporary issues in behavioural science.\nDownload the full syllabus here\n🛠️ Requirements: For students who have no prior experience with statistics and/or STATA, the completion of the following Digital Skills class is highly recommended:\nIntroduction to STATA\n\n\n🎯 Learning Objectives\n\nHow to distinguish Causation from Correlation\nHow different research designs work\nHow to apply different research designs on Stata\nHow to effectively visualise your findings\n\n\n  Enter the class"
  },
  {
    "objectID": "2023/weeks/week01/slides.html",
    "href": "2023/weeks/week01/slides.html",
    "title": "🗓️ Week 01 Introduction",
    "section": "",
    "text": "Prof. Octopian Professor of Octopus Issues Head of the Python vs R Debate Society\n\n\n\n\nPhD in Octopi Engineering\nBackground: Engineering of Deep Ocean constructions\nFormer Lead Data Scientist\n\ndeep ocean  engineering  octopus\n\n\n\n\n\n\n\n\n\n\nOctopnuian Arveda  PhD Candidate at OctopusLab, OUN  MSc in Octopus Transportation 📧 \n\n\n\n\n\n\nOctopnuian Arveda  PhD Candidate at OctopusLab, UOS  MSc in Octopus Transportation 📧"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#your-lecturer",
    "href": "2023/weeks/week01/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#teaching-assistants",
    "href": "2023/weeks/week01/slides.html#teaching-assistants",
    "title": "🗓️ Week 01 Introduction",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\n\n\n\n\n\nOctopnuian Arveda  PhD Candidate at OctopusLab, OUN  MSc in Octopus Transportation 📧 \n\n\n\n\n\n\nOctopnuian Arveda  PhD Candidate at OctopusLab, UOS  MSc in Octopus Transportation 📧 \n\n\n\n\n\n\n\nMY_COURSE_CODE – MY_COURSE_NAME"
  },
  {
    "objectID": "2023/index.html",
    "href": "2023/index.html",
    "title": "PB4A7 - Quantitative Applications for Behavioural Science",
    "section": "",
    "text": "📢 Announcements\n\n\n\n\n\n\nJoin the Class on Discord (18/09/2023)\n\n\n\n\n\n🧑🏻‍🏫 Our Team\n\nCourse ConvenorTeaching Staff\n\n\n\nDr George Melios  Research Fellow  London School of Economics and Political Science 📧 g.melios at lse dot ac dot uk\nOffice Hours:\n\nWhen: Wednesdays 10:00-12:00\nWhere: CON 5.19\nHow to book: Student Hub\n\n\n\n\nLazaros Chatzilazarou  PhD Candidate at City University  📧 L.A.Chatzilazarou at lse dot ac dot uk\nHelp Sessions:\n\nWhen: Wednesdays 16:00-17:00\nWhere: CBG.G.01\n\n\n\n\n\n\n📍 Lecture\nTuesdays 09:00-10:00 at MAR.1.04\n\n\n💻 Seminars\n\nGroup 01\n\n📆 Tuesdays\n⌚ 11:00 - 12:00\n📍 FAW.4.02\n\n\n\n\nGroup 02\n\n📆 Tuesdays\n⌚ 12:00 - 13:00\n📍 FAW.4.02\n\n\n\nGroup 03\n\n📆 Tuesdays\n⌚ 13:00 - 14:00\n📍 FAW.4.02"
  },
  {
    "objectID": "2023/weeks/week01/page.html",
    "href": "2023/weeks/week01/page.html",
    "title": "🗓️ Week 01 - Introduction",
    "section": "",
    "text": "This session will introduce you to the concept of applied quantitative research in general, give a brief outline of the course, and address organizational issues. We will then have a short followup to themes discussed during the presessional."
  },
  {
    "objectID": "2023/weeks/week01/page.html#lecture-slides",
    "href": "2023/weeks/week01/page.html#lecture-slides",
    "title": "🗓️ Week 01 - Introduction",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week01/page.html#recommended-reading",
    "href": "2023/weeks/week01/page.html#recommended-reading",
    "title": "🗓️ Week 01 - Introduction",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nChapters 1 & 2 from The Effect book\nInteresting reading: The Credibility Revolution in Empirical Economics: How Better Research Design Is Taking the Con out of Econometrics"
  },
  {
    "objectID": "2023/weeks/week01/page.html#communication",
    "href": "2023/weeks/week01/page.html#communication",
    "title": "🗓️ Week 01 - Introduction",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-is-this-class",
    "href": "2023/weeks/week01/slides.html#what-is-this-class",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What is this class",
    "text": "What is this class\n\nIt’s a research design course on quasi-experimental methods\nLet’s break it down:\n\nResearch Design -&gt; How you transform an idea / question about the world to applied research\nQuasi-Experiments -&gt; Not by designing new experiments with random assignment. (how? We will see over the next 11 weeks)"
  },
  {
    "objectID": "2023/weeks/week09/slides.html",
    "href": "2023/weeks/week09/slides.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#your-lecturer",
    "href": "2023/weeks/week09/slides.html#your-lecturer",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#what-is-this-class",
    "href": "2023/weeks/week09/slides.html#what-is-this-class",
    "title": "Instrumental Variables",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be\n\n\n\nSo what is econometrics?\nEconometrics focuses on the study of observational data\nObservational data are measurements of things that the researcher does not control\nGiven that we are working with observational data, we still want to understand the causes of things\nThe world is what it is, we are only here to study it"
  },
  {
    "objectID": "2023/weeks/week08/slides.html",
    "href": "2023/weeks/week08/slides.html",
    "title": "🗓️ Week 8 Regresssion Discontinuity Designs",
    "section": "",
    "text": "Check-in\n\nWe’re thinking through ways that we can identify the effect of interest without having to control for everything\nOne way is by focusing on within variation - if all the endogeneity can be controlled for or only varies between-individuals, we can just focus on within variation to identify it\nPro: control for a bunch of stuff\nCon: washes out a lot of variation! Result can be noisier if there’s not much within-variation to work with\nAlso, this requires no endogenous variation over time\nThat might be a tricky assumption! Often there are plenty of back doors that shift over time\n\n\n\n\nRegression Discontinuity\n\nToday we are going to talk about Regression discontinuity design (RDD)\nRDD is currently the darling of the econometric world for estimating causal effects without running an experiment\nIt doesn’t apply everywhere, but when it does, it’s very easy to buy the identification assumptions\nNot that it doesn’t have its own issues, of course, but it’s pretty good!\n\n\n\n\nRegression Discontinuity\nThe basic idea is this:\n\nWe look for a treatment that is assigned on the basis of being above/below a cutoff value of a continuous variable\nFor example, if you get above a certain test score they let you into a “gifted and talented” program\nOr if you are just on one side of a time zone line, your day starts one hour earlier/later\nOr if a candidate gets 50.1% of the vote they’re in, 40.9% and they’re out\nOr if you’re 65 years old you get Medicaid, if you’re 64.99 years old you don’t\n\nWe call these continuous variables “Running variables” because we run along them until we hit the cutoff\n\n\n\nRegression Discontinuity\n\nBut wait, hold on, if treatment is driven by running variables, won’t we have a back door going through those very same running variables?? Yes!\nAnd we can’t just control for RunningVar because that’s where all the variation in treatment comes from. Uh oh!\n\n\n\n\n\n\n\n\n\nRegression Discontinuity\n\nThe key here is realizing that the running variable affects treatment only when you go across the cutoff\nSo really the diagram looks like this!\n\n\n\n\n\n\n\n\n\nRegression Discontinuity\n\nSo what does this mean?\nIf we can control for the running variable everywhere except the cutoff, then…\nWe will be controlling for the running variable, closing that back door\nBut leaving variation at the cutoff open, allowing for variation in treatment\nWe focus on just the variation around the treatment, narrowing the range of the running variable we use so sharply that it’s basically controlled for. Then the effect of cutoff on treatment is like an experiment!\n\n\n\n\nRegression Discontinuity\n\nBasically, the idea is that right around the cutoff, treatment is randomly assigned\nIf you have a test score of 89.9 (not high enough for gifted-and-talented), you’re basically the same as someone who has a test score of 90.0 (just barely high enough)\nSo if we just focus around the cutoff, we close any back doors because it’s basically random which side of the line you’re on\nBut we get variation in treatment!\nThis specifically gives us the effect of treatment for people who are right around the cutoff a.k.a. a “local average treatment effect” (we still won’t know the effect of being put in gifted-and-talented for someone who gets a 30)\n\n\n\n\nRegression Discontinuity\n\nA very basic idea of this, before we even get to regression, is to create a binned chart\nAnd see how the bin values jump at the cutoff\nA binned chart chops the Y-axis up into bins\nThen takes the average Y value within that bin. That’s it!\nThen, we look at how those X bins relate to the Y binned values.\nIf it looks like a pretty normal, continuous relationship… then JUMPS UP at the cutoff X-axis value, that tells us that the treatment itself must be doing something!\n\n\n\n\nRegression Discontinuity\n\n\n\n\n\n\n\n\nConcept Checks\n\nWhy is it important that we look as norrowly as possible around the cutoff? What does this get us over comparing the entire treated and untreated groups?\nCan you think of an example of a treatment that is assigned at least partially on a cutoff?\nWhy can’t we just control for the running variable as we normally would to solve the endogeneity problem?\n\n\n\n\nFitting Lines in RDD\n\nLooking purely just at the cutoff and making no use of the space away from the cutoff throws out a lot of useful information\nWe know that the running variable is related to outcome, so we can probably improve our prediction of what the value on either side of the cutoff should be if we use data away from the cutoff to help with prediction than if we just use data near the cutoff, which is what that animation does\nWe can do this with good ol’ OLS.\nThe bin plot we did can help us pick a functional form for the slope\n\n\n\n\nFitting Lines in RDD\n\nTo be clear, producing the line(s) below is our goal. How can we do it?\nThe true model I’ve made is an RDD effect of .7, with a slope of 1 to the left of the cutoff and a slope of 1.5 to the right\n\n\n\n\n\n\n\n\n\nRegression in RDD\n\nFirst, we need to transform our data\nWe need a “Treated” variable that’s TRUE when treatment is applied - above or below the cutoff\nThen, we are going to want a bunch of things to change at the cutoff. This will be easier if the running variable is centered around the cutoff. So we’ll turn our running variable \\(X\\) into \\(X - cutoff\\) and call that \\(XCentered\\)\n\n\n\n\nVarying Slope\n\nTypically, you will want to let the slope vary to either side\nIn effect, we are fitting an entirely different regression line on each side of the cutoff\nWe can do this by interacting both slope and intercept with \\(treated\\)!\nCoefficient on Treated is how the intercept jumps - that’s our RDD effect. Coefficient on the interaction is how the slope changes\n\n\\[Y = \\beta_0 + \\beta_1Treated + \\beta_2XCentered + \\beta_3Treated\\times XCentered + \\varepsilon\\]\n\n\nOLS estimation, Dep. Var.: Y\nObservations: 1,000 \nStandard-errors: IID \n                        Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)            -0.011133   0.025999 -0.428225 0.66857986    \ntreatedTRUE             0.746688   0.037577 19.870777  &lt; 2.2e-16 ***\nX_centered              0.982500   0.090666 10.836522  &lt; 2.2e-16 ***\ntreatedTRUE:X_centered  0.446961   0.129613  3.448417 0.00058748 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.296605   Adj. R2: 0.847229\n\n\n\n\n\nVarying Slope\n(as an aside, sometimes the effect of interest is the interaction term - the change in slope! This answers the question “does the effect of \\(X\\) on \\(Y\\) change at the cutoff? This is called a”regression kink” design. We won’t go more into it here, but it is out there!)\n\n\n\nPolynomial Terms\n\nWe don’t need to stop at linear slopes!\nJust like we brought in our knowledge of binary and interaction terms to understand the linear slope change, we can bring in polynomials too. Add a square maybe!\nDon’t get too wild with cubes, quartics, etc. - polynomials tend to be at their “weirdest” near the edges, and we don’t want super-weird predictions right at the cutoff. It could give us a mistaken result!\nA square term should be enough\n\n\n\n\nPolynomial Terms\n\nHow do we do this? Interactions again. Take any regression equation…\n\n\\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\\]\n\nAnd just center the \\(X\\) (let’s call it \\(XC\\), add on a set of the same terms multiplied by \\(Treated\\) (don’t forget \\(Treated\\) by itself - that’s \\(Treated\\) times the interaction!)\n\n\\[Y = \\beta_0 + \\beta_1XC + \\beta_2XC^2 + \\beta_3Treated + \\beta_4Treated\\times XC + \\beta_5Treated\\times XC^2 + \\varepsilon\\]\n\nThe coefficient on \\(Treated\\) remains our “jump at the cutoff” - our RDD estimate!\n\n\n\n                              feols(Y ~ X_cent..\nDependent Var.:                                Y\n                                                \nConstant                        -0.0340 (0.0385)\nX_centered                      0.6990. (0.3641)\ntreatedTRUE                   0.7677*** (0.0577)\nX_centered square               -0.5722 (0.7117)\nX_centered x treatedTRUE         0.7509 (0.5359)\ntreatedTRUE x I(X_centered^2)     0.5319 (1.034)\n_____________________________ __________________\nS.E. type                                    IID\nObservations                               1,000\nR2                                       0.84779\nAdj. R2                                  0.84702\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nConcept Checks\n\nWould the coefficient on \\(Treated\\) still be the regression discontinuity effect estimate if we hadn’t centered \\(X\\)? Why or why not?\nWhy might we want to use a polynomial term in our RDD model?\nWhat relationship are we assuming between the outcome variable and the running variable if we choose not to include \\(XCentered\\) in our model at all (i.e. a “zero-order polynomial”)\n\n\n\n\nAssumptions\n\nWe knew there must be some assumptions lurking around here\nSome are more obvious (we should be using the correct functional form)\nOthers are trickier. What are we assuming about the error term and endogeneity here?\nSpecifically, we are assuming that the only thing jumping at the cutoff is treatment\nSort of like parallel trends, but maybe more believable since we’ve narrowed in so far\nFor example, if having an income below 150% of the poverty line gets you access to food stamps AND to job training, then we can’t really use that cutoff to get the effect of just food stamps\nOr if the proportion of people who are self-employed jumps up just below 150% (based on reported income), that’s a back door too!\nThe only thing different about just above/just below should be treatment\n\n\n\n\nGraphically\n\n\n\n\n\n\n\n\nOther Difficulties\nMore assumptions, limitations, and diagnostics!\n\nWindows\nGranular running variables\nManipulated running variables\nFuzzy regression discontinuity\n\n\n\n\nWindows\n\nThe basic idea of RDD is that we’re interested in the cutoff\nThe points away from the cutoff are only useful in helping us predict values at the cutoff\nDo we really want that full range? Is someone’s test score of 30 really going to help us much in predicting \\(Y\\) at a test score of 89?\nSo we might limit our analysis within just a narrow window around the cutoff, just like that initial animation we saw!\nThis makes the exogenous-at-the-jump assumption more plausible, and lets us worry less about functional form (over a narrow range, not too much difference between a linear term and a square), but on the flip side reduces our sample size considerably\n\n\n\n\nWindows\n\nPay attention to the sample sizes, accuracy (true value .7) and standard errors!\n\n\nm1 &lt;- feols(Y~treated*X_centered, data = df)\nm2 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .25))\nm3 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .1))\nm4 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .05))\nm5 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .01))\netable(m1,m2,m3,m4,m5, keep = 'treatedTRUE')\n\n                                         m1                 m2\nDependent Var.:                           Y                  Y\n                                                              \ntreatedTRUE              0.7467*** (0.0376) 0.7723*** (0.0566)\ntreatedTRUE x X_centered 0.4470*** (0.1296)   0.6671. (0.4022)\n________________________ __________________ __________________\nS.E. type                               IID                IID\nObservations                          1,000                492\nR2                                  0.84769            0.74687\nAdj. R2                             0.84723            0.74531\n\n                                         m3                 m4              m5\nDependent Var.:                           Y                  Y               Y\n                                                                              \ntreatedTRUE              0.7086*** (0.0900) 0.6104*** (0.1467) 0.5585 (0.4269)\ntreatedTRUE x X_centered     -1.307 (1.482)      6.280 (4.789)   41.21 (72.21)\n________________________ __________________ __________________ _______________\nS.E. type                               IID                IID             IID\nObservations                            206                 93              15\nR2                                  0.69322            0.59825         0.48853\nAdj. R2                             0.68867            0.58470         0.34904\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nGranular Running Variable\n\nOne assumption we’re making is that the running variable varies more or less continuously\nThat makes it possible to have, say, a test score of 89 compared to a test score of 90 it’s almost certainly the same as except for random chance\nBut what if our data only had test score in big chunks? I don’t know you’re 89 or 90, I just know you’re “80-89” or “90-100”\nA lot less believable that the only difference between these groups is random chance and we’ve closed the back doors by focusing on the cutoff\nPlenty of other things change between 80 and 100! That’s not “smooth at the cutoff”\n\n\n\n\nGranular Running Variable\n\nNot a whole lot we can do about this\nThere are some fancy RDD estimators that allow for granular running variables\nBut in general, if this is what you’re facing, you might be in trouble\nBefore doing an RDD, think “is it plausible that someone with the highest value just below the cutoff, and someone with the lowest value just above the cutoff are only at different values because of random chance?”\n\n\n\n\nLooking for Lumping\n\nOk, now let’s go back to our continuous running variables\nWhat if the running variable is manipulated?\nImagine you’re a teacher grading the gifted-and-talented exam. You see someone with an 89 and think “aww, they’re so close! I’ll just give them an extra point…”\nOr, if you live just barely on one side of a time zone line, but decide to move to the other side because you prefer waking up later\nSuddenly, that treatment is a lot less randomly assigned around the cutoff!\n\n\n\n\nLooking for Lumping\n\nIf there’s manipulation of the running variable around the cutoff, we can often see it in the presence of lumping\nI.e. if there’s a big cluster of observations to one side of the cutoff and a seeming gap missing on the other side\n\n\n\n\nLooking for Lumping\n\nHere’s an example from the real world in medical research - statistically, p-values should be uniformly distributed\nBut it’s hard to get insignificant results published in some journals. So people might “p-hack” until they find some form of analysis that’s significant, and also we have heavy selection into publication based on \\(p &lt; .05\\). Can’t use that cutoff for an RDD!\n\n\n\n\np-value graph from Perneger & Combescure, 2017\n\n\n\n\n\nLooking for Lumping\n\nHow can we look for this stuff?\nWe can look graphically by just checking for a jump at the cutoff in number of observations after binning\n\n\ndf_bin_count &lt;- df %&gt;%\n  # Select breaks so that one of hte breakpoints is the cutoff\n  mutate(X_bins = cut(X, breaks = 0:10/10)) %&gt;%\n  group_by(X_bins) %&gt;%\n  count()\n\n\n\n\nLooking for Lumping\n\nThe first one looks pretty good. We have one that looks not-so-good on the right\n\n\n\n\n\n\n\n\n\nLooking for Lumping\n\nAnother thing we can do is do a “placebo test”\nCheck if variables other than treatment or outcome vary at the cutoff\nWe can do this by re-running our RDD but just swapping out some other variable for our outcome\nIf we get a significant jump, that’s bad! That tells us that other things are changing at the cutoff which implies some sort of manipulation (or just super lousy luck)\n\n\n\n\nFuzzy Regression Discontinuity\n\nSo far, we’ve assumed that you’re either on one side of the cutoff and untreated, or the other and treated\nWhat if it isn’t so simple? What if the cutoff just increases your chances of treatment?\nFor example, maybe about 10% of kids with too-low test scores get into gifted-and-talented, and 80% of kids with high-enough scores do\nFor whatever reason!\nThis is a “fuzzy regression discontinuity”\nNow, our RDD will understate the true effect, since it’s being calculated on the assumption that we added treatment to 100% of people at the cutoff, when really it’s 70%. So we’ll get roughly only about 70% of the effect\n\n\n\n\nFuzzy Regression Discontinuity\n\nWe can account for this with a model designed to take this into account\nSpecifically, we can use something called two-stage least squares (instrumental variables) to handle these sorts of situations\n(you can go see the instrumental variables module if you like for more detail)\nBasically, two-stage least squares estimates how much the chances of treatment go up at the cutoff, and scales the estimate by that change\nSo it would take whatever result we got on the previous slide and divide it by .7 to get the true effect\n\n\n\n\nFuzzy Regression Discontinuity\n\nNotice that the y-axis here isn’t the outcome, it’s “percentage treated”\n\n\n\n\n\n\n\n\n\nFuzzy Regression Discontinuity\n\nWe can perform this using the instrumental-variables features of feols, giving it two treatment-response functions\nThe first is an RDD specification where we use “treatment” - i.e. whether you were actually treated\nThe second uses the same RDD specification, but replaces “treatment” with “above the cutoff”\nfeols(outcome ~ controls  | XC*treated ~ XC*assigned_to_treatment)\n\n\n\n\nFuzzy Regression Discontinuity\n\n(the true effect of treatment is .4 - okay, it’s not perfect)\n\n\npredict_treatment &lt;- feols(treatment ~ X_center*above_cut, data = df)\nwithout_fuzzy &lt;-feols(Y ~ X_center*treatment, data = df)\nfuzzy_rdd &lt;- feols(Y ~ 1 | X_center*treatment ~ X_center*above_cut, data = df)\netable(predict_treatment, without_fuzzy, fuzzy_rdd)\n\n                          predict_treatment      without_fuzzy\nDependent Var.:                   treatment                  Y\n                                                              \nConstant                   0.0605. (0.0354) 0.4263*** (0.0359)\nX_center                    0.0044 (0.1215)  0.4099** (0.1250)\nabove_cutTRUE            0.3053*** (0.0484)                   \nX_center x above_cutTRUE   -0.0392 (0.1687)                   \ntreatmentTRUE                               0.3414*** (0.0937)\nX_center x treatmentTRUE                       0.2199 (0.3315)\nX_center:treatmentTRUE                                        \n________________________ __________________ __________________\nS.E. type                               IID                IID\nObservations                          1,000              1,000\nR2                                  0.13289            0.04967\nAdj. R2                             0.13028            0.04681\n\n                                  fuzzy_rdd\nDependent Var.:                           Y\n                                           \nConstant                 0.4429*** (0.1149)\nX_center                    0.5802 (0.4089)\nabove_cutTRUE                              \nX_center x above_cutTRUE                   \ntreatmentTRUE               0.4458 (0.4158)\nX_center x treatmentTRUE                   \nX_center:treatmentTRUE      -0.8367 (1.610)\n________________________ __________________\nS.E. type                               IID\nObservations                          1,000\nR2                                  0.03968\nAdj. R2                             0.03679\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nRegression Discontinuity in R\n\nWe’ve gone through all kinds of procedures for doing RDD in R already using regression\nBut often, professional researchers won’t do it that way!\nWe’ll use packages and formulas that do things like “picking a bandwidth (window)” for us in a smart way, or not relying so strongly on linearity\nThe rdrobust package does just that!\nLet’s look at help(rdrobust, packge = 'rdrobust')\n\n\n\n\nRegression Discontinuity in R\n\nWe can specify an RDD model by just telling it the dependent variable \\(Y\\), the running variable \\(X\\), and the cutoff \\(c\\).\nWe can also specify how many polynomials to us with p\n(it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)\nIt will also pick a window for us with h\nPlenty of other options\nIncluding a fuzzy option to specify actual treatment outside of the running variable/cutoff combo\n\n\n\n\nrdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5))\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  488          512\nEff. Number of Obs.             120          156\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.142        0.142\nBW bias (b)                   0.213        0.213\nrho (h/b)                     0.668        0.668\nUnique Obs.                     488          512\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.124     0.258     0.481     0.631    [-0.382 , 0.630]     \n        Robust         -         -     0.522     0.602    [-0.448 , 0.774]     \n=============================================================================\n\n\n\n\n\nrdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5, fuzzy = df$treatment))\n\nFuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  488          512\nEff. Number of Obs.             119          156\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.141        0.141\nBW bias (b)                   0.206        0.206\nrho (h/b)                     0.687        0.687\nUnique Obs.                     488          512\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.211     0.103     2.057     0.040     [0.010 , 0.412]     \n        Robust         -         -     1.382     0.167    [-0.072 , 0.416]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.594     1.182     0.503     0.615    [-1.722 , 2.910]     \n        Robust         -         -     0.621     0.535    [-1.921 , 3.702]     \n=============================================================================\n\n\n\n\n\nrdrobust\n\nWe can even have it automatically make plots of our RDD! Same syntax\n\n\nrdplot(df$Y, df$X, c = .5)\n\n\n\n\n\n\n\nThat’s it!\n\nThat’s what we have for RDD\nGo explore the regression discontinuity Swirl\nAnd the homework\nAnd the paper to read!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#your-lecturer",
    "href": "2023/weeks/week08/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#what-is-this-class",
    "href": "2023/weeks/week08/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be\n\n\n\nSo what is econometrics?\nEconometrics focuses on the study of observational data\nObservational data are measurements of things that the researcher does not control\nGiven that we are working with observational data, we still want to understand the causes of things\nThe world is what it is, we are only here to study it"
  },
  {
    "objectID": "2023/weeks/week07/slides.html",
    "href": "2023/weeks/week07/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#your-lecturer",
    "href": "2023/weeks/week07/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#what-is-this-class",
    "href": "2023/weeks/week07/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week06/slides.html",
    "href": "2023/weeks/week06/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week06/slides.html#your-lecturer",
    "href": "2023/weeks/week06/slides.html#your-lecturer",
    "title": "Within Variation and Fixed Effects",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week06/slides.html#what-is-this-class",
    "href": "2023/weeks/week06/slides.html#what-is-this-class",
    "title": "Within Variation and Fixed Effects",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week05/slides.html",
    "href": "2023/weeks/week05/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#your-lecturer",
    "href": "2023/weeks/week05/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#what-is-this-class",
    "href": "2023/weeks/week05/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week04/slides.html",
    "href": "2023/weeks/week04/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week04/slides.html#your-lecturer",
    "href": "2023/weeks/week04/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week04/slides.html#what-is-this-class",
    "href": "2023/weeks/week04/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week03/slides.html",
    "href": "2023/weeks/week03/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#your-lecturer",
    "href": "2023/weeks/week03/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#what-is-this-class",
    "href": "2023/weeks/week03/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week02/slides.html",
    "href": "2023/weeks/week02/slides.html",
    "title": "🗓️ Week 2 Fitting Lines",
    "section": "",
    "text": "What is identification error?\nIdentification is how you link the result you see with the conclusion you draw from it\nFor example, say you observe that kids who play video games are more aggressive in everyday life (result), and you conclude from that result that video games make kids more aggressive (conclusion)\nIf seeing that result is actually evidence for that conclusion, then we are properly identified\n\n\n\n\n\nAnother reason why we might see that result, i.e. if the same result could give us a different conclusion, like kids who are aggressive play more video games or people notice aggression more when kids play video games, then\nwe have made an identification error* - our result was not identified!*\nIdentification error is when your result in the data doesn’t actually have a clear theory (“why” or “because”)\nFor example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#your-lecturer",
    "href": "2023/weeks/week02/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#what-is-this-class",
    "href": "2023/weeks/week02/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week00/slides.html",
    "href": "2023/weeks/week00/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#your-lecturer",
    "href": "2023/weeks/week00/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\nDr.  George Melios Research Fellow www.georgemelios.com\n\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#what-is-this-class",
    "href": "2023/weeks/week00/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is an applied econometrics class for behavioural science\nEconometrics is a field that covers how economists think about statistical analysis\nWhy do we care about econometrics?\n\nMany other social science fields (even epidemiology) pick up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week00/page.html",
    "href": "2023/weeks/week00/page.html",
    "title": "Lecture",
    "section": "",
    "text": "In this pre-sessional week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments, and how we will interact throughout this course.\nIn addition to that we will revise some key concepts in statistics and econometrics."
  },
  {
    "objectID": "2023/weeks/week00/page.html#lecture-slides",
    "href": "2023/weeks/week00/page.html#lecture-slides",
    "title": "Lecture",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week00/page.html#recommended-reading",
    "href": "2023/weeks/week00/page.html#recommended-reading",
    "title": "Lecture",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck syllabus for reading suggestions!"
  },
  {
    "objectID": "2023/weeks/week00/page.html#communication",
    "href": "2023/weeks/week00/page.html#communication",
    "title": "Lecture",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week02/page.html",
    "href": "2023/weeks/week02/page.html",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "",
    "text": "In this week, we will explore how we can use formal procedures to examine if two opposing claims or hypothesis are true or not.\nIn this week, we start looking how we can use data to describe relationships between two variables and distinguish between alternative scenarios."
  },
  {
    "objectID": "2023/weeks/week02/page.html#lecture-slides",
    "href": "2023/weeks/week02/page.html#lecture-slides",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week02/page.html#recommended-reading",
    "href": "2023/weeks/week02/page.html#recommended-reading",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week02/page.html#communication",
    "href": "2023/weeks/week02/page.html#communication",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week03/page.html",
    "href": "2023/weeks/week03/page.html",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "",
    "text": "In this week, we start looking how we can use data to describe relationships between two variables and distinguish between alternative scenarios."
  },
  {
    "objectID": "2023/weeks/week03/page.html#lecture-slides",
    "href": "2023/weeks/week03/page.html#lecture-slides",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week03/page.html#recommended-reading",
    "href": "2023/weeks/week03/page.html#recommended-reading",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week03/page.html#communication",
    "href": "2023/weeks/week03/page.html#communication",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week04/page.html",
    "href": "2023/weeks/week04/page.html",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "",
    "text": "In this week, we will build on Linear regressions by expanding our analysis using multiple regressors."
  },
  {
    "objectID": "2023/weeks/week04/page.html#lecture-slides",
    "href": "2023/weeks/week04/page.html#lecture-slides",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week04/page.html#recommended-reading",
    "href": "2023/weeks/week04/page.html#recommended-reading",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week04/page.html#communication",
    "href": "2023/weeks/week04/page.html#communication",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week05/page.html",
    "href": "2023/weeks/week05/page.html",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "",
    "text": "In this week, we will focus on how we analyse and interpret binary independent variables."
  },
  {
    "objectID": "2023/weeks/week05/page.html#lecture-slides",
    "href": "2023/weeks/week05/page.html#lecture-slides",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week05/page.html#recommended-reading",
    "href": "2023/weeks/week05/page.html#recommended-reading",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week05/page.html#communication",
    "href": "2023/weeks/week05/page.html#communication",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week06/page.html",
    "href": "2023/weeks/week06/page.html",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "",
    "text": "In this week, we will focus on how we analyse within variation and include fixed effects."
  },
  {
    "objectID": "2023/weeks/week06/page.html#lecture-slides",
    "href": "2023/weeks/week06/page.html#lecture-slides",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week06/page.html#recommended-reading",
    "href": "2023/weeks/week06/page.html#recommended-reading",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week06/page.html#communication",
    "href": "2023/weeks/week06/page.html#communication",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week07/page.html",
    "href": "2023/weeks/week07/page.html",
    "title": "🗓️ Week 07 - Counterfactual world",
    "section": "",
    "text": "In this week, we will focus on the mathematical aspect of counterfactuals."
  },
  {
    "objectID": "2023/weeks/week07/page.html#lecture-slides",
    "href": "2023/weeks/week07/page.html#lecture-slides",
    "title": "🗓️ Week 07 - Counterfactual world",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week07/page.html#recommended-reading",
    "href": "2023/weeks/week07/page.html#recommended-reading",
    "title": "🗓️ Week 07 - Counterfactual world",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week07/page.html#communication",
    "href": "2023/weeks/week07/page.html#communication",
    "title": "🗓️ Week 07 - Counterfactual world",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week08/page.html",
    "href": "2023/weeks/week08/page.html",
    "title": "🗓️ Week 08 - Regression Discontinuity Design",
    "section": "",
    "text": "In this week, we will look into the quasi-experimental Regression Discontinuity Design."
  },
  {
    "objectID": "2023/weeks/week08/page.html#lecture-slides",
    "href": "2023/weeks/week08/page.html#lecture-slides",
    "title": "🗓️ Week 08 - Regression Discontinuity Design",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week08/page.html#recommended-reading",
    "href": "2023/weeks/week08/page.html#recommended-reading",
    "title": "🗓️ Week 08 - Regression Discontinuity Design",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week08/page.html#communication",
    "href": "2023/weeks/week08/page.html#communication",
    "title": "🗓️ Week 08 - Regression Discontinuity Design",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week09/page.html",
    "href": "2023/weeks/week09/page.html",
    "title": "🗓️ Week 09 - Instrumental Variables",
    "section": "",
    "text": "In this week, we will focus on estimating causal parameteres through Instrumental Variables. First, we will approach the concept of instruments through Direct Acyclical Graphs (DAGs) and then through the LATE effect (Local Average Treatment Effect)."
  },
  {
    "objectID": "2023/weeks/week09/page.html#lecture-slides",
    "href": "2023/weeks/week09/page.html#lecture-slides",
    "title": "🗓️ Week 09 - Instrumental Variables",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week09/page.html#recommended-reading",
    "href": "2023/weeks/week09/page.html#recommended-reading",
    "title": "🗓️ Week 09 - Instrumental Variables",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week09/page.html#communication",
    "href": "2023/weeks/week09/page.html#communication",
    "title": "🗓️ Week 09 - Instrumental Variables",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/syllabus.html",
    "href": "2023/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Quantitative data collection is an integral component of behavioural science: Testing hypotheses requires designing experiments and analysing the data or performing statistical analyses on secondary data. Whereas another core course in this programme - Experimental Design and Methods for the Behavioural Science - covers best practices in designing and conducting experimental research, Quantitative Applications for Behavioural Science introduces the main statistical background of behavioural research from psychology and economics. The course will cover best practices and state of the art statistical tools that are used by psychologists and economists. All the analyses will be demonstrated on example behavioural science research, and students will learn how to identify, interpret, and evaluate appropriate analyses for different research designs, conduct their own data analysis for each of these designs as well as report the analysis for publication in a journal, and recognise and understand contemporary issues in data science analysis in psychology and economics that need to be considered for best research practices. Emphasis will be on teaching students how the same analyses are presented in psychology and economics journals so students can understand how to integrate research from these two fields that constitute behavioural science."
  },
  {
    "objectID": "2023/communication.html",
    "href": "2023/communication.html",
    "title": "Announcements",
    "section": "",
    "text": "📢 Description\n\n\n\n\n\nWe will use this space to make class announcements throughout the semester!\n\n\n\n\n\n\n\n\n\n📢 Discord\n\n\n\n\n\nFirst and important announcement, Join the Class on Discord.\n\n\n\n\nRequirements\nDownload and install STATA or R. Throughout the class, we will rely on STATA for seminars but codes will be provided for R as well."
  },
  {
    "objectID": "2023/weeks/week00/slides.html#welcome-to-econometrics",
    "href": "2023/weeks/week00/slides.html#welcome-to-econometrics",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Welcome to Econometrics",
    "text": "Welcome to Econometrics\n\nThis is a great course (tough one but great)\nWhy?\nGives you the ability to think about and answer questions you are interested in\nAnd to better understand and judge existing body of literature\n\nThe classic Tik-Tok, Instagram video that starts with “A new study says…”? You can now have an idea of how robust/serious their inferences are."
  },
  {
    "objectID": "2023/weeks/week00/slides.html#welcome-to-econometrics-1",
    "href": "2023/weeks/week00/slides.html#welcome-to-econometrics-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Welcome to Econometrics",
    "text": "Welcome to Econometrics\n\nThis is a great course (tough one but great)\nWhy?\nGives you the ability to think about and answer questions you are interested in\nAnd to better understand and judge existing body of literature\n\nThe classic Tik-Tok, Instagram video that starts with “A new study says…”? You can now have an idea of how robust/serious their inferences are."
  },
  {
    "objectID": "2023/weeks/week00/slides.html#teaching-assistant",
    "href": "2023/weeks/week00/slides.html#teaching-assistant",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\n\n\n\n\nDr. Lazaros Chatzilazarou PhD Candidate Website\n\n\n\n\n\nPhD in Economics (exp 2026)\nBackground: Economics\n\nExperimental Economics"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#so-what-is-econometrics",
    "href": "2023/weeks/week00/slides.html#so-what-is-econometrics",
    "title": "🗓️ Week 0 Presessionals",
    "section": "So what is econometrics?",
    "text": "So what is econometrics?\n\nEconometrics focuses on the study of observational data\nObservational data are measurements of things that the researcher does not control\nGiven that we are working with observational data, we still want to understand the causes of things\nThe world is what it is\nFrom next week onwards though, we will explore ways to study it"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#admin",
    "href": "2023/weeks/week00/slides.html#admin",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Admin",
    "text": "Admin\n\nReview the syllabus (and other materials on Moodle and the PB4A7 website). Reading assignments there\nOur textbook is The Effect, by Huntington-Klein, which is available online for free.\nAlso these slides\nProgramming in STATA (we will get to this on Thursday)\nAssignment: End of term paper & poster\nJoin the Class on Discord"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-and-prediction-1",
    "href": "2023/weeks/week00/slides.html#causality-and-prediction-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nGreat! Still, why do we care about this class?\nIn econometrics, we are working with data\nStatisticians also work with data\nSo do data scientists\nThe goals for these groups differ"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-and-prediction-2",
    "href": "2023/weeks/week00/slides.html#causality-and-prediction-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nData scientists are generally concerned with prediction\nThey want to use the data at hand to predict what comes next\nThey generally don’t care why they’re making the prediction they are\nThis can be really handy for certain tasks - “is this picture a cat or a dog?” “what’s the probability that a customer with qualities X, Y, and Z will end up purchasing our good?” “do you have lymphoma?”"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#inference-error-and-randomness-1",
    "href": "2023/weeks/week00/slides.html#inference-error-and-randomness-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Inference Error and Randomness",
    "text": "Inference Error and Randomness\n\nSo if we look in a data set and see that \\(X\\) and \\(Y\\) appear to be positively related to each other…\nAre they actually positively related, or is that just random chance?\nIf they are positively related, maybe we’re understating or overstating how positively related"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#inference-error-and-randomness-2",
    "href": "2023/weeks/week00/slides.html#inference-error-and-randomness-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Inference Error and Randomness",
    "text": "Inference Error and Randomness\n\nIf the true relationship is 0, then in the data we’ll see a positive relationship half the time, and a negative relationship half the time\nEven though the truth is 0!\nHow can we properly make an inference about whether the relationship is 0 or not (or positive, or negative, or how positive or negative), taking into account this randomness?\nThat’s being careful about inference. The statisticians teach us all about this!"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#identification-error-1",
    "href": "2023/weeks/week00/slides.html#identification-error-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Identification Error",
    "text": "Identification Error\n\nBut if there’s another reason why we might see that result, i.e. if the same result could give us a different conclusion, like kids who are aggressive play more video games or people notice aggression more when kids play video games, then we have made an identification error - our result was not identified!\nIdentification error is when your result in the data doesn’t actually have a clear theoretical (“why” or “because”) interpretation\nFor example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts, you have committed an identification error\nOne day in and all we can do is complain, eesh"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-generating-process-1",
    "href": "2023/weeks/week00/slides.html#data-generating-process-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nAnother example is a model of supply and demand\nWe observe prices and quantities in a competitive market\nWhat led to those being the prices and quantities we see?\nThe supply and demand model and its equilibrium, we theorize!"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-generating-process-2",
    "href": "2023/weeks/week00/slides.html#data-generating-process-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data Generating Process",
    "text": "Data Generating Process"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-generating-process-3",
    "href": "2023/weeks/week00/slides.html#data-generating-process-3",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nThe prices that we observe come from that theoretical construct\nWhen we see the prices and quantities moving, according to our theory, it’s because the S and D lines are moving\nBut we can’t see the S and D lines\nOur goal: use the observations we do see to infer what the theoretical model (data generating process) is"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-generating-process-4",
    "href": "2023/weeks/week00/slides.html#data-generating-process-4",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nHarder than it sounds. What inference about S and D can we draw from these observations?"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-1",
    "href": "2023/weeks/week00/slides.html#causality-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nWe can also represent this set of relationships as a graph, with arrows telling you what variables cause each other"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-2",
    "href": "2023/weeks/week00/slides.html#causality-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nWe do this because most of the relationships we’re interested in are causal - we want to know, if we could reach in and manipulate \\(X\\), would \\(Y\\) change as a result, and how much?\nDoes the minimum wage reduce employment?\nDoes quantitative easing avert recessions?\nDoes six-sigma improve business performance?\nDoes getting an MBA make you a better manager?"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-3",
    "href": "2023/weeks/week00/slides.html#causality-3",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nImagine this is the graph we see for minimum wage and employment"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-4",
    "href": "2023/weeks/week00/slides.html#causality-4",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nDoes that mean that the minimum wage harms employment?\nMaybe! But also maybe not\nWhat the graph shows us is a correlation\nAnd correlation is not the same thing as causation"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-5",
    "href": "2023/weeks/week00/slides.html#causality-5",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nA given correlation, like the negative relationship between minimum wage changes and employment changes, can be consistent with a number of different causal relationships\nAs econometricians, we need to figure out which one it is!\nHow can we narrow it down?\nHow many of the diagrams on the next page can be consistent with that negative relationship?"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#eight-possible-relationships",
    "href": "2023/weeks/week00/slides.html#eight-possible-relationships",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Eight Possible Relationships",
    "text": "Eight Possible Relationships"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-6",
    "href": "2023/weeks/week00/slides.html#causality-6",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nThe only ones we can eliminate are d, g, and h\nAll the rest are possible!\nIf f is correct, we see the negative relationship even though minimum wage has nothing to do with causing employment (like the ice cream and shorts example)\nIf a is correct, then even though we know minimum wage causes employment to change, the size or even direction of the relationship will be wrong (why?)"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-7",
    "href": "2023/weeks/week00/slides.html#causality-7",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nSo which of them is likely to be correct?\nThat depends on what we think \\(\\varepsilon\\) is\n\\(\\varepsilon\\) is everything that determines \\(Y\\) other than \\(X\\)\nPerhaps the health of the economy, or the policies that area has chosen\nSo we almost certainly have a graph with \\(\\varepsilon \\rightarrow Y\\)\nDo those things also affect the choice to raise the minimum wage? If so we’re in graph a. That downward relationship could be due to a null relationship, or even a positive one (or perhaps a more negative one?)"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-and-prediction-3",
    "href": "2023/weeks/week00/slides.html#causality-and-prediction-3",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nEconometricians, on the other hand, care almost exclusively about why\nData scientists want to minimize prediction error\nEconometricians want to minimize inference and identification error\nWe want to correctly understand the underlying data generating process"
  },
  {
    "objectID": "2023/syllabus.html#description",
    "href": "2023/syllabus.html#description",
    "title": "Syllabus",
    "section": "",
    "text": "Quantitative data collection is an integral component of behavioural science: Testing hypotheses requires designing experiments and analysing the data or performing statistical analyses on secondary data. Whereas another core course in this programme - Experimental Design and Methods for the Behavioural Science - covers best practices in designing and conducting experimental research, Quantitative Applications for Behavioural Science introduces the main statistical background of behavioural research from psychology and economics. The course will cover best practices and state of the art statistical tools that are used by psychologists and economists. All the analyses will be demonstrated on example behavioural science research, and students will learn how to identify, interpret, and evaluate appropriate analyses for different research designs, conduct their own data analysis for each of these designs as well as report the analysis for publication in a journal, and recognise and understand contemporary issues in data science analysis in psychology and economics that need to be considered for best research practices. Emphasis will be on teaching students how the same analyses are presented in psychology and economics journals so students can understand how to integrate research from these two fields that constitute behavioural science."
  },
  {
    "objectID": "2023/weeks/week01/page.html#recommended-reading-1",
    "href": "2023/weeks/week01/page.html#recommended-reading-1",
    "title": "🗓️ Week 01 - Introduction, Context & Key Concepts",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week00/slides.html#what-is-pb4a7",
    "href": "2023/weeks/week00/slides.html#what-is-pb4a7",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is PB4A7",
    "text": "What is PB4A7\n\nNot a maths course!\nNot a pure stats course!\nNot a theoretical econometrics course!\nNot a data science course!"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#what-is-pb4a7-1",
    "href": "2023/weeks/week00/slides.html#what-is-pb4a7-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is PB4A7",
    "text": "What is PB4A7"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#textbook",
    "href": "2023/weeks/week00/slides.html#textbook",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Textbook",
    "text": "Textbook"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#why-applications-and-not-econometrics",
    "href": "2023/weeks/week00/slides.html#why-applications-and-not-econometrics",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Why applications and not econometrics?",
    "text": "Why applications and not econometrics?\n\nPB4A7 and PB413 are applied courses. You need to know how to use statistics, and why you’re using them – you will not master the nuts of bolts of statistical theory!\nFor those who want more information\n\nWill provide material on the website of the class\nVisit me during the office hours\nWe will discuss additional courses to audit"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data",
    "href": "2023/weeks/week00/slides.html#data",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-1",
    "href": "2023/weeks/week00/slides.html#data-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-2",
    "href": "2023/weeks/week00/slides.html#data-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#an-example",
    "href": "2023/weeks/week00/slides.html#an-example",
    "title": "🗓️ Week 0 Presessionals",
    "section": "An example",
    "text": "An example"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#housekeeping",
    "href": "2023/weeks/week00/slides.html#housekeeping",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nMoodle (lecture videos and assignments)\nWebsite\nDiscord Channel\nOffice Hours (Book Upfront!!!)"
  },
  {
    "objectID": "2023/Stata.html",
    "href": "2023/Stata.html",
    "title": "Stata",
    "section": "",
    "text": "📢 Description\n\n\n\n\n\nDear All,\nFor anyone that wants extra Intro (from scratch) material for STATA, I can propose the following. These should help you become STATA “experts” in little time. I know this sounds like these ads for “Build a six-pack in 10 steps” but STATA is actually much easier than that. Our seminars and a few of those videos should help you pick it up in no time.\nLSE - DATA SKILLS LAB COURSE\nYou can enrol here and follow at your own pace. It’s supposed to take 10-14 hours to complete in total and should give anyone and superbly good background in stata. You can follow it at your own pace.\nYouTube Channels\nStata Intro - STATA Company itself has a 2hour introduction to STATA that is superb\nEconometrics Academy - Econometrics Academy has a playlist on Introduction to STATA\nData for Development - Data for Development has a nice set of videos as well\nSee you all soon, George"
  },
  {
    "objectID": "2023/R.html",
    "href": "2023/R.html",
    "title": "Stata",
    "section": "",
    "text": "📢 Description\n\n\n\n\n\nDear All,\nHope this finds you well. This channel on the server is for those of you that already have some stata knowledge and want to spend the time you might have the first week(s) to expand more on programming and stats. It’s a mix of LSE courses you can attend and online material that are fantastic to help you progress.\nLSE Modules Courses on dept of methodology - here you can find courses on causal inference, advanced statistics and data science summary of courses here\nCourses from Digital Skills Lab intro to a lot of programming languages (R, STATA, Python)\nCourses from Data Science Institute Courses from B.Sc. in Data Science on Politics\nOnline Resources\n\nHertie Summer School Probably the best and most variable resource for everyone that wants to expand their knowledge on Data Science and statistics is the Hertie Summer School that is free online. Here is a course catalog and they offer courses from intro to Data science to Advanced Machine Learning, NLPs, Scraping data from the web and anything you can imagine. (the one from 2021 has more courses than the 2022 version)\nIsmail Khalil has a fantastic channel with Advanced R programming and Machine Learning on YouTube\n\nWill keep updating this on the relevant Discord Channel and here as much as possible throughout the semester.\nBest wishes, George"
  },
  {
    "objectID": "2023/weeks/week01/page1.html",
    "href": "2023/weeks/week01/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week01/page1.html#seminar-slides",
    "href": "2023/weeks/week01/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week01/page1.html#communication",
    "href": "2023/weeks/week01/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-in-r-1",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-in-r-1",
    "title": "🗓️ Week 8 Regresssion Discontinuity Designs",
    "section": "Regression Discontinuity in R",
    "text": "Regression Discontinuity in R\n\nWe can specify an RDD model by just telling it the dependent variable \\(Y\\), the running variable \\(X\\), and the cutoff \\(c\\).\nWe can also specify how many polynomials to us with p\n(it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)\nIt will also pick a window for us with h\nPlenty of other options\nIncluding a fuzzy option to specify actual treatment outside of the running variable/cutoff combo"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#rdrobust",
    "href": "2023/weeks/week08/slides.html#rdrobust",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nWe’ve gone through all kinds of procedures for doing RDD in R already using regression\nBut often, professional researchers won’t do it that way!\nWe’ll use packages and formulas that do things like “picking a bandwidth (window)” for us in a smart way, or not relying so strongly on linearity\nThe rdrobust package does just that!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-in-stata",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-in-stata",
    "title": "🗓️ Week 8 Regresssion Discontinuity Designs",
    "section": "Regression Discontinuity in STATA",
    "text": "Regression Discontinuity in STATA\n\nWe can specify an RDD model by just telling it the dependent variable \\(Y\\), the running variable \\(X\\), and the cutoff \\(c\\).\nWe can also specify how many polynomials to us with p\n(it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)\nIt will also pick a window for us with h\nPlenty of other options\nIncluding a fuzzy option to specify actual treatment outside of the running variable/cutoff combo"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-in-r",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-in-r",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity in R",
    "text": "Regression Discontinuity in R\n\nWe can specify an RDD model by just telling it the dependent variable \\(Y\\), the running variable \\(X\\), and the cutoff \\(c\\).\nWe can also specify how many polynomials to us with p\n(it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)\nIt will also pick a window for us with h\nPlenty of other options\nIncluding a fuzzy option to specify actual treatment outside of the running variable/cutoff combo"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#check-in",
    "href": "2023/weeks/week08/slides.html#check-in",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Check-in",
    "text": "Check-in\n\nWe’re thinking through ways that we can identify the effect of interest without having to control for everything\nOne way is by focusing on within variation - if all the endogeneity can be controlled for or only varies between-individuals, we can just focus on within variation to identify it\nPro: control for a bunch of stuff\nCon: washes out a lot of variation! Result can be noisier if there’s not much within-variation to work with\nAlso, this requires no endogenous variation over time\nThat might be a tricky assumption! Often there are plenty of back doors that shift over time"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-1",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\nThe basic idea is this:\n\nWe look for a treatment that is assigned on the basis of being above/below a cutoff value of a continuous variable\nFor example, if you get above a certain test score they let you into a “gifted and talented” program\nOr if you are just on one side of a time zone line, your day starts one hour earlier/later\nOr if a candidate gets 50.1% of the vote they’re in, 40.9% and they’re out\nOr if you’re 65 years old you get Medicaid, if you’re 64.99 years old you don’t\n\nWe call these continuous variables “Running variables” because we run along them until we hit the cutoff"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-2",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-2",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nBut wait, hold on, if treatment is driven by running variables, won’t we have a back door going through those very same running variables?? Yes!\nAnd we can’t just control for RunningVar because that’s where all the variation in treatment comes from. Uh oh!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-3",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-3",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nThe key here is realizing that the running variable affects treatment only when you go across the cutoff\nSo really the diagram looks like this!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-4",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-4",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nSo what does this mean?\nIf we can control for the running variable everywhere except the cutoff, then…\nWe will be controlling for the running variable, closing that back door\nBut leaving variation at the cutoff open, allowing for variation in treatment\nWe focus on just the variation around the treatment, narrowing the range of the running variable we use so sharply that it’s basically controlled for. Then the effect of cutoff on treatment is like an experiment!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-5",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-5",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nBasically, the idea is that right around the cutoff, treatment is randomly assigned\nIf you have a test score of 89.9 (not high enough for gifted-and-talented), you’re basically the same as someone who has a test score of 90.0 (just barely high enough)\nSo if we just focus around the cutoff, we close any back doors because it’s basically random which side of the line you’re on\nBut we get variation in treatment!\nThis specifically gives us the effect of treatment for people who are right around the cutoff a.k.a. a “local average treatment effect” (we still won’t know the effect of being put in gifted-and-talented for someone who gets a 30)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-6",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-6",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nA very basic idea of this, before we even get to regression, is to create a binned chart\nAnd see how the bin values jump at the cutoff\nA binned chart chops the Y-axis up into bins\nThen takes the average Y value within that bin. That’s it!\nThen, we look at how those X bins relate to the Y binned values.\nIf it looks like a pretty normal, continuous relationship… then JUMPS UP at the cutoff X-axis value, that tells us that the treatment itself must be doing something!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-7",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-7",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#concept-checks",
    "href": "2023/weeks/week08/slides.html#concept-checks",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy is it important that we look as norrowly as possible around the cutoff? What does this get us over comparing the entire treated and untreated groups?\nCan you think of an example of a treatment that is assigned at least partially on a cutoff?\nWhy can’t we just control for the running variable as we normally would to solve the endogeneity problem?"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#fitting-lines-in-rdd",
    "href": "2023/weeks/week08/slides.html#fitting-lines-in-rdd",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Fitting Lines in RDD",
    "text": "Fitting Lines in RDD\n\nLooking purely just at the cutoff and making no use of the space away from the cutoff throws out a lot of useful information\nWe know that the running variable is related to outcome, so we can probably improve our prediction of what the value on either side of the cutoff should be if we use data away from the cutoff to help with prediction than if we just use data near the cutoff, which is what that animation does\nWe can do this with good ol’ OLS.\nThe bin plot we did can help us pick a functional form for the slope"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#fitting-lines-in-rdd-1",
    "href": "2023/weeks/week08/slides.html#fitting-lines-in-rdd-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Fitting Lines in RDD",
    "text": "Fitting Lines in RDD\n\nTo be clear, producing the line(s) below is our goal. How can we do it?\nThe true model I’ve made is an RDD effect of .7, with a slope of 1 to the left of the cutoff and a slope of 1.5 to the right"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-in-rdd",
    "href": "2023/weeks/week08/slides.html#regression-in-rdd",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression in RDD",
    "text": "Regression in RDD\n\nFirst, we need to transform our data\nWe need a “Treated” variable that’s TRUE when treatment is applied - above or below the cutoff\nThen, we are going to want a bunch of things to change at the cutoff. This will be easier if the running variable is centered around the cutoff. So we’ll turn our running variable \\(X\\) into \\(X - cutoff\\) and call that \\(XCentered\\)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#varying-slope",
    "href": "2023/weeks/week08/slides.html#varying-slope",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Varying Slope",
    "text": "Varying Slope\n\nTypically, you will want to let the slope vary to either side\nIn effect, we are fitting an entirely different regression line on each side of the cutoff\nWe can do this by interacting both slope and intercept with \\(treated\\)!\nCoefficient on Treated is how the intercept jumps - that’s our RDD effect. Coefficient on the interaction is how the slope changes\n\n\\[Y = \\beta_0 + \\beta_1Treated + \\beta_2XCentered + \\beta_3Treated\\times XCentered + \\varepsilon\\]\n\n\nOLS estimation, Dep. Var.: Y\nObservations: 1,000 \nStandard-errors: IID \n                        Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)            -0.011133   0.025999 -0.428225 0.66857986    \ntreatedTRUE             0.746688   0.037577 19.870777  &lt; 2.2e-16 ***\nX_centered              0.982500   0.090666 10.836522  &lt; 2.2e-16 ***\ntreatedTRUE:X_centered  0.446961   0.129613  3.448417 0.00058748 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.296605   Adj. R2: 0.847229"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#varying-slope-1",
    "href": "2023/weeks/week08/slides.html#varying-slope-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Varying Slope",
    "text": "Varying Slope\n(as an aside, sometimes the effect of interest is the interaction term - the change in slope! This answers the question “does the effect of \\(X\\) on \\(Y\\) change at the cutoff? This is called a”regression kink” design. We won’t go more into it here, but it is out there!)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#polynomial-terms",
    "href": "2023/weeks/week08/slides.html#polynomial-terms",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Polynomial Terms",
    "text": "Polynomial Terms\n\nWe don’t need to stop at linear slopes!\nJust like we brought in our knowledge of binary and interaction terms to understand the linear slope change, we can bring in polynomials too. Add a square maybe!\nDon’t get too wild with cubes, quartics, etc. - polynomials tend to be at their “weirdest” near the edges, and we don’t want super-weird predictions right at the cutoff. It could give us a mistaken result!\nA square term should be enough"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#polynomial-terms-1",
    "href": "2023/weeks/week08/slides.html#polynomial-terms-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Polynomial Terms",
    "text": "Polynomial Terms\n\nHow do we do this? Interactions again. Take any regression equation…\n\n\\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\\]\n\nAnd just center the \\(X\\) (let’s call it \\(XC\\), add on a set of the same terms multiplied by \\(Treated\\) (don’t forget \\(Treated\\) by itself - that’s \\(Treated\\) times the interaction!)\n\n\\[Y = \\beta_0 + \\beta_1XC + \\beta_2XC^2 + \\beta_3Treated + \\beta_4Treated\\times XC + \\beta_5Treated\\times XC^2 + \\varepsilon\\]\n\nThe coefficient on \\(Treated\\) remains our “jump at the cutoff” - our RDD estimate!\n\n\n\n                              feols(Y ~ X_cent..\nDependent Var.:                                Y\n                                                \nConstant                        -0.0340 (0.0385)\nX_centered                      0.6990. (0.3641)\ntreatedTRUE                   0.7677*** (0.0577)\nX_centered square               -0.5722 (0.7117)\nX_centered x treatedTRUE         0.7509 (0.5359)\ntreatedTRUE x I(X_centered^2)     0.5319 (1.034)\n_____________________________ __________________\nS.E. type                                    IID\nObservations                               1,000\nR2                                       0.84779\nAdj. R2                                  0.84702\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#concept-checks-1",
    "href": "2023/weeks/week08/slides.html#concept-checks-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWould the coefficient on \\(Treated\\) still be the regression discontinuity effect estimate if we hadn’t centered \\(X\\)? Why or why not?\nWhy might we want to use a polynomial term in our RDD model?\nWhat relationship are we assuming between the outcome variable and the running variable if we choose not to include \\(XCentered\\) in our model at all (i.e. a “zero-order polynomial”)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#assumptions",
    "href": "2023/weeks/week08/slides.html#assumptions",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Assumptions",
    "text": "Assumptions\n\nWe knew there must be some assumptions lurking around here\nSome are more obvious (we should be using the correct functional form)\nOthers are trickier. What are we assuming about the error term and endogeneity here?\nSpecifically, we are assuming that the only thing jumping at the cutoff is treatment\nSort of like parallel trends, but maybe more believable since we’ve narrowed in so far\nFor example, if having an income below 150% of the poverty line gets you access to food stamps AND to job training, then we can’t really use that cutoff to get the effect of just food stamps\nOr if the proportion of people who are self-employed jumps up just below 150% (based on reported income), that’s a back door too!\nThe only thing different about just above/just below should be treatment"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#graphically",
    "href": "2023/weeks/week08/slides.html#graphically",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Graphically",
    "text": "Graphically"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#windows-1",
    "href": "2023/weeks/week08/slides.html#windows-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Windows",
    "text": "Windows\n\nPay attention to the sample sizes, accuracy (true value .7) and standard errors!\n\n\nm1 &lt;- feols(Y~treated*X_centered, data = df)\nm2 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .25))\nm3 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .1))\nm4 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .05))\nm5 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .01))\netable(m1,m2,m3,m4,m5, keep = 'treatedTRUE')\n\n                                         m1                 m2\nDependent Var.:                           Y                  Y\n                                                              \ntreatedTRUE              0.7467*** (0.0376) 0.7723*** (0.0566)\ntreatedTRUE x X_centered 0.4470*** (0.1296)   0.6671. (0.4022)\n________________________ __________________ __________________\nS.E. type                               IID                IID\nObservations                          1,000                492\nR2                                  0.84769            0.74687\nAdj. R2                             0.84723            0.74531\n\n                                         m3                 m4              m5\nDependent Var.:                           Y                  Y               Y\n                                                                              \ntreatedTRUE              0.7086*** (0.0900) 0.6104*** (0.1467) 0.5585 (0.4269)\ntreatedTRUE x X_centered     -1.307 (1.482)      6.280 (4.789)   41.21 (72.21)\n________________________ __________________ __________________ _______________\nS.E. type                               IID                IID             IID\nObservations                            206                 93              15\nR2                                  0.69322            0.59825         0.48853\nAdj. R2                             0.68867            0.58470         0.34904\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#granular-running-variable-1",
    "href": "2023/weeks/week08/slides.html#granular-running-variable-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Granular Running Variable",
    "text": "Granular Running Variable\n\nNot a whole lot we can do about this\nThere are some fancy RDD estimators that allow for granular running variables\nBut in general, if this is what you’re facing, you might be in trouble\nBefore doing an RDD, think “is it plausible that someone with the highest value just below the cutoff, and someone with the lowest value just above the cutoff are only at different values because of random chance?”"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-1",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nIf there’s manipulation of the running variable around the cutoff, we can often see it in the presence of lumping\nI.e. if there’s a big cluster of observations to one side of the cutoff and a seeming gap missing on the other side"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-2",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-2",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nHere’s an example from the real world in medical research - statistically, p-values should be uniformly distributed\nBut it’s hard to get insignificant results published in some journals. So people might “p-hack” until they find some form of analysis that’s significant, and also we have heavy selection into publication based on \\(p &lt; .05\\). Can’t use that cutoff for an RDD!\n\n\np-value graph from Perneger & Combescure, 2017"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-3",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-3",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nHow can we look for this stuff?\nWe can look graphically by just checking for a jump at the cutoff in number of observations after binning\n\n\ndf_bin_count &lt;- df %&gt;%\n  # Select breaks so that one of hte breakpoints is the cutoff\n  mutate(X_bins = cut(X, breaks = 0:10/10)) %&gt;%\n  group_by(X_bins) %&gt;%\n  count()"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-4",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-4",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nThe first one looks pretty good. We have one that looks not-so-good on the right"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-5",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-5",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nAnother thing we can do is do a “placebo test”\nCheck if variables other than treatment or outcome vary at the cutoff\nWe can do this by re-running our RDD but just swapping out some other variable for our outcome\nIf we get a significant jump, that’s bad! That tells us that other things are changing at the cutoff which implies some sort of manipulation (or just super lousy luck)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#rdrobust-1",
    "href": "2023/weeks/week08/slides.html#rdrobust-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5))\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  501          499\nEff. Number of Obs.             185          170\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.174        0.174\nBW bias (b)                   0.293        0.293\nrho (h/b)                     0.594        0.594\nUnique Obs.                     501          499\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.707     0.085     8.311     0.000     [0.540 , 0.874]     \n        Robust         -         -     6.762     0.000     [0.484 , 0.878]     \n============================================================================="
  },
  {
    "objectID": "2023/weeks/week08/slides.html#rdrobust-2",
    "href": "2023/weeks/week08/slides.html#rdrobust-2",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5, fuzzy = df$treatment))\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  501          499\nEff. Number of Obs.             185          170\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.174        0.174\nBW bias (b)                   0.293        0.293\nrho (h/b)                     0.594        0.594\nUnique Obs.                     501          499\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.707     0.085     8.311     0.000     [0.540 , 0.874]     \n        Robust         -         -     6.762     0.000     [0.484 , 0.878]     \n============================================================================="
  },
  {
    "objectID": "2023/weeks/week08/slides.html#rdrobust-3",
    "href": "2023/weeks/week08/slides.html#rdrobust-3",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nWe can even have it automatically make plots of our RDD! Same syntax\n\n\nrdplot(df$Y, df$X, c = .5)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#thats-it",
    "href": "2023/weeks/week08/slides.html#thats-it",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "That’s it!",
    "text": "That’s it!\n\nThat’s what we have for RDD\nGo explore the regression discontinuity Seminar\nAnd the paper to read!\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-you-should-expect",
    "href": "2023/weeks/week01/slides.html#what-you-should-expect",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What you should expect",
    "text": "What you should expect\n\nConfidence: You will feel like you have a good understanding of design-based causal inference by the end such that it doesn’t feel so mysterious or intimidating\nComprehension: You will have learned a lot both conceptually but also in various specifics, particularly with regards to issues around identification and estimation\nCompetency: You will have had some experience working together implementing these methods using code in Stata syntax, possession of programs, knowledge of packages"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#lectures-plan",
    "href": "2023/weeks/week01/slides.html#lectures-plan",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Lectures plan",
    "text": "Lectures plan\n\nWeek 2 - Linear regressions\nWeek 3 - Hypothesis testing\nWeek 4 – Linear regressions with multiple regressors / Non-linear functions\nWeek 5 – Regressions on Binary variables\nWeek 6 – BREAK!!! (eeeehm Reading Week)\nWeek 7 – Recap & POTENTIAL OUTCOMES\nWeek 8 – Panel regressions\nWeek 9 – Regression Discontinuity Designs\nWeek 10 – Instrumental Variables\nWeek 11 – Difference in Differences"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-is-a-good-research-question",
    "href": "2023/weeks/week01/slides.html#what-is-a-good-research-question",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What is a good research question?",
    "text": "What is a good research question?\n\nComing up with questions is easy.\nBut coming up with good ones, is tricky. Good RQ:"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs",
    "href": "2023/weeks/week01/slides.html#research-designs",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs",
    "text": "Research Designs\n\nQuantitative empirical analysis uses data to explore, test or estimate a relationship."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-is-a-good-research-question-1",
    "href": "2023/weeks/week01/slides.html#what-is-a-good-research-question-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What is a good research question?",
    "text": "What is a good research question?\n\nComing up with questions is easy.\nBut coming up with good ones, is tricky. Good RQ:\n\nA question that can be answered / Researchable:\nHow can you answer a question which is unanswerable?\nWhat versus why? Are you trying to determine what causes Y, or why something causes Y.\n\nImprove our understanding of the world:\n\nDoesn’t have to shake the foundations of science and human knowledge\nWhat if I find an unexpected result?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs-1",
    "href": "2023/weeks/week01/slides.html#research-designs-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs",
    "text": "Research Designs\n\nFrom a broad spectrum of methodologies, we will cover:"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causal-inference",
    "href": "2023/weeks/week01/slides.html#causal-inference",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nContemplating interventios that change behaviour:\n\nHow would littering parks change if we increase the severity of fines?\n\nIs public shaming more effective?\n\nWhat if we increase other types of fines (i.e. driving)?\n\nWill people commit less crimes?\n\n\n\nEach of these policies is asking what happens to some outcome if we change on factor – the intervention – and hold all other factors constant."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#a-little-throwback",
    "href": "2023/weeks/week01/slides.html#a-little-throwback",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "A little throwback",
    "text": "A little throwback\n\nOctober 2021’s Nobel Prize in economics went to Card, Angrist and Imbens\nBut it’s arguably as much to Princeton’s mid 1980s Industrial Relations group as it’s ground zero for the credibility revolution\nStarts with Orley Ashenfelter, who had been working on job trainings programs\nKEY individuals: Orley Ashenfelter, David Card (Orley’s student), Josh Angrist (Card and Orley’s student), Alan Krueger (hired by Orley), Bob Lalonde (Card and Orley’s student) and then a generation of students (Levine, Currie, Pischke)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#a-little-throwback-1",
    "href": "2023/weeks/week01/slides.html#a-little-throwback-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "A little throwback",
    "text": "A little throwback\n\nAngrist started working on how randomization in Vietnam drafting can explain later outcomes (we will see this in Week 10)\nMeets Gibens and they both get mentored by Gary Chamberlain\nThey propose the potential outcomes framework\nThis course is about these people, their ideas, subsequent development and how the revolutionised modern empirical research with observational data"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-1",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-2",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment\nAliens come and orbit earth, in superposition.\n\nThey see sick people in hospitals\nWhat do they? think?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-3",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment\nAliens come and orbit earth, in superposition.\n\nThey see sick people in hospitals\nWhat do they? think?\nHospitals kill people. What is the difference? Doctors?\n\nThey kill the doctors, unplug patients from machines, throw open the doors – many more patients inexplicably die\nSounds ridiculous?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#three-types-of-errors",
    "href": "2023/weeks/week01/slides.html#three-types-of-errors",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Three types of errors",
    "text": "Three types of errors\n\nCorrelation =/= causation"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#three-types-of-errors-1",
    "href": "2023/weeks/week01/slides.html#three-types-of-errors-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Three types of errors",
    "text": "Three types of errors\n\nCorrelation =/= causation\nSomething Happening first may not imply causality (rooster)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#three-types-of-errors-2",
    "href": "2023/weeks/week01/slides.html#three-types-of-errors-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Three types of errors",
    "text": "Three types of errors\n\nCorrelation =/= causation\nSomething Happening first may not imply causality (rooster)\nNo correlation does not imply no causation"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-4",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-4",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment\nAliens come and orbit earth, in superposition.\n\nThey see sick people in hospitals\nWhat do they? think?\nHospitals kill people. What is the difference? Doctors?\n\nThey kill the doctors, unplug patients from machines, throw open the doors – many more patients inexplicably die\nSounds ridiculous?\nAren’t we all aliens in our research?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs-and-causality-1",
    "href": "2023/weeks/week01/slides.html#research-designs-and-causality-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs and Causality",
    "text": "Research Designs and Causality\nExample: If we want to know whether a vaccine works\n\nWe compare people who have gotten vaccinated and those who took a placebo instead"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs-and-causality-2",
    "href": "2023/weeks/week01/slides.html#research-designs-and-causality-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs and Causality",
    "text": "Research Designs and Causality\nExample: If we want to know whether a vaccine works\n\nWe compare people who have gotten vaccinated and those who took a placebo instead\nIn a classic clinical experiment, one applies a ‘treatment’ (0 = placebo, 1 = vaccine) to some set of n ‘subjects’ and observes some ‘outcome’ (Y).\nWe can then estimate:\n\nY = infection(0,1)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs-and-causality-3",
    "href": "2023/weeks/week01/slides.html#research-designs-and-causality-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs and Causality",
    "text": "Research Designs and Causality\n\nEach individual i is assigned into one of the treatment options (0 = placebo, 1 = vaccine)\nTherefore, each i as two potential outcomes:\n\nWhat would happen if they got the placebo? Yi (0)\nWhat would happen if they got the vaccine? Yi (1)\n\nDid vaccines prevent infection?\n\nTo answer this we need to know what happened to the individual if they got the vaccine and what happened to the same individual if they got the placebo."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#counterfactuals",
    "href": "2023/weeks/week01/slides.html#counterfactuals",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nWhat actually happened (i.e., the ‘factual’):\n\nI got the vaccine and did not get sick\nTreatment (X) = 1\nObserved outcome = Yi(1)\n\nThe counterfactual: (what would have happened)\n\nIf I did not get the vaccine, would I have fallen sick?\nCounterfactual treatment (X) = 0\nCounterfactual outcome = Yi(0)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#counterfactuals-1",
    "href": "2023/weeks/week01/slides.html#counterfactuals-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nAfter treatment is assigned there is potential for only one outcome to be observed"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#counterfactuals-2",
    "href": "2023/weeks/week01/slides.html#counterfactuals-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nBut ideally we would like to observe two:"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#fundamental-problem-of-causal-inference",
    "href": "2023/weeks/week01/slides.html#fundamental-problem-of-causal-inference",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\n\nOnce we observe one treatment for one individual, we cannot observe a different treatment for the same individual.\nThis is called the “fundamental problem of causal inference.” Each potential outcome is observable, but we can never observe all of them.” (Rubin, 2005, p. 323).\nThen, why are we discussing all these?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#fundamental-problem-of-causal-inference-1",
    "href": "2023/weeks/week01/slides.html#fundamental-problem-of-causal-inference-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\n\nOnce we observe one treatment for one individual, we cannot observe a different treatment for the same individual.\nThis is called the “fundamental problem of causal inference.” Each potential outcome is observable, but we can never observe all of them.” (Rubin, 2005, p. 323).\nThen, why are we discussing all these?\nWe can observe different treatments across different people.\nThis may be a way of solving the fundamental problem, but it introduces a new problem we must consider."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#selection-bias",
    "href": "2023/weeks/week01/slides.html#selection-bias",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Selection Bias",
    "text": "Selection Bias\n\nThis new problem arises because different people are… DIFFERENT!"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#selection-bias-1",
    "href": "2023/weeks/week01/slides.html#selection-bias-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Selection Bias",
    "text": "Selection Bias\n\nDifferences between people following a treatment may be because of the treatment, or they may be because of the differences in the people being treated.\nThis is selection bias.\nLet’s consider some other factors which may matter for selection bias."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#addressing-selection-bias",
    "href": "2023/weeks/week01/slides.html#addressing-selection-bias",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Addressing Selection Bias",
    "text": "Addressing Selection Bias\n\nSelect a large enough random sample and divide them into two groups.\n\nCharacteristics which contribute to selection bias should on average be distributed the same between both groups.\nTherefore, we expect that the treatment and control groups should differ only because of the treatment, and in absence of the treatment, would produce the same results."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#addressing-selection-bias-1",
    "href": "2023/weeks/week01/slides.html#addressing-selection-bias-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Addressing Selection Bias",
    "text": "Addressing Selection Bias\n\nEach group differs within the group…\nBut, on average, the groups themselves are the same, and so are comparable.\nThe effect of treatment on average would then be:\nE(Y | T = 1) – E(Y | T = 0) = Average Treatment Effect (ATE)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#treatment-effect",
    "href": "2023/weeks/week01/slides.html#treatment-effect",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Treatment effect",
    "text": "Treatment effect\n\nThe effect of the intervention then would be:\n\nTreatment effect of intervention = Outvome of Treated - Outcome of Untreated + Selection Bias\nSelection bias is the difference in average outcomes between treatment and control groups due to factors other than the treatment status\nThe true treatment effect, selection bias needs to be eliminated, or shown to be reasonably assumed to be zero.\nTo eliminate selection bias, we need well designed experiments (Matteo’s class) and large enough samples"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#experiments-not-always-the-solution",
    "href": "2023/weeks/week01/slides.html#experiments-not-always-the-solution",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Experiments not always the solution",
    "text": "Experiments not always the solution\n\nTime consuming and expensive\nMay have ethical issues\nRequire large samples for the assumptions to hold\nSuffer from drop-out and non-compliance\nEstimated parameters in an experiment may different from the parameters in which the intervention will actually take place.\nNot very easy to observe ‘real’ behaviours or consequential behaviours because of the setting.\nAn interesting paper on the limits of RCTs from Deaton (Nobel laureate) and Cartwright (2017), if you’re interested!\nWhat do we do then?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causal-inference-1",
    "href": "2023/weeks/week01/slides.html#causal-inference-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causal inference",
    "text": "Causal inference\n\nWe design a strategy (Identification Strategy from now on) that allows us to:\n\nIdentify and isolate the random variation in treatment (i.e. a natural disaster)\nRely on institutional knowledge, theory and data to:\n\nReduce as much as possible Selection Bias\nIdentify outcomes for treated and untreated populations\nEstimate average treatment effects"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-follows",
    "href": "2023/weeks/week01/slides.html#what-follows",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What follows?",
    "text": "What follows?\n\nSeminar today:\n\nIntro to Workflows and Stata\n\nWeek 2: Hypothesis testing\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#background-1",
    "href": "2023/weeks/week01/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Background",
    "text": "Background\n\nThink about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#code-and-software",
    "href": "2023/weeks/week01/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Code and Software",
    "text": "Code and Software\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week01/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week01/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week01/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Anna Karenina Principle",
    "text": "Anna Karenina Principle\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week01/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "What do we learn?",
    "text": "What do we learn?\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#workflow-1",
    "href": "2023/weeks/week01/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week01/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#checklist",
    "href": "2023/weeks/week01/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week01/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week01/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week01/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week01/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week01/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week01/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week01/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week01/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week01/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week01/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week01/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week01/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#data-generating-process-1",
    "href": "2023/weeks/week01/slides.html#data-generating-process-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nAnother example is a model of supply and demand\nWe observe prices and quantities in a competitive market\nWhat led to those being the prices and quantities we see?\nThe supply and demand model and its equilibrium, we theorize!"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#data-generating-process-2",
    "href": "2023/weeks/week01/slides.html#data-generating-process-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#data-generating-process-3",
    "href": "2023/weeks/week01/slides.html#data-generating-process-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nThe prices that we observe come from that theoretical construct\nWhen we see the prices and quantities moving, according to our theory, it’s because the S and D lines are moving\nBut we can’t see the S and D lines\nOur goal: use the observations we do see to infer what the theoretical model (data generating process) is"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#data-generating-process-4",
    "href": "2023/weeks/week01/slides.html#data-generating-process-4",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nHarder than it sounds. What inference about S and D can we draw from these observations?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-1",
    "href": "2023/weeks/week01/slides.html#causality-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nWe can also represent this set of relationships as a graph, with arrows telling you what variables cause each other"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-2",
    "href": "2023/weeks/week01/slides.html#causality-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nWe do this because most of the relationships we’re interested in are causal - we want to know, if we could reach in and manipulate \\(X\\), would \\(Y\\) change as a result, and how much?\nDoes the minimum wage reduce employment?\nDoes quantitative easing avert recessions?\nDoes six-sigma improve business performance?\nDoes getting an MBA make you a better manager?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-3",
    "href": "2023/weeks/week01/slides.html#causality-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nImagine this is the graph we see for minimum wage and employment"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-4",
    "href": "2023/weeks/week01/slides.html#causality-4",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nDoes that mean that the minimum wage harms employment?\nMaybe! But also maybe not\nWhat the graph shows us is a correlation\nAnd correlation is not the same thing as causation"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-5",
    "href": "2023/weeks/week01/slides.html#causality-5",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nA given correlation, like the negative relationship between minimum wage changes and employment changes, can be consistent with a number of different causal relationships\nAs econometricians, we need to figure out which one it is!\nHow can we narrow it down?\nHow many of the diagrams on the next page can be consistent with that negative relationship?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#eight-possible-relationships",
    "href": "2023/weeks/week01/slides.html#eight-possible-relationships",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Eight Possible Relationships",
    "text": "Eight Possible Relationships"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-6",
    "href": "2023/weeks/week01/slides.html#causality-6",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nThe only ones we can eliminate are d, g, and h\nAll the rest are possible!\nIf f is correct, we see the negative relationship even though minimum wage has nothing to do with causing employment (like the ice cream and shorts example)\nIf a is correct, then even though we know minimum wage causes employment to change, the size or even direction of the relationship will be wrong (why?)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-7",
    "href": "2023/weeks/week01/slides.html#causality-7",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nSo which of them is likely to be correct?\nThat depends on what we think \\(\\varepsilon\\) is\n\\(\\varepsilon\\) is everything that determines \\(Y\\) other than \\(X\\)\nPerhaps the health of the economy, or the policies that area has chosen\nSo we almost certainly have a graph with \\(\\varepsilon \\rightarrow Y\\)\nDo those things also affect the choice to raise the minimum wage? If so we’re in graph a. That downward relationship could be due to a null relationship, or even a positive one (or perhaps a more negative one?)"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html",
    "href": "2023/weeks/week09/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#background-1",
    "href": "2023/weeks/week09/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#code-and-software",
    "href": "2023/weeks/week09/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week09/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week09/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week09/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week09/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#workflow-1",
    "href": "2023/weeks/week09/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week09/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#checklist",
    "href": "2023/weeks/week09/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week09/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week09/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week09/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week09/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week09/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week09/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week09/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week09/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week09/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week09/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week09/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week09/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html",
    "href": "2023/weeks/week08/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#background-1",
    "href": "2023/weeks/week08/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#code-and-software",
    "href": "2023/weeks/week08/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week08/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week08/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week08/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week08/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#workflow-1",
    "href": "2023/weeks/week08/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week08/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#checklist",
    "href": "2023/weeks/week08/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week08/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week08/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week08/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week08/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week08/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week08/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week08/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week08/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week08/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week08/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week08/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week08/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html",
    "href": "2023/weeks/week07/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#background-1",
    "href": "2023/weeks/week07/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#code-and-software",
    "href": "2023/weeks/week07/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week07/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week07/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week07/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week07/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#workflow-1",
    "href": "2023/weeks/week07/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week07/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#checklist",
    "href": "2023/weeks/week07/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week07/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week07/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week07/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week07/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week07/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week07/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week07/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week07/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week07/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week07/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week07/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week07/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html",
    "href": "2023/weeks/week06/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#background-1",
    "href": "2023/weeks/week06/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#code-and-software",
    "href": "2023/weeks/week06/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week06/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week06/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week06/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week06/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#workflow-1",
    "href": "2023/weeks/week06/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week06/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#checklist",
    "href": "2023/weeks/week06/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week06/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week06/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week06/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week06/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week06/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week06/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week06/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week06/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week06/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week06/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week06/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week06/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html",
    "href": "2023/weeks/week05/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#background-1",
    "href": "2023/weeks/week05/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#code-and-software",
    "href": "2023/weeks/week05/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week05/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week05/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week05/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week05/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#workflow-1",
    "href": "2023/weeks/week05/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week05/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#checklist",
    "href": "2023/weeks/week05/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week05/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week05/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week05/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week05/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week05/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week05/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week05/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week05/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week05/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week05/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week05/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week05/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html",
    "href": "2023/weeks/week04/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#background-1",
    "href": "2023/weeks/week04/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#code-and-software",
    "href": "2023/weeks/week04/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week04/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week04/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week04/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week04/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#workflow-1",
    "href": "2023/weeks/week04/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week04/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#checklist",
    "href": "2023/weeks/week04/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week04/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week04/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week04/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week04/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week04/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week04/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week04/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week04/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week04/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week04/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week04/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week04/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html",
    "href": "2023/weeks/week03/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#background-1",
    "href": "2023/weeks/week03/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#code-and-software",
    "href": "2023/weeks/week03/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week03/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week03/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week03/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week03/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#workflow-1",
    "href": "2023/weeks/week03/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week03/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#checklist",
    "href": "2023/weeks/week03/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week03/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week03/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week03/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week03/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week03/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week03/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week03/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week03/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week03/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week03/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week03/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week03/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html",
    "href": "2023/weeks/week02/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#background-1",
    "href": "2023/weeks/week02/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#code-and-software",
    "href": "2023/weeks/week02/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week02/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week02/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week02/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week02/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#workflow-1",
    "href": "2023/weeks/week02/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week02/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#checklist",
    "href": "2023/weeks/week02/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week02/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week02/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week02/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week02/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week02/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week02/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week02/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week02/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week02/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week02/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week02/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week02/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week02/page1.html",
    "href": "2023/weeks/week02/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week02/page1.html#seminar-slides",
    "href": "2023/weeks/week02/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week02/page1.html#communication",
    "href": "2023/weeks/week02/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week03/page1.html",
    "href": "2023/weeks/week03/page1.html",
    "title": "💻 Seminar 3 - Linear Regressions / OLS",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week03/page1.html#seminar-slides",
    "href": "2023/weeks/week03/page1.html#seminar-slides",
    "title": "💻 Seminar 3 - Linear Regressions / OLS",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week03/page1.html#communication",
    "href": "2023/weeks/week03/page1.html#communication",
    "title": "💻 Seminar 3 - Linear Regressions / OLS",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week04/page1.html",
    "href": "2023/weeks/week04/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week04/page1.html#seminar-slides",
    "href": "2023/weeks/week04/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week04/page1.html#communication",
    "href": "2023/weeks/week04/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week05/page1.html",
    "href": "2023/weeks/week05/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week05/page1.html#seminar-slides",
    "href": "2023/weeks/week05/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week05/page1.html#communication",
    "href": "2023/weeks/week05/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week06/page1.html",
    "href": "2023/weeks/week06/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week06/page1.html#seminar-slides",
    "href": "2023/weeks/week06/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week06/page1.html#communication",
    "href": "2023/weeks/week06/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week07/page1.html",
    "href": "2023/weeks/week07/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week07/page1.html#seminar-slides",
    "href": "2023/weeks/week07/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week07/page1.html#communication",
    "href": "2023/weeks/week07/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week08/page1.html",
    "href": "2023/weeks/week08/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week08/page1.html#seminar-slides",
    "href": "2023/weeks/week08/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week08/page1.html#communication",
    "href": "2023/weeks/week08/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week09/page1.html",
    "href": "2023/weeks/week09/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week09/page1.html#seminar-slides",
    "href": "2023/weeks/week09/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week09/page1.html#communication",
    "href": "2023/weeks/week09/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#identification-error-1",
    "href": "2023/weeks/week02/slides.html#identification-error-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Identification Error",
    "text": "Identification Error\n\nAnother reason why we might see that result, i.e. if the same result could give us a different conclusion, like kids who are aggressive play more video games or people notice aggression more when kids play video games, then\nwe have made an identification error* - our result was not identified!*\nIdentification error is when your result in the data doesn’t actually have a clear theory (“why” or “because”)\nFor example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-generating-process-2",
    "href": "2023/weeks/week02/slides.html#data-generating-process-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-generating-process-3",
    "href": "2023/weeks/week02/slides.html#data-generating-process-3",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nThe prices that we observe come from that theoretical construct\nWhen we see the prices and quantities moving, according to our theory, it is because the S and D lines are moving\nBut we cannot see the S and D lines\nOur goal: use the observations we do see to infer what the theoretical model (data generating process) is"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-generating-process-4",
    "href": "2023/weeks/week02/slides.html#data-generating-process-4",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nHarder than it sounds. What inference about S and D can we draw from these observations?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality",
    "href": "2023/weeks/week02/slides.html#causality",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Causality",
    "text": "Causality\n\nA data generating process can be described by a series of equations that describe where the data comes from. For example:\n\n\\[ X = \\gamma_0 + \\gamma_1\\varepsilon + \\nu \\]\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nThis says ” \\(X\\) is caused by \\(\\varepsilon\\) and \\(\\nu\\), and \\(Y\\) is caused by \\(X\\) and \\(\\varepsilon\\)”\nThe truth is that an increase in \\(X\\) causally increases \\(Y\\) by \\(\\beta_1\\)\nThe goal of econometrics is to be able to estimate what \\(\\beta_1\\) is accurately"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-1",
    "href": "2023/weeks/week02/slides.html#causality-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Causality",
    "text": "Causality\n\nWe can also represent this set of relationships as a graph, with arrows telling you what variables cause each other"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-2",
    "href": "2023/weeks/week02/slides.html#causality-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Causality",
    "text": "Causality\n\nWe do this because most of the relationships we’re interested in are causal - we want to know, if we could reach in and manipulate \\(X\\), would \\(Y\\) change as a result, and how much?\nDoes the minimum wage reduce employment?\nDoes quantitative easing avert recessions?\nDoes six-sigma improve business performance?\nDoes getting an MBA make you a better manager?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-3",
    "href": "2023/weeks/week02/slides.html#causality-3",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Causality",
    "text": "Causality\n\nImagine this is the graph we see for minimum wage and employment"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-5",
    "href": "2023/weeks/week02/slides.html#causality-5",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Causality",
    "text": "Causality\n\nA given correlation, like the negative relationship between minimum wage changes and employment changes, can be consistent with a number of different causal relationships\nAs econometricians, we need to figure out which one it is!\nHow can we narrow it down?\nHow many of the diagrams on the next page can be consistent with that negative relationship?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#eight-possible-relationships",
    "href": "2023/weeks/week02/slides.html#eight-possible-relationships",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Eight Possible Relationships",
    "text": "Eight Possible Relationships"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-6",
    "href": "2023/weeks/week02/slides.html#causality-6",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Causality",
    "text": "Causality\n\nThe only ones we can eliminate are d, g, and h\nAll the rest are possible!\nIf f is correct, we see the negative relationship even though minimum wage has nothing to do with causing employment (like the ice cream and shorts example)\nIf a is correct, then even though we know minimum wage causes employment to change, the size or even direction of the relationship will be wrong (why?)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-7",
    "href": "2023/weeks/week02/slides.html#causality-7",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Causality",
    "text": "Causality\n\nSo which of them is likely to be correct?\nThat depends on what we think \\(\\varepsilon\\) is\n\\(\\varepsilon\\) is everything that determines \\(Y\\) other than \\(X\\)\nPerhaps the health of the economy, or the policies that area has chosen\nSo we almost certainly have a graph with \\(\\varepsilon \\rightarrow Y\\)\nDo those things also affect the choice to raise the minimum wage? If so we’re in graph a. That downward relationship could be due to a null relationship, or even a positive one (or perhaps a more negative one?)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#endogeneity",
    "href": "2023/weeks/week02/slides.html#endogeneity",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Endogeneity",
    "text": "Endogeneity\n\nSo “correlation isn’t causation” isn’t quite complete\nIt’s more “only certain correlations are causal”\nMany correlations are beset by these problems like endogeneity, i.e. the presence of another variable like \\(\\varepsilon\\) related to both \\(X\\) and \\(Y\\), giving the effect a “back door”\nSo the correlation reflects both the causal effect and also the influence of \\(\\varepsilon\\)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#random-experiments-1",
    "href": "2023/weeks/week02/slides.html#random-experiments-1",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Random Experiments",
    "text": "Random Experiments\n\nFor this reason, random experiments are generally considered the “gold standard”\nAlthough they have their own problems, of course (your experimental sample might not represent the population well, there are plenty of statistical mistakes to make, people may act differently knowing they’re in an experiment, etc. etc.)\nBut regardless, we’re looking here at questions for which we can’t run an experiment, becuase it’s impossible or infeasible or immoral\nSo one way we can think about solving this endogeneity problem with econometrics is to use our observational data in such a way that it behaves as though there were an experiment being run\nPlenty of ways to do this we’ll go over in this course!"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#concept-check",
    "href": "2023/weeks/week02/slides.html#concept-check",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat does it mean to say that \\(X\\) has a causal effect on \\(Y\\)?\nWhy might the relationship between \\(X\\) and \\(Y\\) in data not be the same as the causal effect?\nWhat is an example of observational data?\nConsider the question of “Does getting an MBA make you a better manager?” What are \\(X\\) and \\(Y\\) here? What would be in the error term \\(\\varepsilon\\)? Are we likely to have an endogeneity problem here?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#spurious-correlations",
    "href": "2023/weeks/week02/slides.html#spurious-correlations",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Spurious Correlations",
    "text": "Spurious Correlations\n\nLet’s visit this site all about “spurious” correlations (i.e. correlations that almost certainly do not reveal a true effect of one variable on the other): https://tylervigen.com/spurious-correlations\nTake a look at how easy it is to find variables that are related statistically, even though clearly neither causes the other\nDo you think this correlation is an example of inferential error (just random chance) or identification error (truly related, but not because one causes the other)? Why?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-generating-process-1",
    "href": "2023/weeks/week02/slides.html#data-generating-process-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nAnother example is a model of supply and demand\nWe observe prices and quantities in a competitive market\nWhat led to those being the prices and quantities we see?\nThe supply and demand model and its equilibrium, we theorize!"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#x-and-y",
    "href": "2023/weeks/week02/slides.html#x-and-y",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "\\(X\\) and \\(Y\\)",
    "text": "\\(X\\) and \\(Y\\)\n\nI have an \\(X\\) value of 2.5 and want to predict what \\(Y\\) will be. What can I do?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#x-and-y-1",
    "href": "2023/weeks/week02/slides.html#x-and-y-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "\\(X\\) and \\(Y\\)",
    "text": "\\(X\\) and \\(Y\\)\n\nI can’t just say “just predict whatever values of \\(Y\\) we see for \\(X = 2.5\\), because there are multiple of those!\nPlus, what if we want to predict for a value we DON’T have any actual observations of, like \\(X = 4.3\\)?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-is-granular",
    "href": "2023/weeks/week02/slides.html#data-is-granular",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data is Granular",
    "text": "Data is Granular\n\nIf I try to fit every point, I’ll get a mess that won’t really tell me the relationship between \\(X\\) and \\(Y\\)\nSo, we simplify the relationship into a shape: a line! The line smooths out those three points around 2.5 and fills in that gap around 4.3"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#isnt-this-worse",
    "href": "2023/weeks/week02/slides.html#isnt-this-worse",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Isn’t This Worse?",
    "text": "Isn’t This Worse?\n\nBy adding a line, we are necessarily simplifying our presentation of the data. We’re tossing out information!\nOur prediction of the data we have will be less accurate than if we just make predictions point-by-point\nHowever, we’ll do a better job predicting other data (avoiding “overfitting”)\nAnd, since a shape is something we can interpret, as opposed to a long list of predictions, which we can’t really, the line will do a better job of telling us about the true underlying relationship"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#the-line-does-a-few-things",
    "href": "2023/weeks/week02/slides.html#the-line-does-a-few-things",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "The Line Does a Few Things:",
    "text": "The Line Does a Few Things:\n\nWe can get a prediction of \\(Y\\) for a given value of \\(X\\) (If we follow \\(X = 2.5\\) up to our line we get \\(Y = 7.6\\))\nWe see the relationship: the line slopes up, telling us that “more \\(X\\) means more \\(Y\\) too!”"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#lines",
    "href": "2023/weeks/week02/slides.html#lines",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nThat line we get is the fit of our model\nA model “fit” means we’ve taken a shape (our line) and picked the one that best fits our data\nAll forms of regression do this\nOrdinary least squares specifically uses a straight line as its shape\nThe resulting line we get can also be written out as an actual line, i.e.\n\n\\[ Y = intercept + slope*X \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#lines-1",
    "href": "2023/weeks/week02/slides.html#lines-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nWe can use that line as… a line!\nIf we plug in a value of \\(X\\), we get a prediction for \\(Y\\)\nBecause these \\(Y\\) values are predictions, we’ll give them a hat \\(\\hat{Y}\\)\n\n\\[ Y = 3 + 4*X \\]\n\\[ \\hat{Y} = 3 + 4*(3.2) \\]\n\\[ \\hat{Y} = 15.8 \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#lines-2",
    "href": "2023/weeks/week02/slides.html#lines-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nWe can also use it to explain the relationship\nWhatever the intercept is, that’s what we predict for \\(Y\\) when \\(X = 0\\)\n\n\\[ Y = 3 + 4*X \\]\n\\[ \\hat{Y} = 3 + 4*0 \\]\n\\[ \\hat{Y} = 3 \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#lines-3",
    "href": "2023/weeks/week02/slides.html#lines-3",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nAnd as \\(X\\) increases, we know how much we expect \\(Y\\) to increase because of the slope\n\n\\[ Y = 3 + 4*X \\]\n\\[ \\hat{Y} = 3 + 4*3 = 15 \\]\n\\[ \\hat{Y} = 3 + 4*4 = 19 \\]\n\nWhen \\(X\\) increases by \\(1\\), \\(Y\\) increases by the slope (which is \\(4\\) here)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nRegression fits a shape to the data\nOrdinary least squares specifically fits a straight line to the data\nThe straight line is described using an \\(intercept\\) and a \\(slope\\)\nWhen we plug an \\(X\\) into the line, we get a prediction for \\(Y\\), which we call \\(\\hat{Y}\\)\nWhen \\(X = 0\\), we predict \\(\\hat{Y} = intercept\\)\nWhen \\(X\\) increases by \\(1\\), our prediction of \\(Y\\) increases by the \\(slope\\)\nIf \\(slope &gt; 0\\), \\(X\\) and \\(Y\\) are positively related/correlated\nIf \\(slope &lt; 0\\), \\(X\\) and \\(Y\\) are negatively related/correlated"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#concept-checks",
    "href": "2023/weeks/week02/slides.html#concept-checks",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nHow does producing a line let us use \\(X\\) to predict \\(Y\\)?\nIf our line is \\(Y = 5 - 2*X\\), explain what the \\(-2\\) means in a sentence\nNot all of the points are exactly on the line, meaning some of our predictions will be wrong! Should we be concerned? Why or why not?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#how",
    "href": "2023/weeks/week02/slides.html#how",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "How?",
    "text": "How?\n\nWe know that regression fits a line\nBut how does it do that exactly?\nIt picks the line that produces the smallest squares\nThus, “ordinary least squares”"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#predictions-and-residuals",
    "href": "2023/weeks/week02/slides.html#predictions-and-residuals",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Predictions and Residuals",
    "text": "Predictions and Residuals\n\nWhenever you make a prediction of any kind, you rarely get it exactly right\nThe difference between your prediction and the actual data is the residual\n\n\\[ Y = 3 + 4*X \\]\nIf we have a data point where \\(X = 4\\) and \\(Y = 18\\), then\n\\[ \\hat{Y} = 3 + 4*4 = 19 \\]\nThen the residual is \\(Y - \\hat{Y} = 18 - 19 = -1\\)."
  },
  {
    "objectID": "2023/weeks/week02/slides.html#predictions-and-residuals-1",
    "href": "2023/weeks/week02/slides.html#predictions-and-residuals-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Predictions and Residuals",
    "text": "Predictions and Residuals\nSo really, our relationship doesn’t look like this…\n\\[ Y = intercept + slope*X \\]\nInstead, it’s…\n\\[ Y = intercept + slope*X + residual \\]\nWe still use \\(intercept + slope*X\\) to predict \\(Y\\) though, so this is also\n\\[ Y = \\hat{Y} + residual \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-1",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nAs you’d guess, a good prediction should make the residuals as small as possible\nWe want to pick a line to do that\nAnd in particular, we’re going to square those residuals, so the really-big residuals count even more. We really don’t want to have points that are super far away from the line!\nThen, we pick a line to minimize those squared residuals (“least squares”)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-2",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nStart with our data"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-3",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-3",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nLet’s just pick a line at random, not necessarily from OLS"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-4",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-4",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nThe vertical distance from point to line is the residual"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-5",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-5",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nNow square those residuals"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-6",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-6",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nCan we get the total area in the squares smaller with a different line?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-7",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-7",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nOrdinary Least Squares, I can promise you, gets it the smallest"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-8",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-8",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nHow does it figure out which line makes the smallest squares?\nThere’s a mathematical formula for that!\nFirst, instead of thinking of \\(intercept\\) and \\(slope\\), we reframe the line as having parameters we can pick\n\n\\[ Y = intercept + slope*X + residual \\]\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#terminology-sidenote",
    "href": "2023/weeks/week02/slides.html#terminology-sidenote",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Terminology Sidenote",
    "text": "Terminology Sidenote\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nIn metrics, Greek letters represent “the truth” - in the true process by which the data is generated, a one-unit increase in \\(X\\) is related to a \\(\\beta_1\\) increase in \\(Y\\)\nWhen we put a hat on anything, that is our prediction or estimation of that true thing. \\(\\hat{Y}\\) is our prediction of \\(Y\\), and \\(\\hat{\\beta_1}\\) is our estimate of what we think the true \\(\\beta_1\\) is\nNote “residual” =/= \\(\\varepsilon\\) - residuals are what’s actually left over from our prediction with real data, but the error \\(\\varepsilon\\) is the true difference between our line and \\(Y\\)."
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-9",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-9",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nNow that we have our line in parametric terms, we can pick our estimates of \\(\\beta_0\\) and \\(\\beta_1\\) in order to make the squared residuals as small as possible\nPick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize:\n\n\\[ \\sum_i (residual_i^2) \\]\n\\[ \\sum_i ((Y_i - \\hat{Y})^2) \\]\n\\[ \\sum_i ((Y_i - \\hat{\\beta_0} - \\hat{\\beta_1}X_i)^2) \\]\nWhere the \\(_i\\) refers to a particular observation. \\(\\sum_i\\) means “sum this up over all the observations”\n(Conveniently, you can pick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize that expression with basic calculus)"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#truth-and-reality-1",
    "href": "2023/weeks/week03/slides.html#truth-and-reality-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Truth and Reality",
    "text": "Truth and Reality\n\nToday we’ll be covering hypothesis testing, which is one approach to using reality to get closer to the truth\nIt works by subtraction\nWe test whether certain versions of the truth are likely or unlikely\nAnd if we find that they’re unlikely, we can reject that version of the truth, narrowing down what the actual possibilities are and getting closer and closer to the actual truth"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#truth-and-reality-2",
    "href": "2023/weeks/week03/slides.html#truth-and-reality-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Truth and Reality",
    "text": "Truth and Reality\n\nWhen we’re talking about the truth here, we’re referring to the true data generating process (DGP)\nFor example, if this is the true DGP:\n\n\\[ Wage_i = \\beta_0 + \\beta_1AdultHeight_i + \\varepsilon_i \\]\nwhere \\(cor(AdultHeight_i,\\varepsilon_i) = 0\\), then…\n\nPerson \\(i\\)’s wage is truly determined by a linear function of your height, plus an unrelated error term \\(\\varepsilon_i\\)\nWhy might someone have a high wage? Either they’re tall, or they have a high error term, or both. No other way!"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#truth-and-reality-3",
    "href": "2023/weeks/week03/slides.html#truth-and-reality-3",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Truth and Reality",
    "text": "Truth and Reality\n\nOur ability to estimate that true model depends on our ability to avoid inference and identification error\nIf we assume that’s the true DGP, there’s no endogeneity, and the relationship between \\(Wage\\) and \\(AdultHeight\\) is a straight line, so regular ’ol OLS of \\(Wage\\) on \\(AdultHeight\\) will not give us identification error\nBut we also need to be careful about inference error\nWhen we run that regression, what does our \\(\\hat{\\beta}_1\\) say about the true value \\(\\beta_1\\)?"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#sampling-variation-1",
    "href": "2023/weeks/week03/slides.html#sampling-variation-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Sampling Variation",
    "text": "Sampling Variation\n\nNotice that there is plenty of variation around the true value of \\(2\\)\nNow let’s imagine we don’t know that and are trying to answer the question “is the truth \\(\\beta_1 = 2\\)?”\nWe don’t have the full sampling distribution, we just have a single estimate:\n\n\n\n(Intercept)           X \n   2.798554    2.215880 \n\n\n\nAll we see is \\(\\hat{\\beta}_1 =\\) 2.22. So… is \\(\\beta_1 = 2\\)?"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#null-distribution",
    "href": "2023/weeks/week03/slides.html#null-distribution",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nThe “null distribution” is what the sampling distribution of the estimator would be if our null distribution were true\nWe can see that in the sampling distribution we have!\n\\(\\beta_1 = 2\\) is true, and here’s what the sampling distribution looks like! (although it would be smoother with more samples)"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#null-distribution-1",
    "href": "2023/weeks/week03/slides.html#null-distribution-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nSo the key question that a hypothesis test asks is: given this null distribution, how unlikely is it that we get the result we get?\nIf it’s super unlikely that the null is true and we get our result, well…\nWe definitely got our result…\nSo the null must be the part that’s wrong!\nThat’s when we reject the null - we find that the sampling distribution under the null hardly ever produces a result like ours, so that’s probably the wrong sampling distribution and thus the wrong null!"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#null-distribution-2",
    "href": "2023/weeks/week03/slides.html#null-distribution-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nHow does this work out with our estimate of 2.22?\nLet’s stick it on the graph"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#hypothesis-test",
    "href": "2023/weeks/week03/slides.html#hypothesis-test",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\nOur test comes down to: how weird would it be to get a result this far from the “truth” or farther?\nWe can figure this out by shading in the parts of the null distribution this far from the null truth or farther\nSo we shade 2.22 and above, and also 2 - abs(2.22 - 2) = 1.78 and below."
  },
  {
    "objectID": "2023/weeks/week03/slides.html#hypothesis-test-1",
    "href": "2023/weeks/week03/slides.html#hypothesis-test-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\nBased off of this sampling distribution, there’s a 31% + 26% = 57% chance of getting something as weird as we got or weirder (or for a one-tailed test, a 26% chance of getting something that high or higher) if the null of \\(\\beta_1 = 2\\) is true\nThat’s not too unlikely! So, we would fail to reject the null of \\(\\beta_1 = 2\\)\nThis doesn’t mean that we conclude that \\(\\beta_1 = 2\\) is true, it just means we can’t rule it out"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#the-null-distribution",
    "href": "2023/weeks/week03/slides.html#the-null-distribution",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\nOf course, we generated this null distribution by just randomly creating a few random samples\nWe also happen to know that if we had infinite samples, the sampling distribution of OLS would be a normal distribution with the mean at the true value and the standard deviation determined by \\(var(X)\\), the variance of the residual, and the sample size. The real null distribution looks like this:"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#the-null-distribution-1",
    "href": "2023/weeks/week03/slides.html#the-null-distribution-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\nSo with the estimate we made from the sample we got (2.22), we can’t reject a null \\(\\beta_1 = 2\\)\nWhich is good!! That’s the truth. We don’t want to reject it!\nHow about other nulls? Can we reject those?\nCan we reject a null that \\(\\beta_1 = 0\\)?\n(by default, most null hypotheses are that the parameter is 0)\nLet’s follow the same steps!"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#the-null-distribution-2",
    "href": "2023/weeks/week03/slides.html#the-null-distribution-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\nNow that’s unlikely. We can reject that the true value is 0."
  },
  {
    "objectID": "2023/weeks/week03/slides.html#p-values-1",
    "href": "2023/weeks/week03/slides.html#p-values-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "p-values",
    "text": "p-values\n\nThe lower the p-value, the less likely it is that we got our result AND the null is true\nAnd since we definitely got our result, a really low p-value says we should reject the null\nHow low does it need to be to reject the null?\nWell…"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#p-values-and-.05",
    "href": "2023/weeks/week03/slides.html#p-values-and-.05",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "p-values and .05",
    "text": "p-values and .05\n\nIt’s common practice to decide on a confidence level and a corresponding \\(\\alpha\\), most commonly a 95% confidence level \\(\\rightarrow \\alpha = .05\\), and reject the null if the p-value is lower than \\(\\alpha\\)\nWhy 95%? Completely arbitrary. Someone picked it out of thin air a hundred years ago as a just-for-instance and we still use it 🤷\nHaving a hard-and-fast threshold like this is not a great idea ( \\(p=.04\\) is rejection, but \\(p=.06\\) is not?), but it’s a very hard habit to break\nKey takeaway: get familiar with the concept of rejecting the null when the p-value is lower than .05, because you’ll see it\nBUT don’t get too hung up on black-and-white rejection in general"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#power-1",
    "href": "2023/weeks/week03/slides.html#power-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Power",
    "text": "Power\n\nBecause the smaller we make our confidence level, the less likely we are to reject the null in general\nWhich means we’ll also fail to reject it if it’s actually false (a “false negative”)\nWe want a low false-positive rate, but also a low false-negative rate\nThe false negative rate is called “power.” If we will reject the null 80% of the time when it’s actually false, we have 80% power\nAs \\(\\alpha\\) shrinks, false-positive rates decline, but false-negative rates increase\nStrike a balance!\n\n(minor sidenote: “false positive” and “false negative” are sometimes referred to as “Type I Error” and “Type II Error” - these are not great terms because they are hard to remember! If you encounter them, just remember that in “The boy who cried wolf” the townspeople think there’s a wolf when there’s not, then think there’s not a wolf when there is, committing Type I and II error in that order)"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#what-we-just-did",
    "href": "2023/weeks/week03/slides.html#what-we-just-did",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "What we just did…",
    "text": "What we just did…\n\nThis is how we thought of it - we picked a null, figured out the null distribution (the sampling distribution of the estimator assuming the null was true), and checked if our estimate was unlikely enough that we could reject the null"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#instead",
    "href": "2023/weeks/week03/slides.html#instead",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Instead…",
    "text": "Instead…\n\nBy thinking about standard errors, we instead center the sampling distribution around our estimate. If the null is far away, we reject that null (result should be the same)!"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#confidence-intervals",
    "href": "2023/weeks/week03/slides.html#confidence-intervals",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nSo what we’re thinking now is not “is our estimate close to the null?” but rather “is that null close to our estimate?”\nWe can go one step further and ask “which nulls are close to our estimate?” and figure out which nulls we can think about rejecting from there\nA confidence interval shows the range of nulls that would not be rejected by our estimate\nEverything outside that range can be rejected"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#confidence-intervals-1",
    "href": "2023/weeks/week03/slides.html#confidence-intervals-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThis takes the form of\n\n\\[ \\hat{\\beta}_1 \\pm Z(s.e.) \\]\n\nWhere \\(Z\\) is some value from our distribution that gives us the \\(1-\\alpha\\) percentile (for a 95% confidence interval with a normal sampling distribution, \\(Z = 1.96\\))\nand \\(s.e.\\) is the standard error of \\(\\hat{\\beta}_1\\)"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#confidence-intervals-2",
    "href": "2023/weeks/week03/slides.html#confidence-intervals-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThinking back to our estimate:\n\n\n\n\n\n\n\nModel 1\n\n\n(Intercept)\n2.80 \n\n\n\n(0.21)\n\n\nX\n2.22 \n\n\n\n(0.37)\n\n\nN\n100    \n\n\n\n\n\n\n\n\nOur estimate of the coefficient is 2.22, and our estimate of the standard error is 0.37\nSo for a 95% confidence interval, assuming normality, we get 2.22 \\(\\pm\\) 1.96* 0.37, or [1.49,2.94]"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#confidence-intervals-3",
    "href": "2023/weeks/week03/slides.html#confidence-intervals-3",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nWe can see this graphically as well - we should only reject nulls in those 5% tails for a 95% confidence interval"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#concept-checks-2",
    "href": "2023/weeks/week03/slides.html#concept-checks-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhat feature of the sampling distribution of the \\(\\beta\\) does the standard error describe?\nWhy do we get the same reject/don’t reject result if we center the sampling distribution around the null as around our estimate?\nWe perform an estimate of \\(\\hat{\\beta}_1\\) and get a 95% confidence interval of [-1.3, 2.1]. Describe what this means in a sentence.\nIn the above confidence interval, can we reject the null of \\(\\beta_1 = 0\\)?\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#ols-1",
    "href": "2023/weeks/week03/slides_sem.html#ols-1",
    "title": "💻 Seminar 3 Hypothesis testing & OLS",
    "section": "OLS",
    "text": "OLS\n\nDuring this seminar:\n\nWe will go through Seminar 2 & 3 exercises.\nPlease conduct your analyses on your do-files and keep a log-file.\nSee how we can run OLS on STATA and test hypotheses"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#tasks",
    "href": "2023/weeks/week03/slides_sem.html#tasks",
    "title": "💻 Seminar 3 Hypothesis testing & OLS",
    "section": "Tasks",
    "text": "Tasks\n\nRegress education on yearsexp. Interpret the coefficient. What does the result tell us?\nCreate a set of dummy variables to capture the different levels of education on yearsexp."
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#tasks-1",
    "href": "2023/weeks/week03/slides_sem.html#tasks-1",
    "title": "💻 Seminar 3 Hypothesis testing & OLS",
    "section": "Tasks",
    "text": "Tasks\n\nTest the null hypothesis that there is no effect of earning a college degree on yearsexp using an OLS regression model. State the alternate hypothesis.\n\nInterpret the estimated coefficient.\nBased on the results, do you reject the null hypothesis? Explain using t-statistic, p-values and confidence intervals.\nWhat is the prediction for an applicant who has not earned a a college degree?\nWhat is the prediction for an applicant who has earned a a college degree?"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#tasks-2",
    "href": "2023/weeks/week03/slides_sem.html#tasks-2",
    "title": "💻 Seminar 3 Hypothesis testing & OLS",
    "section": "Tasks",
    "text": "Tasks\n\nTest the null hypothesis that there is no effect of earning a college degree on yearsexp using an OLS regression model using robust Standard Errors. Do the coefficient values change?\nTest the null hypothesis that there is no effect of earning a college degree on yearsexp using ANOVA.\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#the-right-hand-side-1",
    "href": "2023/weeks/week05/slides.html#the-right-hand-side-1",
    "title": "Binary Variables and Functional Form",
    "section": "The Right Hand Side",
    "text": "The Right Hand Side\nWe will look at three features of the right-hand side\n\nWhat if the variable is categorical or binary? (binary variables)\nWhat if the variable has a nonlinear effect on \\(Y\\) (polynomials and logarithms)\nWhat if the effect of one variable depends on the value of another variable? (interaction terms)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means",
    "href": "2023/weeks/week05/slides.html#comparison-of-means",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nWhen a binary variable is an independent variable, what we are often interested in doing is comparing means\nIs mean income higher inside the US or outside?\nIs mean height higher for kids who got a nutrition supplement or those who didn’t?\nIs mean GDP growth higher with or without a floating exchange rate?"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-1",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-1",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nLet’s compare log earnings in 1993 between married people 30 or older vs. never-married people 30 or older\nSeems to be a slight favor to the married men\n\n\ndata(PSID, package = 'Ecdat')\nPSID &lt;- PSID %&gt;%\n  filter(age &gt;= 30, married %in% c('married','never married'), earnings &gt; 0) %&gt;%\n  mutate(married  = married == 'married')\nPSID %&gt;%\n  group_by(married) %&gt;%\n  summarize(log_earnings = mean(log(earnings)))\n\n# A tibble: 2 × 2\n  married log_earnings\n  &lt;lgl&gt;          &lt;dbl&gt;\n1 FALSE           9.26\n2 TRUE            9.47"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-2",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-2",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-3",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-3",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nThe difference between the means follows a t-distribution under the null that they’re identical\nSo of course we can do a hypothesis test of whether they’re different\nBut why bother trotting out a specific test when we can just do a regression?\n(In fact, a lot of specific tests can be replaced with basic regression, see this explainer)\n\n\nfeols(log(earnings) ~ married, data = PSID)\n\nOLS estimation, Dep. Var.: log(earnings)\nObservations: 2,830 \nStandard-errors: IID \n            Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept) 9.255160   0.052891 174.98636  &lt; 2.2e-16 ***\nmarriedTRUE 0.212731   0.057760   3.68305 0.00023478 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.13028   Adj. R2: 0.004422"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-4",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-4",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nNotice:\n\nThe intercept gives the mean for the non-married group\nThe coefficient on marriedTRUE gives the married minus non-married difference\nThe t-stat and p-value on that coefficient are exactly the same as that t.test (except the t is reversed; same deal)\ni.e. the coefficient on a binary variable in a regression gives the difference in means\nIf we’d defined it the other way, with “not married” as the independent variable, the intercept would be the mean for the married group (i.e. “not married = 0”), and the coefficient would be the exact same but times \\(-1\\) (same difference, just opposite direction!)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-5",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-5",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nWhy does OLS give us a comparison of means when you give it a binary variable?\n\nThe only \\(X\\) values are 0 (FALSE) and 1 (TRUE)\nBecause of this, OLS no longer really fits a line, it’s more of two separate means\nAnd when you’re estimating to minimize the sum of squared errors separately for each group, can’t do any better than to predict the mean!\nSo you get the mean of each group as each group’s prediction"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#binary-with-controls",
    "href": "2023/weeks/week05/slides.html#binary-with-controls",
    "title": "Binary Variables and Functional Form",
    "section": "Binary with Controls",
    "text": "Binary with Controls\n\nObviously this is handy for including binary controls, but why do this for binary treatments? Because we can add controls!\n\n\n\n                feols(log(earning..\nDependent Var.:       log(earnings)\n                                   \nConstant          8.740*** (0.1478)\nmarriedTRUE      0.3404*** (0.0579)\nkids            -0.2259*** (0.0159)\nage              0.0223*** (0.0038)\n_______________ ___________________\nS.E. type                       IID\nObservations                  2,803\nR2                          0.07609\nAdj. R2                     0.07510\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#multicollinearity",
    "href": "2023/weeks/week05/slides.html#multicollinearity",
    "title": "Binary Variables and Functional Form",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nWhy is just one side of it on the regression? Why aren’t “married” and “not married” BOTH included?\nBecause regression couldn’t give an answer!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#multicollinearity-1",
    "href": "2023/weeks/week05/slides.html#multicollinearity-1",
    "title": "Binary Variables and Functional Form",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMean of married is \\(9.47\\) and of non-married is \\(9.26\\). \\[ \\log(Earnings) = 0 + 9.47Married + 9.26NonMarried \\] \\[ \\log(Earnings) = 3 + 6.47Married + 6.26NonMarried \\]\nThese (and infinitely many other options) all give the exact same predictions! OLS can’t pick between them. There’s no single best way to minimize squared residuals\nSo we pick one with convenient properties, setting one of the categories to have a coefficient of 0 (dropping it) and making the coefficient on the other the difference relative to the one we left out"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#more-than-two-categories",
    "href": "2023/weeks/week05/slides.html#more-than-two-categories",
    "title": "Binary Variables and Functional Form",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nThat interpretation - dropping one and making the other relative to that, conveniently extends to multi-category variables\nWhy stop at binary categorical variables? There are plenty of categorical variables with more than two values\nWhat is your education level? What is your religious denomination? What continent are you on?\nWe can put these in a regression by turning each value into its own binary variable\n(and then dropping one so the coefficients on the others give you the difference with the omitted one)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#more-than-two-categories-1",
    "href": "2023/weeks/week05/slides.html#more-than-two-categories-1",
    "title": "Binary Variables and Functional Form",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nMake the mean of group A be 1, of group B be 2, etc.\n\n\ntib &lt;- tibble(group = sample(LETTERS[1:4], 10000, replace = TRUE)) %&gt;%\n  mutate(Y = rnorm(10000) + (group == \"A\") + 2*(group == \"B\") + 3*(group == \"C\") + 4*(group == \"D\"))\nfeols(Y ~ group, data = tib)\n\nOLS estimation, Dep. Var.: Y\nObservations: 10,000 \nStandard-errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 0.992778   0.019805  50.1275 &lt; 2.2e-16 ***\ngroupB      0.983373   0.028318  34.7257 &lt; 2.2e-16 ***\ngroupC      2.015278   0.028268  71.2908 &lt; 2.2e-16 ***\ngroupD      2.985321   0.028117 106.1760 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.00187   Adj. R2: 0.557065"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#more-than-two-categories-2",
    "href": "2023/weeks/week05/slides.html#more-than-two-categories-2",
    "title": "Binary Variables and Functional Form",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nBy changing the reference group, the coefficients change because they’re “different from” a different group!\nAnd notice that, as before, the intercept is the mean of the omitted group (although this changes once you add controls; the intercept is the predicted mean when all right-hand-side variables are 0)\n\n\ntib &lt;- tib %&gt;% mutate(group = factor(group, levels = c('B','A','C','D')))\nfeols(Y ~ group, data = tib)\n\nOLS estimation, Dep. Var.: Y\nObservations: 10,000 \nStandard-errors: IID \n             Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept)  1.976151   0.020241  97.6327 &lt; 2.2e-16 ***\ngroupA      -0.983373   0.028318 -34.7257 &lt; 2.2e-16 ***\ngroupC       1.031905   0.028575  36.1118 &lt; 2.2e-16 ***\ngroupD       2.001948   0.028425  70.4286 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.00187   Adj. R2: 0.557065"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#more-than-two-categories-3",
    "href": "2023/weeks/week05/slides.html#more-than-two-categories-3",
    "title": "Binary Variables and Functional Form",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nSome Interpretations: Controlling for number of kids and age, people with a high school degree have log earnings .324 higher than those without a high school degree (earnings 32.4% higher). BA-holders have earnings 84.8% higher than those without a HS degree\nControlling for kids and age, a graduate degree earns (.976 - .848 =) 12.8% more than someone with a BA (glht() could help!)\n\n\n\n\nNo High School Degree    High School Degree          Some College \n                  333                  1105                   708 \n    Bachelor's Degree       Graduate Degree \n                  356                   301 \n\n\nOLS estimation, Dep. Var.: log(earnings)\nObservations: 2,803 \nStandard-errors: IID \n                             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept)                  8.360245   0.156293 53.49092  &lt; 2.2e-16 ***\neducationHigh School Degree  0.323807   0.067175  4.82036 1.5096e-06 ***\neducationSome College        0.576478   0.071976  8.00934 1.6758e-15 ***\neducationBachelor's Degree   0.848200   0.082697 10.25666  &lt; 2.2e-16 ***\neducationGraduate Degree     0.976291   0.086842 11.24212  &lt; 2.2e-16 ***\nkids                        -0.149679   0.015689 -9.54045  &lt; 2.2e-16 ***\nage                          0.023137   0.003741  6.18453 7.1388e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.05675   Adj. R2: 0.123473"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#concept-checks",
    "href": "2023/weeks/week05/slides.html#concept-checks",
    "title": "Binary Variables and Functional Form",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nIf \\(X\\) is binary, in sentences interpret the coefficients from the estimated OLS equation \\(Y = 4 + 3X + 2Z\\)\nHow might a comparison of means come in handy if you wanted to analyze the results of a randomized policy experiment?\nIf you had a data set of people from every continent and added “continent” as a control, how many coefficients would this add to your model?\nIf in that regression you really wanted to compare Europe to Asia specifically, what might you do so that the regression made this easy?"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interpreting-ols",
    "href": "2023/weeks/week05/slides.html#interpreting-ols",
    "title": "Binary Variables and Functional Form",
    "section": "Interpreting OLS",
    "text": "Interpreting OLS\n\nTo think more about the right-hand-side, let’s go back to our original interpretation of an OLS coefficient \\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\nA one-unit change in \\(X\\) is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\nThis logic still works with binary variables since “a one-unit change in \\(X\\)” means “changing \\(X\\) from No to Yes”\nNotice that this assumes that a one-unit change in \\(X\\) always has the same effect on \\(\\beta_1\\) no matter what else is going on\nWhat if that’s not true?"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#functional-form",
    "href": "2023/weeks/week05/slides.html#functional-form",
    "title": "Binary Variables and Functional Form",
    "section": "Functional Form",
    "text": "Functional Form\n\nWe talked before about times when a linear model like standard OLS might not be sufficient\nHowever, as long as those non-linearities are on the right hand side, we can fix the problem easily but just having \\(X\\) enter non-linearly! Run it through a transformation!\nThe most common transformations by far are polynomials and logarithms"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#functional-form-1",
    "href": "2023/weeks/week05/slides.html#functional-form-1",
    "title": "Binary Variables and Functional Form",
    "section": "Functional Form",
    "text": "Functional Form\n\nWhy do this? Because sometimes a straight line is clearly not going to do the trick!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials",
    "href": "2023/weeks/week05/slides.html#polynomials",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\n\\(\\beta_1X\\) is a “first order polynomial” - there’s one term\n\\(\\beta_1X + \\beta_2X^2\\) is a “second order polynomial” or a “quadratic” - two terms (note both included, it’s not just \\(X^2\\))\n\\(\\beta_1X + \\beta_2X^2 + \\beta_3X^3\\) is a third-order or cubic, etc."
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-1",
    "href": "2023/weeks/week05/slides.html#polynomials-1",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\nWhat do they do?\n\nThe more polynomial terms, the more flexible the line can be. With enough terms you can mimic any shape of relationship\nOf course, if you just add a whole buncha terms, it gets very noisy, and prediction out-of-sample gets very bad\nKeep it minimal - quadratics are almost always enough, unless you have reason to believe there’s a true more-complex relationship. You can try adding higher-order terms and see if they make a difference"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-2",
    "href": "2023/weeks/week05/slides.html#polynomials-2",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nThe true relationship is quadratic"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-3",
    "href": "2023/weeks/week05/slides.html#polynomials-3",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nHigher-order terms don’t do anything for us here (because a quadratic is sufficient!)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-in-r",
    "href": "2023/weeks/week05/slides.html#polynomials-in-r",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials in R",
    "text": "Polynomials in R\n\nWe can add an I() function to our regression to do a calculation on a variable before including it. So I(X^2) adds a squared term\nThere’s also a poly() function but avoid it - it does something slightly different\n\n\n# Linear\nfeols(Y ~ X, data = df)\n# Quadratic\nfeols(Y ~ X + I(X^2), data = df)\n# Cubic\nfeols(Y ~ X + I(X^2) + I(X^3), data = df)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#concept-check",
    "href": "2023/weeks/week05/slides.html#concept-check",
    "title": "Binary Variables and Functional Form",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat’s the effect of a one-unit change in \\(X\\) at \\(X = 0\\), \\(X = 1\\), and \\(X = 2\\) for each of these?\n\n\n\n                feols(Y ~ X, dat.. feols(Y ~ X + I(.. feols(Y ~ X + I(...1\nDependent Var.:                  Y                  Y                    Y\n                                                                          \nConstant         7.285*** (0.5660)   -0.1295 (0.3839)      0.0759 (0.5091)\nX               -8.934*** (0.1953)   0.9779* (0.3831)      0.4542 (0.9331)\nX square                           -2.003*** (0.0752)   -1.738*** (0.4368)\nX cube                                                    -0.0354 (0.0574)\n_______________ __________________ __________________   __________________\nS.E. type                      IID                IID                  IID\nObservations                   200                200                  200\nR2                         0.91357            0.98122              0.98126\nAdj. R2                    0.91313            0.98103              0.98097\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms",
    "href": "2023/weeks/week05/slides.html#logarithms",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\nAnother common transformation, both for dependent and independent variables, is to take the logarithm\nThis has the effect of pulling in extreme values from strongly right-skewed data and making linear relationships pop out\nIncome, for example, is almost always used with a logarithm\nIt also gives the coefficients a nice percentage-based interpretation"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms-1",
    "href": "2023/weeks/week05/slides.html#logarithms-1",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#or-if-you-prefer",
    "href": "2023/weeks/week05/slides.html#or-if-you-prefer",
    "title": "Binary Variables and Functional Form",
    "section": "Or if you prefer…",
    "text": "Or if you prefer…\n\nNotice the change in axes"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms-2",
    "href": "2023/weeks/week05/slides.html#logarithms-2",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\nHow can we interpret them?\nThe key is to remember that \\(\\log(X) + a \\approx \\log((1+a)X)\\), meaning that a \\(a\\)-unit change in \\(log(X)\\) is similar to a \\(a\\times100%\\) change in \\(X\\)\nSo, walk through our “one-unit change in the variable” logic from before, but whenever we hit a log, change that into a percentage!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms-3",
    "href": "2023/weeks/week05/slides.html#logarithms-3",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\n\\(Y = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1X\\) a one-unit change in \\(X\\) is associated with a \\(\\beta_1\\times 100\\)% change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(\\log(Y)\\), or a \\(\\beta_1\\times100\\)% change in \\(Y\\).\n(Try also with changes smaller than one unit - that’s usually more reasonable)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms-4",
    "href": "2023/weeks/week05/slides.html#logarithms-4",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\nDownsides:\n\nLogarithms require that all data be positive. No negatives or zeroes!\nFairly rare that a variable with negative values wants a log anyway\nBut zeroes are common! A common practice is to just do \\(log(X+1)\\) but this is pretty arbitrary"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#functional-form-2",
    "href": "2023/weeks/week05/slides.html#functional-form-2",
    "title": "Binary Variables and Functional Form",
    "section": "Functional Form",
    "text": "Functional Form\n\nIn general, you want the shape of your function to match the shape of the relationship in the data (or, even better, the true relationship)\nPolynomials and logs can usually get you there!\nWhich to use? Use logs for highly skewed data or variables with exponential relationships\nUse polynomials if it doesn’t look straight! Check that scatterplot and see how not-straight it is!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#concept-checks-1",
    "href": "2023/weeks/week05/slides.html#concept-checks-1",
    "title": "Binary Variables and Functional Form",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhich of the following variables would you likely want to log before using them? Income, height, wealth, company size, home square footage\nIn each of the following estimated OLS lines, interpret the coefficient by filling in “A [blank] change in X is associated with a [blank] change in Y”:\n\n\\[ Y = 1 + 2\\log(X) \\] \\[ \\log(Y) = 3 + 2\\log(X) \\]\n\\[ \\log(Y) = 4 + 3X \\]"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions",
    "href": "2023/weeks/week05/slides.html#interactions",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nFor both polynomials and logarithms, the effect of a one-unit change in \\(X\\) differs depending on its current value (for logarithms, a 1-unit change in \\(X\\) is different percentage changes in \\(X\\) depending on current value)\nBut why stop there? Maybe the effect of \\(X\\) differs depending on the current value of other variables!\nEnter interaction terms!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z + \\varepsilon \\] - Interaction terms are a little tough but also extremely important."
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-1",
    "href": "2023/weeks/week05/slides.html#interactions-1",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\nExpect to come back to these slides, as you’re almost certainly going to use interaction terms in both our assessment and the dissertation"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-2",
    "href": "2023/weeks/week05/slides.html#interactions-2",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nChange in the value of a control can shift a regression line up and down\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z\\), estimated as \\(Y = .01 + 1.2X + .95Z\\):"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-3",
    "href": "2023/weeks/week05/slides.html#interactions-3",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nBut an interaction can both shift the line up and down AND change its slope\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z\\), estimated as \\(Y = .035 + 1.14X + .94Z + 1.02X\\times Z\\):"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-4",
    "href": "2023/weeks/week05/slides.html#interactions-4",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nHow can we interpret an interaction?\nThe idea is that the interaction shows how the effect of one variable changes as the value of the other changes\nThe derivative helps!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z \\] \\[ \\partial Y/\\partial X = \\beta_1 + \\beta_3 Z \\]\n\nThe effect of \\(X\\) is \\(\\beta_1\\) when \\(Z = 0\\), or \\(\\beta_1 + \\beta_3\\) when \\(Z = 1\\), or \\(\\beta_1 + 3\\beta_3\\) if \\(Z = 3\\)!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-5",
    "href": "2023/weeks/week05/slides.html#interactions-5",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nOften we are doing interactions with binary variables to see how an effect differs across groups\nNow, instead of the intercept giving the baseline and the binary coefficient giving the difference, the coefficient on \\(X\\) is the baseline effect of \\(X\\) and the interaction is the difference in the effect of \\(X\\)\nThe interaction coefficient becomes “the difference in the effect of \\(X\\) between the \\(Z\\) =”No” and \\(Z\\) = “Yes” groups”\n(What if it’s continuous? Mathematically the same but the thinking changes - the interaction term is the difference in the effect of \\(X\\) you get when increasing \\(Z\\) by one unit)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#notes-on-interactions",
    "href": "2023/weeks/week05/slides.html#notes-on-interactions",
    "title": "Binary Variables and Functional Form",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nLike with polynomials, the coefficients on their own now have little meaning and must be evaluated alongside each other. \\(\\beta_1\\) by itself is just “the effect of \\(X\\) when \\(Z = 0\\)”, not “the effect of \\(X\\)”\nYes, you do almost always want to include both variables in un-interacted form and interacted form. Otherwise the interpretation gets very thorny"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#in-r",
    "href": "2023/weeks/week05/slides.html#in-r",
    "title": "Binary Variables and Functional Form",
    "section": "In R!",
    "text": "In R!\n\nBinary variables in R (on the right-hand-side) you can just treat as normal variables\nCategorical variables too (although if it’s numeric you may need to run it through factor() first, or i() in feols())\nIn feols() you can specify which group gets dropped using i() and setting ref in it\n\n\n# drops married = FALSE\nfeols(log(earnings) ~ married, data = PSID)\n# drops married = TRUE\nfeols(log(earnings) ~ i(married, ref = 'TRUE'), data = PSID)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#binary-variables-1",
    "href": "2023/weeks/week05/slides.html#binary-variables-1",
    "title": "Binary Variables and Functional Form",
    "section": "Binary Variables",
    "text": "Binary Variables\n\n\n                            feols(log(earning..\nDependent Var.:                   log(earnings)\n                                               \nConstant                      10.06*** (0.0701)\neducationGraduateDegree        0.1673* (0.0842)\neducationHighSchoolDegree   -0.5433*** (0.0658)\neducationNoHighSchoolDegree -0.9404*** (0.0826)\neducationSomeCollege        -0.2893*** (0.0699)\nI(kids&gt;0)TRUE               -0.2922*** (0.0548)\n___________________________ ___________________\nS.E. type                                   IID\nObservations                              2,803\nR2                                      0.09907\nAdj. R2                                 0.09746\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-and-logarithms",
    "href": "2023/weeks/week05/slides.html#polynomials-and-logarithms",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials and Logarithms",
    "text": "Polynomials and Logarithms\n\nAs previously discussed, \\(I()\\) will let us do functions like \\(X^2\\)\nWe can also do log() straight in the regression.\n\n\nlm(Y ~ X + I(X^2) + I(X^3), data = df)\nlm(log(Y) ~ log(X), data = df)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-6",
    "href": "2023/weeks/week05/slides.html#interactions-6",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nMarriage for those without a college degree raises earnings by 24%. A college degree reduces the marriage premium by 25%. Marriage for those with a college degree reduces earnings by .24 - .25 = -1%\n\n\n\n                          feols(log(earnin..\nDependent Var.:                log(earnings)\n                                            \nConstant                   9.087*** (0.0583)\nmarriedTRUE               0.2381*** (0.0638)\ncollegeTRUE               0.8543*** (0.1255)\nmarriedTRUE x collegeTRUE  -0.2541. (0.1363)\n_________________________ __________________\nS.E. type                                IID\nObservations                           2,803\nR2                                   0.06253\nAdj. R2                              0.06153\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#tests",
    "href": "2023/weeks/week05/slides.html#tests",
    "title": "Binary Variables and Functional Form",
    "section": "Tests",
    "text": "Tests\n\nwald() can be handy for testing groups of binary variables for a categorical\nAlso good for testing all the polynomial terms, or testing if the effect of \\(X\\) is significant at a certain value of \\(Z\\)\n\n\n# Is the education effect zero overall?\nm1 &lt;- feols(log(earnings)~educatn, data = PSID)\nwald(m1, 'educatn')\n\n# Does X have any effect?\nm2 &lt;- feols(Y ~ X + I(X^2) + I(X^3), data = df)\nwald(m2, 'X') # Gets all coefficients with an 'X' anywhere in the name - check this is right!\n\n# Is the effect of X significant when Z = 5?\nlibrary(multcomp)\nm3 &lt;- lm(Y ~ X*Z, data = df)\nglht(m3, 'X + 5*X:Z= 0') %&gt;% summary()\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-4",
    "href": "2023/weeks/week05/slides.html#polynomials-4",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nInterpret polynomials using the derivative\n\\(\\partial Y/\\partial X\\) will be different depending on the value of \\(X\\) (as it should! Notice in the graph that the slope changes for different values of \\(X\\))\n\n\\[ Y = \\beta_1X + \\beta_2X^2 \\] \\[ \\partial Y/\\partial X = \\beta_1 + 2\\beta_2X \\]\nSo at \\(X = 0\\), the effect of a one-unit change in \\(X\\) is \\(\\beta_1\\). At \\(X = 1\\), it’s \\(\\beta_1 + \\beta_2\\). At \\(X = 5\\) it’s \\(\\beta_1 + 5\\beta_2\\)."
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-5",
    "href": "2023/weeks/week05/slides.html#polynomials-5",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nIMPORTANT: when you have a polynomial, the coefficients on each individual term mean very little on their own. You have to consider them alongisde the other coefficients from the polynomial! Never interpret \\(\\beta_1\\) here without thinking about \\(\\beta_2\\) alongside. Also, the significance of the individual terms doesn’t really matter - consider doing an F-test of all of them at once."
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-7",
    "href": "2023/weeks/week05/slides.html#interactions-7",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nX*Z will include X, Z, and also their interaction\nIf necessary, X:Z is the interaction only, but you rarely need this. However, it’s handy for referring to the interaction term in linearHypothesis!\nIn feols() the i() function is a very powerful way of doing interactions\n\n\nfeols(Y ~ X*Z, data = df)\nfeols(Y ~ X + X:Z, data = df)\nfeols(Y ~ i(Z, X), data = df)\nfeols(Y ~ i(Z, X), data = df) |&gt; iplot()"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#notes-on-interactions-1",
    "href": "2023/weeks/week05/slides.html#notes-on-interactions-1",
    "title": "Binary Variables and Functional Form",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nInteraction effects are poorly powered. You need a lot of data to be able to tell whether an effect is different in two groups. If \\(N\\) observations is adequate power to see if the effect itself is different from zero, you need a sample of roughly \\(16\\times N\\) to see if the difference in effects is nonzero. Sixteen times!!\nIt’s tempting to try interacting your effect with everything to see if it’s bigger/smaller/nonzero in some groups, but because it’s poorly powered, this is a bad idea! You’ll get a lot of false positives\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  }
]