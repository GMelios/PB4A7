[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PB4A7 - Quantitative Applications for Behavioural Science",
    "section": "",
    "text": "📑 Course Brief\nDescription: In this course, students will immerse themselves in the world of causal inference methodologies, a cornerstone in behavioural science research. The curriculum guides learners through the essential processes of cleaning, analysing, and visualising secondary data, equipping them with the skills to conduct robust and insightful research. Through practical examples, students will learn to adeptly navigate various research designs, and develop the proficiency to communicate their findings effectively, fostering a deeper understanding and application of best practices in behavioural science.\nFocus: Understand the fundamentals of causal inference and its applications in Behavioural Science. Master statistical tools used by psychologists, political scientists, and economists. Recognize and address contemporary issues in behavioural science.\nDownload the full syllabus here\n🛠️ Requirements: For students who have no prior experience with statistics and/or STATA, the completion of the following Digital Skills class is highly recommended:\nIntroduction to STATA\n\n\n🎯 Learning Objectives\n\nHow to distinguish Causation from Correlation\nHow different research designs work\nHow to apply different research designs on Stata\nHow to effectively visualise your findings\n\n\n  Enter the class"
  },
  {
    "objectID": "2024/index.html",
    "href": "2024/index.html",
    "title": "PB4A7 - Quantitative Applications for Behavioural Science",
    "section": "",
    "text": "🧑🏻‍🏫 Our Team\n\nCourse ConvenorTeaching Staff\n\n\n\nDr George Melios  Research Fellow  London School of Economics and Political Science 📧 g.melios at lse dot ac dot uk\nOffice Hours:\n\nWhen: Mondays 11:30 - 12:30\nWhere: Zoom\nHow to book: Student Hub\n\n\n\n\nLazaros Chatzilazarou  PhD Candidate at City University  📧 L.A.Chatzilazarou at lse dot ac dot uk\nHelp Sessions:\n\nWhen: Tuesdays 11:00-12:00\nWhere: CBG.G.01\n\n\n\n\n\n\n📍 Lecture\nWednesdays 13:00-14:00 at MAR.1.10\n\n\n💻 Seminars\n\nGroup 01\n\n📆 Wednesdays\n⌚ 14:00 - 15:00\n📍 FAW.4.02\n\n\n\n\nGroup 02\n\n📆 Wednesdays\n⌚ 15:00 - 16:00\n📍 FAW.4.02\n\n\n\nGroup 03\n\n📆 Wednesdays\n⌚ 16:00 - 17:00\n📍 FAW.4.02"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html",
    "href": "2024/weeks/week11/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#background-1",
    "href": "2024/weeks/week11/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#code-and-software",
    "href": "2024/weeks/week11/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#making-mistakes",
    "href": "2024/weeks/week11/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#making-mistakes-1",
    "href": "2024/weeks/week11/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#anna-karenina-principle",
    "href": "2024/weeks/week11/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#what-do-we-learn",
    "href": "2024/weeks/week11/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#workflow-1",
    "href": "2024/weeks/week11/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#empirical-workflow",
    "href": "2024/weeks/week11/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#checklist",
    "href": "2024/weeks/week11/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2024/weeks/week11/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#step-1---read-the-codebook",
    "href": "2024/weeks/week11/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#do-not-touch-the-original-data",
    "href": "2024/weeks/week11/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2024/weeks/week11/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2024/weeks/week11/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#step-2---helping-your-future-self",
    "href": "2024/weeks/week11/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2024/weeks/week11/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#step-2---automation",
    "href": "2024/weeks/week11/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#step-2---beautiful-code",
    "href": "2024/weeks/week11/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#step-3---eyeballing",
    "href": "2024/weeks/week11/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#step-3---eyeballing-1",
    "href": "2024/weeks/week11/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2024/weeks/week11/slides_sem.html#step-4---missing-observations",
    "href": "2024/weeks/week11/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2024/weeks/week12/page.html",
    "href": "2024/weeks/week12/page.html",
    "title": "🗓️ Guest Lecture - Difference in Differences",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "2024/weeks/week12/page.html#lecture-slides",
    "href": "2024/weeks/week12/page.html#lecture-slides",
    "title": "🗓️ Guest Lecture - Difference in Differences",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nSlides will be uploaded closer to the event\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week12/page.html#papers-to-review",
    "href": "2024/weeks/week12/page.html#papers-to-review",
    "title": "🗓️ Guest Lecture - Difference in Differences",
    "section": "📚 Papers to review",
    "text": "📚 Papers to review"
  },
  {
    "objectID": "2024/weeks/week11/page.html",
    "href": "2024/weeks/week11/page.html",
    "title": "🗓️ Week 11 - Difference in Differences",
    "section": "",
    "text": "In this week, we will focus on causal inference through Difference in Difference estimators. First, we will decompose the mechanics and conditions of Diff-in-Diff estimators and then we will briefly discuss current methodological debates and advances."
  },
  {
    "objectID": "2024/weeks/week11/page.html#lecture-slides",
    "href": "2024/weeks/week11/page.html#lecture-slides",
    "title": "🗓️ Week 11 - Difference in Differences",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week11/page.html#recommended-reading",
    "href": "2024/weeks/week11/page.html#recommended-reading",
    "title": "🗓️ Week 11 - Difference in Differences",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week11/page.html#communication",
    "href": "2024/weeks/week11/page.html#communication",
    "title": "🗓️ Week 11 - Difference in Differences",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/weeks/week10/page.html",
    "href": "2024/weeks/week10/page.html",
    "title": "🗓️ Week 10 Instrumental Variables",
    "section": "",
    "text": "In this week, we will focus on estimating causal parameteres through Instrumental Variables. First, we will approach the concept of instruments through Direct Acyclical Graphs (DAGs) and then through the LATE effect (Local Average Treatment Effect)."
  },
  {
    "objectID": "2024/weeks/week10/page.html#lecture-slides",
    "href": "2024/weeks/week10/page.html#lecture-slides",
    "title": "🗓️ Week 10 Instrumental Variables",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week10/page.html#recommended-reading",
    "href": "2024/weeks/week10/page.html#recommended-reading",
    "title": "🗓️ Week 10 Instrumental Variables",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week10/page.html#communication",
    "href": "2024/weeks/week10/page.html#communication",
    "title": "🗓️ Week 10 Instrumental Variables",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/weeks/week09/page.html",
    "href": "2024/weeks/week09/page.html",
    "title": "🗓️ Week 09 - Regression Discontinuity Design",
    "section": "",
    "text": "In this week, we will look into the quasi-experimental Regression Discontinuity Design."
  },
  {
    "objectID": "2024/weeks/week09/page.html#lecture-slides",
    "href": "2024/weeks/week09/page.html#lecture-slides",
    "title": "🗓️ Week 09 - Regression Discontinuity Design",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week09/page.html#recommended-reading",
    "href": "2024/weeks/week09/page.html#recommended-reading",
    "title": "🗓️ Week 09 - Regression Discontinuity Design",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week09/page.html#communication",
    "href": "2024/weeks/week09/page.html#communication",
    "title": "🗓️ Week 09 - Regression Discontinuity Design",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#check-in",
    "href": "2024/weeks/week09/slides.html#check-in",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Check-in",
    "text": "Check-in\n\nWe’re thinking through ways that we can identify the effect of interest without having to control for everything\nOne way is by focusing on within variation - if all the endogeneity can be controlled for or only varies between-individuals, we can just focus on within variation to identify it\nCon: washes out a lot of variation! Result can be noisier if there’s not much within-variation to work with\nAlso, this requires no endogenous variation over time\nThat might be a tricky assumption! Often there are plenty of back doors that shift over time"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#regression-discontinuity-1",
    "href": "2024/weeks/week09/slides.html#regression-discontinuity-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\nThe basic idea is this:\n\nWe look for a treatment that is assigned on the basis of being above/below a cutoff value of a continuous variable\nFor example, if you get above a certain test score they let you into a “gifted and talented” program\n\nOr if you are just on one side of a time zone line, your day starts one hour earlier/later\nOr if a candidate gets 50.1% of the vote they’re in, 40.9% and they’re out\n\n\nWe call these continuous variables “Running variables” because we run along them until we hit the cutoff"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#regression-discontinuity-2",
    "href": "2024/weeks/week09/slides.html#regression-discontinuity-2",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nBut wait, hold on, if treatment is driven by running variables, won’t we have a back door going through those very same running variables?? Yes!\nAnd we can’t just control for RunningVar because that’s where all the variation in treatment comes from. Uh oh!"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#regression-discontinuity-3",
    "href": "2024/weeks/week09/slides.html#regression-discontinuity-3",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nThe key here is realizing that the running variable affects treatment only when you go across the cutoff\nSo really the diagram looks like this!"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#regression-discontinuity-4",
    "href": "2024/weeks/week09/slides.html#regression-discontinuity-4",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nSo what does this mean?\nIf we can control for the running variable everywhere except the cutoff, then…\nWe will be controlling for the running variable, closing that back door\nBut leaving variation at the cutoff open, allowing for variation in treatment\nWe focus on just the variation around the treatment, narrowing the range of the running variable we use so sharply that it’s basically controlled for. Then the effect of cutoff on treatment is like an experiment!"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#regression-discontinuity-5",
    "href": "2024/weeks/week09/slides.html#regression-discontinuity-5",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nBasically, the idea is that right around the cutoff, treatment is randomly assigned\nIf you have a test score of 89.9 (not high enough for gifted-and-talented), you’re basically the same as someone who has a test score of 90.0 (just barely high enough)\nBut we get variation in treatment!\nThis specifically gives us the effect of treatment for people who are right around the cutoff a.k.a. a “local average treatment effect” (we still won’t know the effect of being put in gifted-and-talented for someone who gets a 30)"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#regression-discontinuity-6",
    "href": "2024/weeks/week09/slides.html#regression-discontinuity-6",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nA very basic idea of this, before we even get to regression, is to create a binned chart\nAnd see how the bin values jump at the cutoff\nA binned chart chops the Y-axis up into bins\nThen takes the average Y value within that bin. That’s it!\nThen, we look at how those X bins relate to the Y binned values.\nIf it looks like a pretty normal, continuous relationship… then JUMPS UP at the cutoff X-axis value, that tells us that the treatment itself must be doing something!"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#regression-discontinuity-7",
    "href": "2024/weeks/week09/slides.html#regression-discontinuity-7",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#concept-checks",
    "href": "2024/weeks/week09/slides.html#concept-checks",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy is it important that we look as norrowly as possible around the cutoff? What does this get us over comparing the entire treated and untreated groups?\nCan you think of an example of a treatment that is assigned at least partially on a cutoff?\nWhy can’t we just control for the running variable as we normally would to solve the endogeneity problem?"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#fitting-lines-in-rdd",
    "href": "2024/weeks/week09/slides.html#fitting-lines-in-rdd",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Fitting Lines in RDD",
    "text": "Fitting Lines in RDD\n\nLooking purely just at the cutoff and making no use of the space away from the cutoff throws out a lot of useful information\nWe know that the running variable is related to outcome, so we can probably improve our prediction of what the value on either side of the cutoff should be if we use data away from the cutoff to help with prediction than if we just use data near the cutoff, which is what that animation does\nWe can do this with good ol’ OLS.\nThe bin plot we did can help us pick a functional form for the slope"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#fitting-lines-in-rdd-1",
    "href": "2024/weeks/week09/slides.html#fitting-lines-in-rdd-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Fitting Lines in RDD",
    "text": "Fitting Lines in RDD\n\nTo be clear, producing the line(s) below is our goal. How can we do it?\nThe true model I’ve made is an RDD effect of .7, with a slope of 1 to the left of the cutoff and a slope of 1.5 to the right"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#regression-in-rdd",
    "href": "2024/weeks/week09/slides.html#regression-in-rdd",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression in RDD",
    "text": "Regression in RDD\n\nFirst, we need to transform our data\nWe need a “Treated” variable that’s TRUE when treatment is applied - above or below the cutoff\nThen, we are going to want a bunch of things to change at the cutoff. This will be easier if the running variable is centered around the cutoff. So we’ll turn our running variable \\(X\\) into \\(X - cutoff\\) and call that \\(XCentered\\)"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#varying-slope",
    "href": "2024/weeks/week09/slides.html#varying-slope",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Varying Slope",
    "text": "Varying Slope\n\nTypically, you will want to let the slope vary to either side\nIn effect, we are fitting an entirely different regression line on each side of the cutoff\nWe can do this by interacting both slope and intercept with \\(treated\\)!\nCoefficient on Treated is how the intercept jumps - that’s our RDD effect. Coefficient on the interaction is how the slope changes\n\n\\[Y = \\beta_0 + \\beta_1Treated + \\beta_2XCentered + \\beta_3Treated\\times XCentered + \\varepsilon\\]"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#varying-slope-1",
    "href": "2024/weeks/week09/slides.html#varying-slope-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Varying Slope",
    "text": "Varying Slope\n(as an aside, sometimes the effect of interest is the interaction term - the change in slope! This answers the question “does the effect of \\(X\\) on \\(Y\\) change at the cutoff? This is called a”regression kink” design. We won’t go more into it here, but it is out there!)"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#polynomial-terms",
    "href": "2024/weeks/week09/slides.html#polynomial-terms",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Polynomial Terms",
    "text": "Polynomial Terms\n\nWe don’t need to stop at linear slopes!\nJust like we brought in our knowledge of binary and interaction terms to understand the linear slope change, we can bring in polynomials too. Add a square maybe!\nDon’t get too wild with cubes, quartics, etc. - polynomials tend to be at their “weirdest” near the edges, and we don’t want super-weird predictions right at the cutoff. It could give us a mistaken result!\nA square term should be enough"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#polynomial-terms-1",
    "href": "2024/weeks/week09/slides.html#polynomial-terms-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Polynomial Terms",
    "text": "Polynomial Terms\n\nHow do we do this? Interactions again. Take any regression equation… \\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\\]\nAnd just center the \\(X\\) (let’s call it \\(XC\\), add on a set of the same terms multiplied by \\(Treated\\) (don’t forget \\(Treated\\) by itself - that’s \\(Treated\\) times the interaction!)\n\n\\[Y = \\beta_0 + \\beta_1XC + \\beta_2XC^2 + \\beta_3Treated + \\beta_4Treated\\times XC + \\beta_5Treated\\times XC^2 + \\varepsilon\\]\n\nThe coefficient on \\(Treated\\) remains our “jump at the cutoff” - our RDD estimate!\n\n\n\n                              feols(Y ~ X_cent..\nDependent Var.:                                Y\n                                                \nConstant                        -0.0340 (0.0385)\nX_centered                      0.6990. (0.3641)\ntreatedTRUE                   0.7677*** (0.0577)\nX_centered square               -0.5722 (0.7117)\nX_centered x treatedTRUE         0.7509 (0.5359)\ntreatedTRUE x I(X_centered^2)     0.5319 (1.034)\n_____________________________ __________________\nS.E. type                                    IID\nObservations                               1,000\nR2                                       0.84779\nAdj. R2                                  0.84702\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#concept-checks-1",
    "href": "2024/weeks/week09/slides.html#concept-checks-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWould the coefficient on \\(Treated\\) still be the regression discontinuity effect estimate if we hadn’t centered \\(X\\)? Why or why not?\nWhy might we want to use a polynomial term in our RDD model?\nWhat relationship are we assuming between the outcome variable and the running variable if we choose not to include \\(XCentered\\) in our model at all (i.e. a “zero-order polynomial”)"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#assumptions",
    "href": "2024/weeks/week09/slides.html#assumptions",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Assumptions",
    "text": "Assumptions\n\nWe knew there must be some assumptions lurking around here\nWhat are we assuming about the error term and endogeneity here?\nSpecifically, we are assuming that the only thing jumping at the cutoff is treatment\nSort of like parallel trends (see week 11), but maybe more believable since we’ve narrowed in so far\nFor example, if having an income below 150% of the poverty line gets you access to food stamps AND to job training, then we can’t really use that cutoff to get the effect of just food stamps\nThe only thing different about just above/just below should be treatment"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#graphically",
    "href": "2024/weeks/week09/slides.html#graphically",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Graphically",
    "text": "Graphically"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#windows-1",
    "href": "2024/weeks/week09/slides.html#windows-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Windows",
    "text": "Windows\n\nPay attention to the sample sizes, accuracy (true value .7) and standard errors!\n\n\nm1 &lt;- feols(Y~treated*X_centered, data = df)\nm2 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .25))\nm3 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .1))\nm4 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .05))\nm5 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .01))\netable(m1,m2,m3,m4,m5, keep = 'treatedTRUE')\n\n                                         m1                 m2\nDependent Var.:                           Y                  Y\n                                                              \ntreatedTRUE              0.7467*** (0.0376) 0.7723*** (0.0566)\ntreatedTRUE x X_centered 0.4470*** (0.1296)   0.6671. (0.4022)\n________________________ __________________ __________________\nS.E. type                               IID                IID\nObservations                          1,000                492\nR2                                  0.84769            0.74687\nAdj. R2                             0.84723            0.74531\n\n                                         m3                 m4              m5\nDependent Var.:                           Y                  Y               Y\n                                                                              \ntreatedTRUE              0.7086*** (0.0900) 0.6104*** (0.1467) 0.5585 (0.4269)\ntreatedTRUE x X_centered     -1.307 (1.482)      6.280 (4.789)   41.21 (72.21)\n________________________ __________________ __________________ _______________\nS.E. type                               IID                IID             IID\nObservations                            206                 93              15\nR2                                  0.69322            0.59825         0.48853\nAdj. R2                             0.68867            0.58470         0.34904\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#granular-running-variable-1",
    "href": "2024/weeks/week09/slides.html#granular-running-variable-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Granular Running Variable",
    "text": "Granular Running Variable\n\nNot a whole lot we can do about this\nThere are some fancy RDD estimators that allow for granular running variables\nBut in general, if this is what you’re facing, you might be in trouble\nBefore doing an RDD, think “is it plausible that someone with the highest value just below the cutoff, and someone with the lowest value just above the cutoff are only at different values because of random chance?”"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#looking-for-lumping-1",
    "href": "2024/weeks/week09/slides.html#looking-for-lumping-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nIf there’s manipulation of the running variable around the cutoff, we can often see it in the presence of lumping\nI.e. if there’s a big cluster of observations to one side of the cutoff and a seeming gap missing on the other side"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#looking-for-lumping-2",
    "href": "2024/weeks/week09/slides.html#looking-for-lumping-2",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nHere’s an example from the real world in medical research - statistically, p-values should be uniformly distributed\nBut it’s hard to get insignificant results published in some journals. So people might “p-hack” until they find some form of analysis that’s significant, and also we have heavy selection into publication based on \\(p &lt; .05\\). Can’t use that cutoff for an RDD!\n\n\np-value graph from Perneger & Combescure, 2017"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#looking-for-lumping-3",
    "href": "2024/weeks/week09/slides.html#looking-for-lumping-3",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nHow can we look for this stuff?\nWe can look graphically by just checking for a jump at the cutoff in number of observations after binning\n\n\ndf_bin_count &lt;- df %&gt;%\n  # Select breaks so that one of hte breakpoints is the cutoff\n  mutate(X_bins = cut(X, breaks = 0:10/10)) %&gt;%\n  group_by(X_bins) %&gt;%\n  count()"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#looking-for-lumping-4",
    "href": "2024/weeks/week09/slides.html#looking-for-lumping-4",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nThe first one looks pretty good. We have one that looks not-so-good on the right"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#looking-for-lumping-5",
    "href": "2024/weeks/week09/slides.html#looking-for-lumping-5",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nAnother thing we can do is do a “placebo test”\nCheck if variables other than treatment or outcome vary at the cutoff\nWe can do this by re-running our RDD but just swapping out some other variable for our outcome\nIf we get a significant jump, that’s bad! That tells us that other things are changing at the cutoff which implies some sort of manipulation (or just super lousy luck)"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#regression-discontinuity-in-r",
    "href": "2024/weeks/week09/slides.html#regression-discontinuity-in-r",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity in R",
    "text": "Regression Discontinuity in R\n\nWe can specify an RDD model by just telling it the dependent variable \\(Y\\), the running variable \\(X\\), and the cutoff \\(c\\).\nWe can also specify how many polynomials to us with p\n(it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)\nIt will also pick a window for us with h\nPlenty of other options\nIncluding a fuzzy option to specify actual treatment outside of the running variable/cutoff combo"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#rdrobust",
    "href": "2024/weeks/week09/slides.html#rdrobust",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nWe’ve gone through all kinds of procedures for doing RDD in R already using regression\nBut often, professional researchers won’t do it that way!\nWe’ll use packages and formulas that do things like “picking a bandwidth (window)” for us in a smart way, or not relying so strongly on linearity\nThe rdrobust package does just that!"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#rdrobust-1",
    "href": "2024/weeks/week09/slides.html#rdrobust-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5))\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  501          499\nEff. Number of Obs.             185          170\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.174        0.174\nBW bias (b)                   0.293        0.293\nrho (h/b)                     0.594        0.594\nUnique Obs.                     501          499\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.707     0.085     8.311     0.000     [0.540 , 0.874]     \n        Robust         -         -     6.762     0.000     [0.484 , 0.878]     \n============================================================================="
  },
  {
    "objectID": "2024/weeks/week09/slides.html#rdrobust-2",
    "href": "2024/weeks/week09/slides.html#rdrobust-2",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5, fuzzy = df$treatment))\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  501          499\nEff. Number of Obs.             185          170\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.174        0.174\nBW bias (b)                   0.293        0.293\nrho (h/b)                     0.594        0.594\nUnique Obs.                     501          499\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.707     0.085     8.311     0.000     [0.540 , 0.874]     \n        Robust         -         -     6.762     0.000     [0.484 , 0.878]     \n============================================================================="
  },
  {
    "objectID": "2024/weeks/week09/slides.html#rdrobust-3",
    "href": "2024/weeks/week09/slides.html#rdrobust-3",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nWe can even have it automatically make plots of our RDD! Same syntax\n\n\nrdplot(df$Y, df$X, c = .5)"
  },
  {
    "objectID": "2024/weeks/week09/slides.html#thats-it",
    "href": "2024/weeks/week09/slides.html#thats-it",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "That’s it!",
    "text": "That’s it!\n\nThat’s what we have for RDD\nGo explore the regression discontinuity Seminar\nAnd the paper to read!\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2024/weeks/week08/page.html",
    "href": "2024/weeks/week08/page.html",
    "title": "🗓️ Week 08 - Panel data / Within Variation",
    "section": "",
    "text": "In this week, we will look into how we analyse within variation and include fixed effects."
  },
  {
    "objectID": "2024/weeks/week08/page.html#lecture-slides",
    "href": "2024/weeks/week08/page.html#lecture-slides",
    "title": "🗓️ Week 08 - Panel data / Within Variation",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week08/page.html#recommended-reading",
    "href": "2024/weeks/week08/page.html#recommended-reading",
    "title": "🗓️ Week 08 - Panel data / Within Variation",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week08/page.html#communication",
    "href": "2024/weeks/week08/page.html#communication",
    "title": "🗓️ Week 08 - Panel data / Within Variation",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#a-pickle",
    "href": "2024/weeks/week08/slides.html#a-pickle",
    "title": "🗓️ Week 8  Within variation",
    "section": "A Pickle",
    "text": "A Pickle\n\nSo obviously this is a problem, and it’s not one we can reason or trick our way out of\nIf we don’t have the variable we need to control for, we don’t have it\n… or do we?"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#the-rest-of-the-term",
    "href": "2024/weeks/week08/slides.html#the-rest-of-the-term",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Rest of the Term",
    "text": "The Rest of the Term\n\nMuch of the rest of the term is going to be focused on finding ways to control for stuff that we can’t measure\nSeems impossible! But it is possible, at least in some circumstances\nToday, we will be talking about within variation and between variation, and the ability to control for all between variation using fixed effects"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#panel-data-1",
    "href": "2024/weeks/week08/slides.html#panel-data-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Panel Data",
    "text": "Panel Data\n\nHere’s what (a few rows from) a panel data set looks like - a variable for individual (county), a variable for time (year), and then the data\n\n\n\n\n\n\nCounty\nYear\nCrimeRate\nProbofArrest\n\n\n\n\n1\n81\n0.0398849\n0.289696\n\n\n1\n82\n0.0383449\n0.338111\n\n\n1\n83\n0.0303048\n0.330449\n\n\n1\n84\n0.0347259\n0.362525\n\n\n1\n85\n0.0365730\n0.325395\n\n\n1\n86\n0.0347524\n0.326062\n\n\n1\n87\n0.0356036\n0.298270\n\n\n3\n81\n0.0163921\n0.202899\n\n\n3\n82\n0.0190651\n0.162218\n\n\n\nNote: 9 rows out of 630. “Prob. of Arrest” is estimated probability of being arrested when you commit a crime"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#between-and-within-1",
    "href": "2024/weeks/week08/slides.html#between-and-within-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nIf we look at the overall variation, just pretending this is all together, we get this"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#between-and-within-2",
    "href": "2024/weeks/week08/slides.html#between-and-within-2",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nBETWEEN variation is what we get if we look at the relationship between the means of each county"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#between-and-within-3",
    "href": "2024/weeks/week08/slides.html#between-and-within-3",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nAnd I mean it! Only look at those means! The individual year-to-year variation within county doesn’t matter."
  },
  {
    "objectID": "2024/weeks/week08/slides.html#between-and-within-4",
    "href": "2024/weeks/week08/slides.html#between-and-within-4",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nWithin variation goes the other way - it treats those orange crosses as their own individualized sets of axes and looks at variation within county from year-to-year only!\nWe basically slide the crosses over on top of each other and then analyze that data"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#between-and-within-5",
    "href": "2024/weeks/week08/slides.html#between-and-within-5",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nWe can clearly see that between counties there’s a strong positive relationship\nBut if you look within a given county, the relationship isn’t that strong, and actually seems to be negative\nWhich would make sense - if you think your chances of getting arrested are high, that should be a deterrent to crime\nBut what are we actually doing here? Let’s think about the causal diagram / data-generating process!\nWhat goes into the probability of arrest and the crime rate? Lots of stuff!"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#the-crime-rate",
    "href": "2024/weeks/week08/slides.html#the-crime-rate",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Crime Rate",
    "text": "The Crime Rate\n\n“LocalStuff” is just all the things unique to that area\n“LawAndOrder” is how committed local politicians are to “Law and Order Politics”"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#between-and-within-6",
    "href": "2024/weeks/week08/slides.html#between-and-within-6",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nFor each of these variables we can ask if they vary between groups and/or within groups\nLocalStuff is all the stuff unique to that county - geography, landmarks, the quality of the schools, almost by definition this only varies between groups. It’s not like the things that make your county unique are different each year (or at least not very different)\nWhether the county has LawAndOrder and how many CivilRights you’re allowed might change a bit year to year, but in general, political climates like that change pretty slowly. At a bit of a stretch we can call that something that only varies between groups too\nPolice budgets (and thus number of police on the streets) and Poverty (which varies with the economy) vary both between counties, but also within counties from year to year\nVariables with between variation only (by our assumption): LocalStuff, LawAndOrder, CivilRights\nVariables with both between and within variation: Police, Poverty"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#between-and-within-7",
    "href": "2024/weeks/week08/slides.html#between-and-within-7",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nLet’s simplify our graph!\nSome of the variables only vary between counties\nSo, we can replace those variables on the graph with the variable County\nRight? That’s where all the variation is anyway"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#the-crime-rate-1",
    "href": "2024/weeks/week08/slides.html#the-crime-rate-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Crime Rate",
    "text": "The Crime Rate\n\n“LocalStuff” is just all the things unique to that area\n“LawAndOrder” is how committed local politicians are to “Law and Order Politics”"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#between-and-within-8",
    "href": "2024/weeks/week08/slides.html#between-and-within-8",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nNow the task of identifying ProbArrest \\(\\rightarrow\\) CrimeRate becomes much simpler!\nIf we control for County, that will close a lot of back doors for us\n(based on the diagram, all we need to control for is County and Poverty!)\nConveniently, we can control for County just like it was any other variable!\nAnd when we do, we automatically control for all variables that only have between variation, whatever they are, even if we can’t measure them directly or didn’t think about them\nAll that’s left is the within variation"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#concept-checks",
    "href": "2024/weeks/week08/slides.html#concept-checks",
    "title": "🗓️ Week 8  Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nFor each of these variables, would we expect them to have within variation, between variation, or both?\n(Individual = person) How a child’s height changes as they age.\n(Individual = person) In a data set tracking many people over many years, the variation in the number of children a person has in a given year.\n(Individual = city) Overall, Paris, France has more restaurants than Paris, Texas.\n(Individual = genre) The average pop music album sells more copies than the average jazz album\n(Individual = genre) Miles Davis’ Kind of Blue sold very well for a jazz album.\n(Individual = genre) Michael Jackson’s Thriller, a pop album, sold many more copies than Kind of Blue, a jazz album."
  },
  {
    "objectID": "2024/weeks/week08/slides.html#removing-between-variation",
    "href": "2024/weeks/week08/slides.html#removing-between-variation",
    "title": "🗓️ Week 8  Within variation",
    "section": "Removing Between Variation",
    "text": "Removing Between Variation\n\nOkay so that’s the concept\nRemove all the between variation so that all that’s left is within variation\nAnd in the process control for any variables that are made up only of between variation\nHow can we actually do this? And what’s really going on?\nLet’s first talk about the regression model itself that this implies\nThen let’s actually do the thing. There are two main ways: de-meaning and binary variables (they give the same result, for balanced panels anyway)"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#estimation-vs.-design",
    "href": "2024/weeks/week08/slides.html#estimation-vs.-design",
    "title": "🗓️ Week 8  Within variation",
    "section": "Estimation vs. Design",
    "text": "Estimation vs. Design\n\nTo be clear, this is exactly 0% different from what we’ve done before in terms of controlling for stuff\nAnd in fact we’re about to do the exact same thing we did before by just adding a categorical control variable for county or whatever\n(and in fact the “within” thing holds with other categorical controls - a categorical control for education isolates variation “within education levels”)\nThe difference is the reason we’re doing it. It’s fixed effects because a categorical control for individual controls for a lot of stuff, and we think closes a lot of back doors for us, not just one, and not just ones we can measure"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#the-model",
    "href": "2024/weeks/week08/slides.html#the-model",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Model",
    "text": "The Model\nThe \\(it\\) subscript says this variable varies over individual \\(i\\) and time \\(t\\)\n\\[Y_{it} = \\beta_0 + \\beta_1 X_{it} + \\varepsilon_{it}\\]\n\n\\(X_{it}\\) is related to LocalStuff which is not in the model and thus in the error term!\nRegular ol’ omitted variable bias. If we don’t adjust for the individual effect, we get a biased \\(\\hat{\\beta}_1\\)\n(this bias is called “pooling bias” although it’s really just a form of omitted variable bias)\nWe really have this then: \\[Y_{it} = \\beta_0 + \\beta_1 X_{it} + (\\alpha_i + \\varepsilon_{it})\\]"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#de-meaning",
    "href": "2024/weeks/week08/slides.html#de-meaning",
    "title": "🗓️ Week 8  Within variation",
    "section": "De-meaning",
    "text": "De-meaning\n\nLet’s do de-meaning first, since it’s most closely and obviously related to the “removing between variation” explanation we’ve been going for\nThe process here is simple!\n\n\nFor each variable \\(X_{it}\\), \\(Y_{it}\\), etc., get the mean value of that variable for each individual \\(\\bar{X}_i, \\bar{Y}_i\\)\nSubtract out that mean to get residuals \\((X_{it} - \\bar{X}_i), (Y_{it} - \\bar{Y}_i)\\)\nWork with those residuals\n\n\nThat’s it!"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#how-does-this-work",
    "href": "2024/weeks/week08/slides.html#how-does-this-work",
    "title": "🗓️ Week 8  Within variation",
    "section": "How does this work?",
    "text": "How does this work?\n\nThat \\(\\alpha_i\\) term gets absorbed\nThe residuals are, by construction, no longer related to the \\(\\alpha_i\\), so it no longer goes in the residuals!\n\n\\[(Y_{it} - \\bar{Y}_i) = \\beta_0 + \\beta_1(X_{it} - \\bar{X}_i) + \\varepsilon_{it}\\]"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#lets-do-it",
    "href": "2024/weeks/week08/slides.html#lets-do-it",
    "title": "🗓️ Week 8  Within variation",
    "section": "Let’s do it!",
    "text": "Let’s do it!\n\nWe can use group_by to get means-within-groups and subtract them out\n\n\ndata(crime4, package = 'wooldridge')\ncrime4 &lt;- crime4 %&gt;%\n  ## Filter to the data points from our graph\n  filter(county %in% c(1,3,7, 23),\n         prbarr &lt; .5) %&gt;%\n  group_by(county) %&gt;%\n  mutate(mean_crime = mean(crmrte),\n         mean_prob = mean(prbarr)) %&gt;%\n  mutate(demeaned_crime = crmrte - mean_crime,\n         demeaned_prob = prbarr - mean_prob)"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#and-regress",
    "href": "2024/weeks/week08/slides.html#and-regress",
    "title": "🗓️ Week 8  Within variation",
    "section": "And Regress!",
    "text": "And Regress!\n\norig_data &lt;- feols(crmrte ~ prbarr, data = crime4)\nde_mean &lt;- feols(demeaned_crime ~ demeaned_prob, data = crime4)\netable(orig_data, de_mean)\n\n                        orig_data           de_mean\nDependent Var.:            crmrte    demeaned_crime\n                                                   \nConstant         0.0118* (0.0050) 1.41e-18 (0.0004)\nprbarr          0.0486** (0.0167)                  \ndemeaned_prob                     -0.0305* (0.0117)\n_______________ _________________ _________________\nS.E. type                     IID               IID\nObservations                   27                27\nR2                        0.25308           0.21445\nAdj. R2                   0.22321           0.18303\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#interpreting-a-within-relationship",
    "href": "2024/weeks/week08/slides.html#interpreting-a-within-relationship",
    "title": "🗓️ Week 8  Within variation",
    "section": "Interpreting a Within Relationship",
    "text": "Interpreting a Within Relationship\n\nHow can we interpret that slope of -0.03?\nThis is all within variation so our interpretation must be within-county\nSo, “comparing a county in year A where its arrest probability is 1 (100 percentage points) higher than it is in year B, we expect the number of crimes per person to drop by .03”\nOr if we think we’ve causally identified it (and want to work on a more realistic scale), “raising the arrest probability by 1 percentage point in a county reduces the number of crimes per person in that county by .0003”.\nWe’re basically “controlling for county” (and will do that explicitly in a moment)\nSo your interpretation should think of it in that way - holding county constant i.e. comparing two observations with the same value of county i.e. comparing a county to itself at a different point in time"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#concept-checks-1",
    "href": "2024/weeks/week08/slides.html#concept-checks-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy does subtracting the within-individual mean of each variable “control for individual”?\nIn a sentence, interpret the slope coefficient in the estimated model \\((Y_{it} - \\bar{Y}_i) = 2 + 3(X_{it} - \\bar{X}_i)\\) where \\(Y\\) is “blood pressure”, \\(X\\) is “stress at work”, and \\(i\\) is an individual person"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#the-least-squares-dummy-variable-approach",
    "href": "2024/weeks/week08/slides.html#the-least-squares-dummy-variable-approach",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Least Squares Dummy Variable Approach",
    "text": "The Least Squares Dummy Variable Approach\n\nDe-meaning the data isn’t the only way to do it!\nYou can also use the least squares dummy variable (another word for “binary variable”) method\nWe just treat “individual” like the categorical variable it is and add it as a control! Again, the regression approach is exactly the same as with any categorical control, but the research design reason for doing it is different"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#lets-do-it-1",
    "href": "2024/weeks/week08/slides.html#lets-do-it-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Let’s do it!",
    "text": "Let’s do it!\n\nlsdv &lt;- feols(crmrte ~ prbarr + factor(county), data = crime4)\netable(orig_data, de_mean, lsdv, keep = c('prbarr', 'demeaned_prob'))\n\n                        orig_data           de_mean              lsdv\nDependent Var.:            crmrte    demeaned_crime            crmrte\n                                                                     \nprbarr          0.0486** (0.0167)                   -0.0305* (0.0124)\ndemeaned_prob                     -0.0305* (0.0117)                  \n_______________ _________________ _________________ _________________\nS.E. type                     IID               IID               IID\nObservations                   27                27                27\nR2                        0.25308           0.21445           0.94114\nAdj. R2                   0.22321           0.18303           0.93044\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#the-same",
    "href": "2024/weeks/week08/slides.html#the-same",
    "title": "🗓️ Week 8  Within variation",
    "section": "The same!",
    "text": "The same!\n\nThe result is the same, as it should be\nExcept for that \\(R^2\\) - What is that “within R2”?\nBecause de-meaning takes out the part explained by the fixed effects ( \\(\\alpha_i\\) ) before running the regression, while LSDV does it in the regression\nSo the .94 is the portion of crmrte explained by prbarr and county, whereas the .21 is the “within - \\(R^2\\)” - the portion of the within variation that’s explained by prbarr\nNeither is wrong (and the .94 isn’t “better”), they’re just measuring different things"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#why-lsdv",
    "href": "2024/weeks/week08/slides.html#why-lsdv",
    "title": "🗓️ Week 8  Within variation",
    "section": "Why LSDV?",
    "text": "Why LSDV?\n\nA benefit of the LSDV approach is that it calculates the fixed effects \\(\\alpha_i\\) for you\nWe left those out of the table with the coefs argument of export_summs (we rarely want them) but here they are:\n\n\n\nOLS estimation, Dep. Var.: crmrte\nObservations: 27\nStandard-errors: IID \n                  Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)       0.045631   0.004116  11.08640 1.7906e-10 ***\nprbarr           -0.030491   0.012442  -2.45068 2.2674e-02 *  \nfactor(county)3  -0.025308   0.002165 -11.68996 6.5614e-11 ***\nfactor(county)7  -0.009870   0.001418  -6.96313 5.4542e-07 ***\nfactor(county)23 -0.008587   0.001258  -6.82651 7.3887e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.001933   Adj. R2: 0.930441\n\n\n\nInterpretation is exactly the same as with a categorical variable - we have an omitted county, and these show the difference relative to that omitted county"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#why-lsdv-1",
    "href": "2024/weeks/week08/slides.html#why-lsdv-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Why LSDV?",
    "text": "Why LSDV?\n\nThis also makes clear another element of what’s happening! Just like with a categorical var, the line is moving up and down to meet the counties\nGraphically, de-meaning moves all the points together in the middle to draw a line, while LSDV moves the line up and down to meet the points"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#why-not-lsdv",
    "href": "2024/weeks/week08/slides.html#why-not-lsdv",
    "title": "🗓️ Week 8  Within variation",
    "section": "Why Not LSDV?",
    "text": "Why Not LSDV?\n\nLSDV is computationally expensive\nIf there are a lot of individuals, or big data, or if you have many sets of fixed effects (yes you can do more than just individual - we’ll get to that next time!), it can be very slow\nMost professionally made fixed-effects commands use de-meaning, but then adjust the standard errors properly\n(They also leave the fixed effects coefficients off the regression table by default)"
  },
  {
    "objectID": "2024/weeks/week08/slides.html#concept-checks-2",
    "href": "2024/weeks/week08/slides.html#concept-checks-2",
    "title": "🗓️ Week 8  Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy can’t we use individual-person fixed effects to study the impact of race on traffic stops?\nThe within \\(R^2\\) from is .3, and the overall \\(R^2\\) is .5. Interpret these two numbers in sentences\nIn a sentence, interpret the slope coefficient in the estimated model \\((Y_{it} - \\bar{Y}_i) = 1 + .5(X_{it} - \\bar{X}_i)\\) where \\(Y\\) is “school funding per child” and \\(X\\) is “population growth”, and \\(i\\) is city\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#identification-error-1",
    "href": "2024/weeks/week02/slides.html#identification-error-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Identification Error",
    "text": "Identification Error\n\nBut the same “result” could give us a different conclusion, like kids who are aggressive play more video games or people notice aggression more when kids play video games, then\nwe have made an identification error - our result was not identified!*\nIdentification error is when your result in the data doesn’t actually have a clear theory (“why” or “because”)\nFor example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#data-generating-process-1",
    "href": "2024/weeks/week02/slides.html#data-generating-process-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nAnother example is a model of supply and demand\nWe observe prices and quantities in a competitive market\nWhat led to those being the prices and quantities we see?\nThe supply and demand model and its equilibrium, we theorize!"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#data-generating-process-2",
    "href": "2024/weeks/week02/slides.html#data-generating-process-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#data-generating-process-3",
    "href": "2024/weeks/week02/slides.html#data-generating-process-3",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nThe prices that we observe come from that theoretical construct\nWhen we see the prices and quantities moving, according to our theory, it is because the S and D lines are moving\nBut we cannot see the S and D lines\nOur goal: use the observations we do see to infer what the theoretical model (data generating process) is"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#data-generating-process-4",
    "href": "2024/weeks/week02/slides.html#data-generating-process-4",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nHarder than it sounds. What inference about S and D can we draw from these observations?"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#causality",
    "href": "2024/weeks/week02/slides.html#causality",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Causality",
    "text": "Causality\n\nA DGP can be described by a series of equations describing where the data comes from:\n\n\\[ X = \\gamma_0 + \\gamma_1\\varepsilon + \\nu \\]\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nThis says ” \\(X\\) is caused by \\(\\varepsilon\\) and \\(\\nu\\), and \\(Y\\) is caused by \\(X\\) and \\(\\varepsilon\\)” - an increase in \\(X\\) causally increases \\(Y\\) by \\(\\beta_1\\)\nThe goal of econometrics is to be able to estimate what \\(\\beta_1\\) is accurately"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#causality-1",
    "href": "2024/weeks/week02/slides.html#causality-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Causality",
    "text": "Causality\n\nWe can also represent this set of relationships as a graph, with arrows telling you what variables cause each other"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#causality-2",
    "href": "2024/weeks/week02/slides.html#causality-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Causality",
    "text": "Causality\n\nWe do this because most of the relationships we’re interested in are causal - we want to know, if we could reach in and manipulate \\(X\\), would \\(Y\\) change as a result, and how much?\nDoes the minimum wage reduce employment?\nDoes quantitative easing avert recessions?\nDoes six-sigma improve business performance?\nDoes getting an MBA make you a better manager?"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#x-and-y",
    "href": "2024/weeks/week02/slides.html#x-and-y",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "\\(X\\) and \\(Y\\)",
    "text": "\\(X\\) and \\(Y\\)\n\nI have an \\(X\\) value of 2.5 and want to predict what \\(Y\\) will be. What can I do?"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#x-and-y-1",
    "href": "2024/weeks/week02/slides.html#x-and-y-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "\\(X\\) and \\(Y\\)",
    "text": "\\(X\\) and \\(Y\\)\n\nI can’t just say “just predict whatever values of \\(Y\\) we see for \\(X = 2.5\\), because there are multiple of those!\nPlus, what if we want to predict for a value we DON’T have any actual observations of, like \\(X = 4.3\\)?"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#data-is-granular",
    "href": "2024/weeks/week02/slides.html#data-is-granular",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data is Granular",
    "text": "Data is Granular\n\nIf I try to fit every point, I’ll get a mess that won’t really tell me the relationship between \\(X\\) and \\(Y\\)\nSo, we simplify the relationship into a shape: a line! The line smooths out those three points around 2.5 and fills in that gap around 4.3"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#isnt-this-worse",
    "href": "2024/weeks/week02/slides.html#isnt-this-worse",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Isn’t This Worse?",
    "text": "Isn’t This Worse?\n\nBy adding a line, we are necessarily simplifying our presentation of the data. We’re tossing out information!\nOur prediction of the data we have will be less accurate than if we just make predictions point-by-point\nHowever, we’ll do a better job predicting other data (avoiding “overfitting”)\nAnd, since a shape is something we can interpret, as opposed to a long list of predictions, which we can’t really, the line will do a better job of telling us about the true underlying relationship"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#the-line-does-a-few-things",
    "href": "2024/weeks/week02/slides.html#the-line-does-a-few-things",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "The Line Does a Few Things:",
    "text": "The Line Does a Few Things:\n\nWe can get a prediction of \\(Y\\) for a given value of \\(X\\) (If we follow \\(X = 2.5\\) up to our line we get \\(Y = 7.6\\))\nWe see the relationship: the line slopes up, telling us that “more \\(X\\) means more \\(Y\\) too!”"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#lines",
    "href": "2024/weeks/week02/slides.html#lines",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nThat line we get is the fit of our model\nA model “fit” means we’ve taken a shape (our line) and picked the one that best fits our data\nAll forms of regression do this\nOrdinary least squares specifically uses a straight line as its shape\nThe resulting line we get can also be written out as an actual line, i.e.\n\n\\[ Y = intercept + slope*X \\]"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#lines-1",
    "href": "2024/weeks/week02/slides.html#lines-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nWe can use that line as… a line!\nIf we plug in a value of \\(X\\), we get a prediction for \\(Y\\)\nBecause these \\(Y\\) values are predictions, we’ll give them a hat \\(\\hat{Y}\\)\n\n\\[ Y = 3 + 4*X \\]\n\\[ \\hat{Y} = 3 + 4*(3.2) \\]\n\\[ \\hat{Y} = 15.8 \\]"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#lines-2",
    "href": "2024/weeks/week02/slides.html#lines-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nWe can also use it to explain the relationship\nWhatever the intercept is, that’s what we predict for \\(Y\\) when \\(X = 0\\)\n\n\\[ Y = 3 + 4*X \\]\n\\[ \\hat{Y} = 3 + 4*0 \\]\n\\[ \\hat{Y} = 3 \\]"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#lines-3",
    "href": "2024/weeks/week02/slides.html#lines-3",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nAnd as \\(X\\) increases, we know how much we expect \\(Y\\) to increase because of the slope\n\n\\[ Y = 3 + 4*X \\]\n\\[ \\hat{Y} = 3 + 4*3 = 15 \\]\n\\[ \\hat{Y} = 3 + 4*4 = 19 \\]\n\nWhen \\(X\\) increases by \\(1\\), \\(Y\\) increases by the slope (which is \\(4\\) here)"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ordinary-least-squares",
    "href": "2024/weeks/week02/slides.html#ordinary-least-squares",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nRegression fits a shape to the data\nOLS specifically fits a straight line to the data described using an \\(intercept\\) and a \\(slope\\)\nWhen we plug an \\(X\\) into the line, we get a prediction for \\(Y\\), which we call \\(\\hat{Y}\\)\nWhen \\(X = 0\\), we predict \\(\\hat{Y} = intercept\\)\nWhen \\(X\\) increases by \\(1\\), our prediction of \\(Y\\) increases by the \\(slope\\)\nIf \\(slope &gt; 0\\), \\(X\\) and \\(Y\\) are positively related/correlated"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#concept-checks",
    "href": "2024/weeks/week02/slides.html#concept-checks",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nHow does producing a line let us use \\(X\\) to predict \\(Y\\)?\nIf our line is \\(Y = 5 - 2*X\\), explain what the \\(-2\\) means in a sentence\nNot all of the points are exactly on the line, meaning some of our predictions will be wrong! Should we be concerned? Why or why not?"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#how",
    "href": "2024/weeks/week02/slides.html#how",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "How?",
    "text": "How?\n\nWe know that regression fits a line\nBut how does it do that exactly?\nIt picks the line that produces the smallest squares\nThus, “ordinary least squares”"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#predictions-and-residuals",
    "href": "2024/weeks/week02/slides.html#predictions-and-residuals",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Predictions and Residuals",
    "text": "Predictions and Residuals\n\nWhenever you make a prediction of any kind, you rarely get it exactly right\nThe difference between your prediction and the actual data is the residual\n\n\\[ Y = 3 + 4*X \\]\nIf we have a data point where \\(X = 4\\) and \\(Y = 18\\), then\n\\[ \\hat{Y} = 3 + 4*4 = 19 \\]\nThen the residual is \\(Y - \\hat{Y} = 18 - 19 = -1\\)."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#predictions-and-residuals-1",
    "href": "2024/weeks/week02/slides.html#predictions-and-residuals-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Predictions and Residuals",
    "text": "Predictions and Residuals\nSo really, our relationship doesn’t look like this…\n\\[ Y = intercept + slope*X \\]\nInstead, it’s…\n\\[ Y = intercept + slope*X + residual \\]\nWe still use \\(intercept + slope*X\\) to predict \\(Y\\) though, so this is also\n\\[ Y = \\hat{Y} + residual \\]"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ordinary-least-squares-1",
    "href": "2024/weeks/week02/slides.html#ordinary-least-squares-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nAs you’d guess, a good prediction should make the residuals as small as possible\nWe want to pick a line to do that\nAnd in particular, we’re going to square those residuals, so the really-big residuals count even more. We really don’t want to have points that are super far away from the line!\nThen, we pick a line to minimize those squared residuals (“least squares”)"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ordinary-least-squares-2",
    "href": "2024/weeks/week02/slides.html#ordinary-least-squares-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nStart with our data"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ordinary-least-squares-3",
    "href": "2024/weeks/week02/slides.html#ordinary-least-squares-3",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nLet’s just pick a line at random, not necessarily from OLS"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ordinary-least-squares-4",
    "href": "2024/weeks/week02/slides.html#ordinary-least-squares-4",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nThe vertical distance from point to line is the residual"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ordinary-least-squares-5",
    "href": "2024/weeks/week02/slides.html#ordinary-least-squares-5",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nNow square those residuals"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ordinary-least-squares-6",
    "href": "2024/weeks/week02/slides.html#ordinary-least-squares-6",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nCan we get the total area in the squares smaller with a different line?"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ordinary-least-squares-7",
    "href": "2024/weeks/week02/slides.html#ordinary-least-squares-7",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nOrdinary Least Squares, I can promise you, gets it the smallest"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ordinary-least-squares-8",
    "href": "2024/weeks/week02/slides.html#ordinary-least-squares-8",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nHow does it figure out which line makes the smallest squares?\nThere’s a mathematical formula for that!\nFirst, instead of thinking of \\(intercept\\) and \\(slope\\), we reframe the line as having parameters we can pick\n\n\\[ Y = intercept + slope*X + residual \\]\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#terminology-sidenote",
    "href": "2024/weeks/week02/slides.html#terminology-sidenote",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Terminology Sidenote",
    "text": "Terminology Sidenote\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nIn metrics, Greek letters represent “the truth” - in the true process by which the data is generated, a one-unit increase in \\(X\\) is related to a \\(\\beta_1\\) increase in \\(Y\\)\nWhen we put a hat on anything, that is our prediction or estimation of that true thing. \\(\\hat{Y}\\) is our prediction of \\(Y\\), and \\(\\hat{\\beta_1}\\) is our estimate of what we think the true \\(\\beta_1\\) is\nNote “residual” =/= \\(\\varepsilon\\) - residuals are what’s actually left over from our prediction with real data, but the error \\(\\varepsilon\\) is the true difference between our line and \\(Y\\)."
  },
  {
    "objectID": "2024/syllabus.html",
    "href": "2024/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Quantitative data collection is an integral component of behavioural science: Testing hypotheses requires designing experiments and analysing the data or performing statistical analyses on secondary data. Whereas another core course in this programme - Experimental Design and Methods for the Behavioural Science - covers best practices in designing and conducting experimental research, Quantitative Applications for Behavioural Science introduces the main statistical background of behavioural research from psychology and economics. The course will cover best practices and state of the art statistical tools that are used by psychologists and economists. All the analyses will be demonstrated on example behavioural science research, and students will learn how to identify, interpret, and evaluate appropriate analyses for different research designs, conduct their own data analysis for each of these designs as well as report the analysis for publication in a journal, and recognise and understand contemporary issues in data science analysis in psychology and economics that need to be considered for best research practices. Emphasis will be on teaching students how the same analyses are presented in psychology and economics journals so students can understand how to integrate research from these two fields that constitute behavioural science."
  },
  {
    "objectID": "2024/syllabus.html#description",
    "href": "2024/syllabus.html#description",
    "title": "Syllabus",
    "section": "",
    "text": "Quantitative data collection is an integral component of behavioural science: Testing hypotheses requires designing experiments and analysing the data or performing statistical analyses on secondary data. Whereas another core course in this programme - Experimental Design and Methods for the Behavioural Science - covers best practices in designing and conducting experimental research, Quantitative Applications for Behavioural Science introduces the main statistical background of behavioural research from psychology and economics. The course will cover best practices and state of the art statistical tools that are used by psychologists and economists. All the analyses will be demonstrated on example behavioural science research, and students will learn how to identify, interpret, and evaluate appropriate analyses for different research designs, conduct their own data analysis for each of these designs as well as report the analysis for publication in a journal, and recognise and understand contemporary issues in data science analysis in psychology and economics that need to be considered for best research practices. Emphasis will be on teaching students how the same analyses are presented in psychology and economics journals so students can understand how to integrate research from these two fields that constitute behavioural science."
  },
  {
    "objectID": "2024/weeks/week01/slides.html#what-is-this-class",
    "href": "2024/weeks/week01/slides.html#what-is-this-class",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What is this class",
    "text": "What is this class\n\nIt’s a research design course on quasi-experimental methods\nLet’s break it down:\n\nResearch Design -&gt; How you transform an idea / question about the world to applied research\nQuasi-Experiments -&gt; Not by designing new experiments with random assignment. (how? We will see over the next 11 weeks)"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#what-you-should-expect",
    "href": "2024/weeks/week01/slides.html#what-you-should-expect",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What you should expect",
    "text": "What you should expect\n\nConfidence: You will feel like you have a good understanding of design-based causal inference by the end such that it doesn’t feel mysterious or intimidating\nComprehension: You will have learned a lot both conceptually but also in various specifics, particularly with regards to issues around identification and estimation\nCompetency: You will have had some experience working together implementing these methods using code in Stata syntax"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#lectures-plan",
    "href": "2024/weeks/week01/slides.html#lectures-plan",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Lectures plan",
    "text": "Lectures plan\n\nWeek 2 - Linear regressions\nWeek 3 - Hypothesis testing\nWeek 4 – Linear regressions with multiple regressors / Non-linear functions\nWeek 5 – Regressions on Binary variables\nWeek 6 – BREAK!!! (eeeehm Reading Week)\nWeek 7 – Recap & POTENTIAL OUTCOMES\nWeek 8 – Panel regressions\nWeek 9 – Regression Discontinuity Designs\nWeek 10 – Instrumental Variables\nWeek 11 – Difference in Differences"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#what-is-a-good-research-question",
    "href": "2024/weeks/week01/slides.html#what-is-a-good-research-question",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What is a good research question?",
    "text": "What is a good research question?\n\nComing up with questions is easy.\nBut coming up with good ones, is tricky. Good RQ:"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#what-is-a-good-research-question-1",
    "href": "2024/weeks/week01/slides.html#what-is-a-good-research-question-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What is a good research question?",
    "text": "What is a good research question?\n\nComing up with questions is easy.\nBut coming up with good ones, is tricky. Good RQ:\n\nA question that can be answered / Researchable:\nHow can you answer a question which is unanswerable?\nWhat versus why? Are you trying to determine what causes Y, or why something causes Y.\n\nImprove our understanding of the world:\n\nDoesn’t have to shake the foundations of science and human knowledge\nWhat if I find an unexpected result?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#research-designs",
    "href": "2024/weeks/week01/slides.html#research-designs",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs",
    "text": "Research Designs\n\nQuantitative empirical analysis uses data to explore, test or estimate a relationship."
  },
  {
    "objectID": "2024/weeks/week01/slides.html#research-designs-1",
    "href": "2024/weeks/week01/slides.html#research-designs-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs",
    "text": "Research Designs\n\nFrom a broad spectrum of methodologies, we will cover:"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causal-inference",
    "href": "2024/weeks/week01/slides.html#causal-inference",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nContemplating interventios that change behaviour:\n\nHow would littering parks change if we increase the severity of fines?\n\nIs public shaming more effective?\n\nWhat if we increase other types of fines (i.e. driving)?\n\nWill people commit less crimes?\n\n\n\nEach of these policies is asking what happens to some outcome if we make an intervention - keep everything the same but change one factor –"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#a-little-throwback",
    "href": "2024/weeks/week01/slides.html#a-little-throwback",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "A little throwback",
    "text": "A little throwback\n\nOctober 2021’s Nobel Prize in economics went to D. Card, J. Angrist and G. Imbens\nBut it’s arguably as much to Princeton’s mid 1980s Industrial Relations group as it’s ground zero for the credibility revolution\nStarts with Orley Ashenfelter, who had been working on job trainings programs\nKEY individuals: Orley Ashenfelter, David Card (Orley’s student), Josh Angrist (Card and Orley’s student), Alan Krueger (hired by Orley), Bob Lalonde (Card and Orley’s student) and then a generation of students (Levine, Currie, Pischke)"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#a-little-throwback-1",
    "href": "2024/weeks/week01/slides.html#a-little-throwback-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "A little throwback",
    "text": "A little throwback\n\nAngrist started working on how randomization in Vietnam drafting can explain later outcomes (we will see this in Week 10)\nMeets Gibens and they both get mentored by Gary Chamberlain\nThey propose the potential outcomes framework\nThis course is about these people, their ideas, subsequent development and how the revolutionised modern empirical research with observational data"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#introduction-to-counterfactuals-1",
    "href": "2024/weeks/week01/slides.html#introduction-to-counterfactuals-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#introduction-to-counterfactuals-2",
    "href": "2024/weeks/week01/slides.html#introduction-to-counterfactuals-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment\nAliens come and orbit earth, in superposition.\n\nThey see sick people in hospitals\nWhat do they? think?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#introduction-to-counterfactuals-3",
    "href": "2024/weeks/week01/slides.html#introduction-to-counterfactuals-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment\nAliens come and orbit earth, in superposition.\n\nThey see sick people in hospitals\nWhat do they? think?\nHospitals kill people. What is the difference? Doctors?\n\nThey kill the doctors, unplug patients from machines, throw open the doors – many more patients inexplicably die\nSounds ridiculous?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#introduction-to-counterfactuals-4",
    "href": "2024/weeks/week01/slides.html#introduction-to-counterfactuals-4",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment\nAliens come and orbit earth, in superposition.\n\nThey see sick people in hospitals\nWhat do they? think?\nHospitals kill people. What is the difference? Doctors?\n\nThey kill the doctors, unplug patients from machines, throw open the doors – many more patients inexplicably die\nSounds ridiculous?\nAren’t we all aliens in our research?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#three-types-of-errors",
    "href": "2024/weeks/week01/slides.html#three-types-of-errors",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Three types of errors",
    "text": "Three types of errors\n\nCorrelation =/= causation"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#three-types-of-errors-1",
    "href": "2024/weeks/week01/slides.html#three-types-of-errors-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Three types of errors",
    "text": "Three types of errors\n\nCorrelation =/= causation\nSomething Happening first may not imply causality (rooster)"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#three-types-of-errors-2",
    "href": "2024/weeks/week01/slides.html#three-types-of-errors-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Three types of errors",
    "text": "Three types of errors\n\nCorrelation =/= causation\nSomething Happening first may not imply causality (rooster)\nNo correlation does not imply no causation"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#research-designs-and-causality-1",
    "href": "2024/weeks/week01/slides.html#research-designs-and-causality-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs and Causality",
    "text": "Research Designs and Causality\nExample: If we want to know whether a vaccine works\n\nWe compare people who have gotten vaccinated and those who took a placebo instead"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#research-designs-and-causality-2",
    "href": "2024/weeks/week01/slides.html#research-designs-and-causality-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs and Causality",
    "text": "Research Designs and Causality\nExample: If we want to know whether a vaccine works\n\nWe compare people who have gotten vaccinated and those who took a placebo instead\nIn a classic clinical experiment, one applies a ‘treatment’ (0 = placebo, 1 = vaccine) to some set of n ‘subjects’ and observes some ‘outcome’ (Y).\nWe can then estimate:\n\nY = infection(0,1)"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#research-designs-and-causality-3",
    "href": "2024/weeks/week01/slides.html#research-designs-and-causality-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs and Causality",
    "text": "Research Designs and Causality\n\nEach individual i is assigned into one of the treatment options (0 = placebo, 1 = vaccine)\nTherefore, each i as two potential outcomes:\n\nWhat would happen if they got the placebo? Yi (0)\nWhat would happen if they got the vaccine? Yi (1)\n\nDid vaccines prevent infection?\n\nTo answer this we need to know what happened to the individual if they got the vaccine and what happened to the same individual if they got the placebo."
  },
  {
    "objectID": "2024/weeks/week01/slides.html#counterfactuals",
    "href": "2024/weeks/week01/slides.html#counterfactuals",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nWhat actually happened (i.e., the ‘factual’):\n\nI got the vaccine and did not get sick\nTreatment (X) = 1\nObserved outcome = Yi(1)\n\nThe counterfactual: (what would have happened)\n\nIf I did not get the vaccine, would I have fallen sick?\nCounterfactual treatment (X) = 0\nCounterfactual outcome = Yi(0)"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#counterfactuals-1",
    "href": "2024/weeks/week01/slides.html#counterfactuals-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nAfter treatment is assigned there is potential for only one outcome to be observed"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#counterfactuals-2",
    "href": "2024/weeks/week01/slides.html#counterfactuals-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nBut ideally we would like to observe two:"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#fundamental-problem-of-causal-inference",
    "href": "2024/weeks/week01/slides.html#fundamental-problem-of-causal-inference",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\n\nOnce we observe one treatment for one individual, we cannot observe a different treatment for the same individual.\nThis is called the “fundamental problem of causal inference.” Each potential outcome is observable, but we can never observe all of them.” (Rubin, 2005, p. 323).\nThen, why are we discussing all these?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#fundamental-problem-of-causal-inference-1",
    "href": "2024/weeks/week01/slides.html#fundamental-problem-of-causal-inference-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\n\nOnce we observe one treatment for one individual, we cannot observe a different treatment for the same individual.\nThis is called the “fundamental problem of causal inference.” Each potential outcome is observable, but we can never observe all of them.” (Rubin, 2005, p. 323).\nThen, why are we discussing all these?\nWe can observe different treatments across different people.\nThis may be a way of solving the fundamental problem, but it introduces a new problem we must consider."
  },
  {
    "objectID": "2024/weeks/week01/slides.html#selection-bias",
    "href": "2024/weeks/week01/slides.html#selection-bias",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Selection Bias",
    "text": "Selection Bias\n\nThis new problem arises because different people are… DIFFERENT!"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#selection-bias-1",
    "href": "2024/weeks/week01/slides.html#selection-bias-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Selection Bias",
    "text": "Selection Bias\n\nDifferences between people following a treatment may be because of the treatment, or they may be because of the differences in the people being treated.\nThis is selection bias.\nLet’s consider some other factors which may matter for selection bias."
  },
  {
    "objectID": "2024/weeks/week01/slides.html#addressing-selection-bias",
    "href": "2024/weeks/week01/slides.html#addressing-selection-bias",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Addressing Selection Bias",
    "text": "Addressing Selection Bias\n\nSelect a large enough random sample and divide them into two groups.\n\nCharacteristics which contribute to selection bias should on average be distributed the same between both groups.\nTherefore, we expect that the treatment and control groups should differ only because of the treatment, and in absence of the treatment, would produce the same results."
  },
  {
    "objectID": "2024/weeks/week01/slides.html#addressing-selection-bias-1",
    "href": "2024/weeks/week01/slides.html#addressing-selection-bias-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Addressing Selection Bias",
    "text": "Addressing Selection Bias\n\nEach group differs within the group…\nBut, on average, the groups themselves are the same, and so are comparable.\nThe effect of treatment on average would then be:\nE(Y | T = 1) – E(Y | T = 0) = Average Treatment Effect (ATE)"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#treatment-effect",
    "href": "2024/weeks/week01/slides.html#treatment-effect",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Treatment effect",
    "text": "Treatment effect\n\nThe effect of the intervention then would be:\n\nTreatment effect of intervention = Outvome of Treated - Outcome of Untreated + Selection Bias\nSelection bias is the difference in average outcomes between treatment and control groups due to factors other than the treatment status\nThe true treatment effect, selection bias needs to be eliminated, or shown to be reasonably assumed to be zero.\nTo eliminate selection bias, we need well designed experiments (Matteo’s class) and large enough samples"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#experiments-not-always-the-solution",
    "href": "2024/weeks/week01/slides.html#experiments-not-always-the-solution",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Experiments not always the solution",
    "text": "Experiments not always the solution\n\nTime consuming and expensive (large samples)\nMay have ethical issues\nSuffer from drop-out and non-compliance\nEstimated parameters in an experiment may different from the parameters in the field.\nNot very easy to observe ‘real’ behaviours or consequential behaviours because of the setting.\nAn interesting paper on the limits of RCTs from Deaton (Nobel laureate) and Cartwright (2017), if you’re interested!\nWhat do we do then?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causal-inference-1",
    "href": "2024/weeks/week01/slides.html#causal-inference-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causal inference",
    "text": "Causal inference\n\nWe design a strategy (Identification Strategy from now on) that allows us to:\n\nIdentify and isolate the random variation in treatment (i.e. a natural disaster)\nRely on institutional knowledge, theory and data to:\n\nReduce as much as possible Selection Bias\nIdentify outcomes for treated and untreated populations\nEstimate average treatment effects"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#data-generating-process-1",
    "href": "2024/weeks/week01/slides.html#data-generating-process-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nAnother example is a model of supply and demand\nWe observe prices and quantities in a competitive market\nWhat led to those being the prices and quantities we see?\nThe supply and demand model and its equilibrium, we theorize!"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#data-generating-process-2",
    "href": "2024/weeks/week01/slides.html#data-generating-process-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#data-generating-process-3",
    "href": "2024/weeks/week01/slides.html#data-generating-process-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nThe prices that we observe come from that theoretical construct\nWhen we see the prices and quantities moving, according to our theory, it’s because the S and D lines are moving\nBut we can’t see the S and D lines\nOur goal: use the observations we do see to infer what the theoretical model (data generating process) is"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#data-generating-process-4",
    "href": "2024/weeks/week01/slides.html#data-generating-process-4",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nHarder than it sounds. What inference about S and D can we draw from these observations?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causality-1",
    "href": "2024/weeks/week01/slides.html#causality-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nWe can also represent this set of relationships as a graph, with arrows telling you what variables cause each other"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causality-2",
    "href": "2024/weeks/week01/slides.html#causality-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nWe do this because most of the relationships we’re interested in are causal - we want to know, if we could reach in and manipulate \\(X\\), would \\(Y\\) change as a result, and how much?\nDoes the minimum wage reduce employment?\nDoes quantitative easing avert recessions?\nDoes six-sigma improve business performance?\nDoes getting an MBA make you a better manager?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causality-3",
    "href": "2024/weeks/week01/slides.html#causality-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nImagine this is the graph we see for minimum wage and employment"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causality-4",
    "href": "2024/weeks/week01/slides.html#causality-4",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nDoes that mean that the minimum wage harms employment?\nMaybe! But also maybe not\nWhat the graph shows us is a correlation\nAnd correlation is not the same thing as causation"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causality-5",
    "href": "2024/weeks/week01/slides.html#causality-5",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nA given correlation, like the negative relationship between minimum wage changes and employment changes, can be consistent with a number of different causal relationships\nAs econometricians, we need to figure out which one it is!\nHow can we narrow it down?\nHow many of the diagrams on the next page can be consistent with that negative relationship?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#eight-possible-relationships",
    "href": "2024/weeks/week01/slides.html#eight-possible-relationships",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Eight Possible Relationships",
    "text": "Eight Possible Relationships"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causality-6",
    "href": "2024/weeks/week01/slides.html#causality-6",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nThe only ones we can eliminate are d, g, and h\nAll the rest are possible!\nIf f is correct, we see the negative relationship even though minimum wage has nothing to do with causing employment (like the ice cream and shorts example)\nIf a is correct, then even though we know minimum wage causes employment to change, the size or even direction of the relationship will be wrong (why?)"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causality-7",
    "href": "2024/weeks/week01/slides.html#causality-7",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nSo which of them is likely to be correct?\nThat depends on what we think \\(\\varepsilon\\) is\n\\(\\varepsilon\\) is everything that determines \\(Y\\) other than \\(X\\)\nPerhaps the health of the economy, or the policies that area has chosen\nSo we almost certainly have a graph with \\(\\varepsilon \\rightarrow Y\\)\nDo those things also affect the choice to raise the minimum wage? If so we’re in graph a. That downward relationship could be due to a null relationship, or even a positive one (or perhaps a more negative one?)"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#what-follows",
    "href": "2024/weeks/week01/slides.html#what-follows",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What follows?",
    "text": "What follows?\n\nSeminar today:\n\nIntro to Workflows and Stata\n\nWeek 2: Hypothesis testing\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#background-1",
    "href": "2024/weeks/week01/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Background",
    "text": "Background\n\nThink about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#code-and-software",
    "href": "2024/weeks/week01/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Code and Software",
    "text": "Code and Software\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#making-mistakes",
    "href": "2024/weeks/week01/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#making-mistakes-1",
    "href": "2024/weeks/week01/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#anna-karenina-principle",
    "href": "2024/weeks/week01/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Anna Karenina Principle",
    "text": "Anna Karenina Principle\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#what-do-we-learn",
    "href": "2024/weeks/week01/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "What do we learn?",
    "text": "What do we learn?\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#workflow-1",
    "href": "2024/weeks/week01/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#empirical-workflow",
    "href": "2024/weeks/week01/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#checklist",
    "href": "2024/weeks/week01/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2024/weeks/week01/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#step-1---read-the-codebook",
    "href": "2024/weeks/week01/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#do-not-touch-the-original-data",
    "href": "2024/weeks/week01/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2024/weeks/week01/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2024/weeks/week01/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#step-2---helping-your-future-self",
    "href": "2024/weeks/week01/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2024/weeks/week01/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#step-2---automation",
    "href": "2024/weeks/week01/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#step-2---beautiful-code",
    "href": "2024/weeks/week01/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#step-3---eyeballing",
    "href": "2024/weeks/week01/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#step-3---eyeballing-1",
    "href": "2024/weeks/week01/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2024/weeks/week01/slides_sem.html#step-4---missing-observations",
    "href": "2024/weeks/week01/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#your-lecturer",
    "href": "2024/weeks/week00/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\nDr.  George Melios Research Fellow www.georgemelios.com\n\n\n\n\n\nPhD in Economics\nBackground: Political Economy, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#teaching-assistant",
    "href": "2024/weeks/week00/slides.html#teaching-assistant",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\n\n\n\n\nDr. Lazaros Chatzilazarou PhD Candidate Website\n\n\n\n\n\nPhD in Economics (exp 2026)\nBackground: Economics\n\nExperimental Economics"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#housekeeping",
    "href": "2024/weeks/week00/slides.html#housekeeping",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nMoodle (lecture videos and assignments)\nWebsite\nOffice Hours (Book Upfront!!!)"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#what-is-pb4a7",
    "href": "2024/weeks/week00/slides.html#what-is-pb4a7",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is PB4A7",
    "text": "What is PB4A7\n\nNot a maths course!\nNot a pure stats course!\nNot a theoretical econometrics course!\nNot a data science course!"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#what-is-pb4a7-1",
    "href": "2024/weeks/week00/slides.html#what-is-pb4a7-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is PB4A7",
    "text": "What is PB4A7"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#what-is-this-class",
    "href": "2024/weeks/week00/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is an applied econometrics class for behavioural science\nEconometrics is a field that covers how economists think about statistical analysis\nWhy do we care about econometrics?\n\nMany other social science fields (even epidemiology) pick up econometric tools as well because of how useful they tend to be"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#so-what-is-econometrics",
    "href": "2024/weeks/week00/slides.html#so-what-is-econometrics",
    "title": "🗓️ Week 0 Presessionals",
    "section": "So what is econometrics?",
    "text": "So what is econometrics?\n\nEconometrics focuses on the study of observational data\nObservational data are measurements of things that the researcher does not control\nGiven that we are working with observational data, we still want to understand the causes of things\nThe world is what it is\nFrom next week onwards though, we will explore ways to study it"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#welcome-to-econometrics",
    "href": "2024/weeks/week00/slides.html#welcome-to-econometrics",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Welcome to Econometrics",
    "text": "Welcome to Econometrics\n\nThis is a great course (tough one but great)\nWhy?\nGives you the ability to think about and answer questions you are interested in\nAnd to better understand and judge the existing body of literature\n\nThe classic Tik-Tok, Instagram video that starts with “A new study says…”? You can now have an idea of how robust/serious their inferences are."
  },
  {
    "objectID": "2024/weeks/week00/slides.html#why-applications-and-not-econometrics",
    "href": "2024/weeks/week00/slides.html#why-applications-and-not-econometrics",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Why applications and not econometrics?",
    "text": "Why applications and not econometrics?\n\nPB4A7 and PB413 are applied courses. You need to know how to use statistics, and why you’re using them – you will not master the nuts or bolts of statistical theory!\nFor those who want more information\n\nWill provide material on the website of the class\nVisit me during the office hours\nWe will discuss additional courses to audit"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#admin",
    "href": "2024/weeks/week00/slides.html#admin",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Admin",
    "text": "Admin\n\nReview the syllabus (and other materials on Moodle and the PB4A7 website). Reading assignments there\nOur textbook is The Effect, by Huntington-Klein, available online for free.\nAlso these slides\nProgramming in STATA (we will get to this on later)\nAssignment: End of term paper & poster"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#textbook",
    "href": "2024/weeks/week00/slides.html#textbook",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Textbook",
    "text": "Textbook"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#causality-and-prediction",
    "href": "2024/weeks/week00/slides.html#causality-and-prediction",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nGreat! Still, why do we care about this class?\nIn econometrics, we are working with data\nStatisticians also work with data\nSo do data scientists\nThe goals for these groups differ"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#causality-and-prediction-1",
    "href": "2024/weeks/week00/slides.html#causality-and-prediction-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nData scientists are generally concerned with prediction\nThey want to use the data at hand to predict what comes next\nThey generally don’t care why they’re making the prediction they are\nThis can be really handy for certain tasks - “is this picture a cat or a dog?” “what’s the probability that a customer with qualities X, Y, and Z will end up purchasing our good?” “do you have lymphoma?”"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#causality-and-prediction-2",
    "href": "2024/weeks/week00/slides.html#causality-and-prediction-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nEconometricians, on the other hand, care almost exclusively about why\nData scientists want to minimize prediction error\nEconometricians want to minimize inference and identification error\nWe want to correctly understand the underlying data generating process"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#data",
    "href": "2024/weeks/week00/slides.html#data",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#data-1",
    "href": "2024/weeks/week00/slides.html#data-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#data-2",
    "href": "2024/weeks/week00/slides.html#data-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#inference-error-and-randomness-1",
    "href": "2024/weeks/week00/slides.html#inference-error-and-randomness-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Inference Error and Randomness",
    "text": "Inference Error and Randomness\n\nSo if we look in a data set and see that \\(X\\) and \\(Y\\) appear to be positively related to each other…\nAre they actually positively related, or is that just a random chance?\nIf they are positively related, maybe we’re understating or overstating how positively related"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#inference-error-and-randomness-2",
    "href": "2024/weeks/week00/slides.html#inference-error-and-randomness-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Inference Error and Randomness",
    "text": "Inference Error and Randomness\n\nIf the true relationship is 0, then in the data we’ll see a positive relationship half the time, and a negative relationship half the time\nEven though the truth is 0!\nHow can we properly make an inference about whether the relationship is 0 or not (or positive, or negative, or how positive or negative), taking into account this randomness?\nThat’s being careful about inference. The statisticians teach us all about this!"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#identification-error-1",
    "href": "2024/weeks/week00/slides.html#identification-error-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Identification Error",
    "text": "Identification Error\n\nBut if there’s another reason why we might see that result, i.e. if the same result could give us a different conclusion, like kids who are aggressive play more video games or people notice aggression more when kids play video games, then we have made an identification error - our result was not identified!\nIdentification error is when your result in the data doesn’t actually have a clear theoretical (“why” or “because”) interpretation"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#truth-and-reality-1",
    "href": "2024/weeks/week03/slides.html#truth-and-reality-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Truth and Reality",
    "text": "Truth and Reality\n\nToday we’ll be covering hypothesis testing, which is one approach to using reality to get closer to the truth\nIt works by subtraction\nWe test whether certain versions of the truth are likely or unlikely\nAnd if we find that they’re unlikely, we can reject that version of the truth, narrowing down what the actual possibilities are and getting closer and closer to the actual truth"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#truth-and-reality-2",
    "href": "2024/weeks/week03/slides.html#truth-and-reality-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Truth and Reality",
    "text": "Truth and Reality\n\nWhen we’re talking about the truth here, we’re referring to the true data generating process (DGP)\nFor example, if this is the true DGP:\n\n\\[ Wage_i = \\beta_0 + \\beta_1AdultHeight_i + \\varepsilon_i \\]\nwhere \\(cor(AdultHeight_i,\\varepsilon_i) = 0\\), then…\n\nPerson \\(i\\)’s wage is truly determined by a linear function of your height, plus an unrelated error term \\(\\varepsilon_i\\)\nWhy might someone have a high wage? Either they’re tall, or they have a high error term, or both. No other way!"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#truth-and-reality-3",
    "href": "2024/weeks/week03/slides.html#truth-and-reality-3",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Truth and Reality",
    "text": "Truth and Reality\n\nOur ability to estimate that true model depends on our ability to avoid inference and identification error\nIf we assume that’s the true DGP, there’s no endogeneity, and the relationship between \\(Wage\\) and \\(AdultHeight\\) is a straight line, so regular ’ol OLS of \\(Wage\\) on \\(AdultHeight\\) will not give us identification error\nBut we also need to be careful about inference error\nWhen we run that regression, what does our \\(\\hat{\\beta}_1\\) say about the true value \\(\\beta_1\\)?"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#sampling-variation-1",
    "href": "2024/weeks/week03/slides.html#sampling-variation-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Sampling Variation",
    "text": "Sampling Variation\n\nNotice that there is plenty of variation around the true value of \\(2\\)\nNow let’s imagine we don’t know that and are trying to answer the question “is the truth \\(\\beta_1 = 2\\)?”\nWe don’t have the full sampling distribution, we just have a single estimate:\n\n\n\n(Intercept)           X \n   2.798554    2.215880 \n\n\n\nAll we see is \\(\\hat{\\beta}_1 =\\) 2.22. So… is \\(\\beta_1 = 2\\)?"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#null-distribution",
    "href": "2024/weeks/week03/slides.html#null-distribution",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nThe “null distribution” is what the sampling distribution of the estimator would be if our null distribution were true\nWe can see that in the sampling distribution we have!\n\\(\\beta_1 = 2\\) is true, and here’s what the sampling distribution looks like! (although it would be smoother with more samples)"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#null-distribution-1",
    "href": "2024/weeks/week03/slides.html#null-distribution-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nSo the key question that a hypothesis test asks is: given this null distribution, how unlikely is it that we get the result we get?\nIf it’s super unlikely that the null is true and we get our result, well…\nWe definitely got our result…\nSo the null must be the part that’s wrong!\nThat’s when we reject the null - we find that the sampling distribution under the null hardly ever produces a result like ours, so that’s probably the wrong sampling distribution and thus the wrong null!"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#null-distribution-2",
    "href": "2024/weeks/week03/slides.html#null-distribution-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nHow does this work out with our estimate of 2.22?\nLet’s stick it on the graph"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#hypothesis-test",
    "href": "2024/weeks/week03/slides.html#hypothesis-test",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\nOur test comes down to: how weird would it be to get a result this far from the “truth” or farther?\nWe can figure this out by shading in the parts of the null distribution this far from the null truth or farther\nSo we shade 2.22 and above, and also 2 - abs(2.22 - 2) = 1.78 and below."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#hypothesis-test-1",
    "href": "2024/weeks/week03/slides.html#hypothesis-test-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\nBased off of this sampling distribution, there’s a 31% + 26% = 57% chance of getting something as weird as we got or weirder (or for a one-tailed test, a 26% chance of getting something that high or higher) if the null of \\(\\beta_1 = 2\\) is true\nThat’s not too unlikely! So, we would fail to reject the null of \\(\\beta_1 = 2\\)\nThis doesn’t mean that we conclude that \\(\\beta_1 = 2\\) is true, it just means we can’t rule it out"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#the-null-distribution",
    "href": "2024/weeks/week03/slides.html#the-null-distribution",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\nOf course, we generated this null distribution by just randomly creating a few random samples\nWe also happen to know that if we had infinite samples, the sampling distribution of OLS would be a normal distribution with the mean at the true value and the standard deviation determined by \\(var(X)\\), the variance of the residual, and the sample size. The real null distribution looks like this:"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#the-null-distribution-1",
    "href": "2024/weeks/week03/slides.html#the-null-distribution-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\nSo with the estimate we made from the sample we got (2.22), we can’t reject a null \\(\\beta_1 = 2\\)\nWhich is good!! That’s the truth. We don’t want to reject it!\nHow about other nulls? Can we reject those?\nCan we reject a null that \\(\\beta_1 = 0\\)?\n(by default, most null hypotheses are that the parameter is 0)\nLet’s follow the same steps!"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#the-null-distribution-2",
    "href": "2024/weeks/week03/slides.html#the-null-distribution-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\nNow that’s unlikely. We can reject that the true value is 0."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#p-values-1",
    "href": "2024/weeks/week03/slides.html#p-values-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "p-values",
    "text": "p-values\n\nThe lower the p-value, the less likely it is that we got our result AND the null is true\nAnd since we definitely got our result, a really low p-value says we should reject the null\nHow low does it need to be to reject the null?\nWell…"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#p-values-and-.05",
    "href": "2024/weeks/week03/slides.html#p-values-and-.05",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "p-values and .05",
    "text": "p-values and .05\n\nIt’s common practice to decide on a confidence level and a corresponding \\(\\alpha\\), most commonly a 95% confidence level \\(\\rightarrow \\alpha = .05\\), and reject the null if the p-value is lower than \\(\\alpha\\)\nWhy 95%? Completely arbitrary. Someone picked it out of thin air a hundred years ago as a just-for-instance and we still use it 🤷\nHaving a hard-and-fast threshold like this is not a great idea ( \\(p=.04\\) is rejection, but \\(p=.06\\) is not?), but it’s a very hard habit to break\nKey takeaway: get familiar with the concept of rejecting the null when the p-value is lower than .05, because you’ll see it\nBUT don’t get too hung up on black-and-white rejection in general"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#power-1",
    "href": "2024/weeks/week03/slides.html#power-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Power",
    "text": "Power\n\nBecause the smaller we make our confidence level, the less likely we are to reject the null in general\nWhich means we’ll also fail to reject it if it’s actually false (a “false negative”)\nWe want a low false-positive rate, but also a low false-negative rate\nThe false negative rate is called “power.” If we will reject the null 80% of the time when it’s actually false, we have 80% power\nAs \\(\\alpha\\) shrinks, false-positive rates decline, but false-negative rates increase\nStrike a balance!\n\n(minor sidenote: “false positive” and “false negative” are sometimes referred to as “Type I Error” and “Type II Error” - these are not great terms because they are hard to remember! If you encounter them, just remember that in “The boy who cried wolf” the townspeople think there’s a wolf when there’s not, then think there’s not a wolf when there is, committing Type I and II error in that order)"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#what-we-just-did",
    "href": "2024/weeks/week03/slides.html#what-we-just-did",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "What we just did…",
    "text": "What we just did…\n\nThis is how we thought of it - we picked a null, figured out the null distribution (the sampling distribution of the estimator assuming the null was true), and checked if our estimate was unlikely enough that we could reject the null"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#instead",
    "href": "2024/weeks/week03/slides.html#instead",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Instead…",
    "text": "Instead…\n\nBy thinking about standard errors, we instead center the sampling distribution around our estimate. If the null is far away, we reject that null (result should be the same)!"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#confidence-intervals",
    "href": "2024/weeks/week03/slides.html#confidence-intervals",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nSo what we’re thinking now is not “is our estimate close to the null?” but rather “is that null close to our estimate?”\nWe can go one step further and ask “which nulls are close to our estimate?” and figure out which nulls we can think about rejecting from there\nA confidence interval shows the range of nulls that would not be rejected by our estimate\nEverything outside that range can be rejected"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#confidence-intervals-1",
    "href": "2024/weeks/week03/slides.html#confidence-intervals-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThis takes the form of\n\n\\[ \\hat{\\beta}_1 \\pm Z(s.e.) \\]\n\nWhere \\(Z\\) is some value from our distribution that gives us the \\(1-\\alpha\\) percentile (for a 95% confidence interval with a normal sampling distribution, \\(Z = 1.96\\))\nand \\(s.e.\\) is the standard error of \\(\\hat{\\beta}_1\\)"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#confidence-intervals-2",
    "href": "2024/weeks/week03/slides.html#confidence-intervals-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThinking back to our estimate:\n\n\n\n\n\n\nModel 1\n\n(Intercept)2.80 \n\n(0.21)\n\nX2.22 \n\n(0.37)\n\nN100    \n\n\n\n\n\nOur estimate of the coefficient is 2.22, and our estimate of the standard error is 0.37\nSo for a 95% confidence interval, assuming normality, we get 2.22 \\(\\pm\\) 1.96* 0.37, or [1.49,2.94]"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#confidence-intervals-3",
    "href": "2024/weeks/week03/slides.html#confidence-intervals-3",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nWe can see this graphically as well - we should only reject nulls in those 5% tails for a 95% confidence interval"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#concept-checks-2",
    "href": "2024/weeks/week03/slides.html#concept-checks-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhat feature of the sampling distribution of the \\(\\beta\\) does the standard error describe?\nWhy do we get the same reject/don’t reject result if we center the sampling distribution around the null as around our estimate?\nWe perform an estimate of \\(\\hat{\\beta}_1\\) and get a 95% confidence interval of [-1.3, 2.1]. Describe what this means in a sentence.\nIn the above confidence interval, can we reject the null of \\(\\beta_1 = 0\\)?\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#bias-and-the-error-term",
    "href": "2024/weeks/week05/slides.html#bias-and-the-error-term",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Bias and the Error Term",
    "text": "Bias and the Error Term\n\nAll of the nice stuff we’ve gotten so far makes some assumptions about our true model\n\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nIn particular, we’ve made some assumptions about the error term \\(\\varepsilon\\)\nSo what is that error term exactly, and what are we assuming about it?"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#the-error-term",
    "href": "2024/weeks/week05/slides.html#the-error-term",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "The Error Term",
    "text": "The Error Term\n\nThe error term contains everything that isn’t in our model\nIf \\(Y\\) were a pure function of \\(X\\), for example if \\(Y\\) was “height in feet” and \\(X\\) was “height in inches”, we wouldn’t have an error term, because a straight line fully describes the relationship perfectly with no variation\nBut in most cases, the line is a simplification - we’re leaving other stuff out! That’s in the error term"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#the-error-term-1",
    "href": "2024/weeks/week05/slides.html#the-error-term-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "The Error Term",
    "text": "The Error Term\n\nConsider this data generating process:\n\n\\[ ClassGrade = \\beta_0 + \\beta_1 StudyTime + \\varepsilon \\]\n\nSurely StudyTime isn’t the only thing that determines your ClassGrade\nEverything else is in the error term!\nProfessorLeniency, InterestInSubject, Intelligence, and so on and so on…"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#the-error-term-2",
    "href": "2024/weeks/week05/slides.html#the-error-term-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "The Error Term",
    "text": "The Error Term\n\nIsn’t that really bad? We’re leaving out a bunch of important stuff!\nIf you want to predict \\(Y\\) as accurately as possible then we’re probably going to do a bad job of it\nBut if our real interest is *figuring out the relationship between \\(X\\) and \\(Y\\), then it’s fine to leave stuff out, as long as whatever’s left in the error term obeys a few important assumptions"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#error-term-assumptions",
    "href": "2024/weeks/week05/slides.html#error-term-assumptions",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Error Term Assumptions",
    "text": "Error Term Assumptions\n\nThe most important assumption about the error term is that it is unrelated to \\(X\\)\nIf \\(X\\) and \\(\\varepsilon\\) are correlated, \\(\\hat{\\beta}_1\\) will be biased - its distribution no longer has the true \\(\\beta_1\\) as its mean\nIn these cases we can say ” \\(X\\) is endogenous” or “we have omitted variable bias”\nNo amount of additional sample size will fix that problem!\n(what will fix the problem? We’ll get to that one later)"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#omitted-variable-bias",
    "href": "2024/weeks/week05/slides.html#omitted-variable-bias",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\n\nWe can intuitively think about whether omitted variable bias is likely to make our estimates too high or too low\nThe sign of the bias will be the sign of the relationship between the omitted variable and \\(X\\), times the sign of the relationship between the omitted variable bias and \\(Y\\)\nInterestInSubject is positively related to both StudyTime and ClassGrade, and \\(+\\times+ = +\\), so our estimates are positively biased / too high"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#thinking-through-this-bias",
    "href": "2024/weeks/week05/slides.html#thinking-through-this-bias",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Thinking Through this Bias",
    "text": "Thinking Through this Bias\n\nThis is a good way to keep that “direction of bias” problem in mind\nAnd you do want to keep it in mind! This is important for understanding general correlations you see in the wild, too\nAnd helps keep in line some things - for example, if \\(Z\\) is unrelated to \\(X\\), it won’t bias you!!"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#thinking-through-this-bias-1",
    "href": "2024/weeks/week05/slides.html#thinking-through-this-bias-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Thinking Through this Bias",
    "text": "Thinking Through this Bias\n\nThat means that when you’re thinking about the controls you need, that only includes things related to \\(X\\)\nAdding controls for things related to \\(Y\\) not \\(X\\) can make the model predict better and reduce standard errors, but won’t remove omitted variable bias"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#less-serious-error-concerns",
    "href": "2024/weeks/week05/slides.html#less-serious-error-concerns",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Less Serious Error Concerns",
    "text": "Less Serious Error Concerns\n\nOmitted variable bias can, well, bias us, which is very bad\nThere are some other assumptions that can fail that may also pose a problem to us but less so\nWe’ve assumed so far not just that \\(\\varepsilon\\) is unrelated to \\(X\\), but also that the variance of \\(\\varepsilon\\) is unrelated to \\(X\\), and that the \\(\\varepsilon\\)s are unrelated to each other\nIf these assumptions fail, our standard errors will be wrong, but we won’t be biased, and also there are ways to fix the standard errors\nWe will cover these only briefly, they’ll come back later"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#heteroskedasticity",
    "href": "2024/weeks/week05/slides.html#heteroskedasticity",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nIf the variance of the error term is different for different values of \\(X\\), then we have “heteroskedasticity”\nNotice in the below graph how the spread of the points around the line (the variance of the error term) is bigger on the right than the left"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#heteroskedasticity-1",
    "href": "2024/weeks/week05/slides.html#heteroskedasticity-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nWe can correct for this using heteroskedasticity-robust standard errors which sort of “squash down” the big variances and then re-estimate the standard errors\nWe can do this in feols with vcov = 'hetero' in R or with “robust” in STATA"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#correlated-errors",
    "href": "2024/weeks/week05/slides.html#correlated-errors",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Correlated Errors",
    "text": "Correlated Errors\n\nIf the error terms are correlated with each other, then our standard errors will also be wrong\nHow could this happen? For example, maybe you’ve surveyed a bunch of people in different towns - the error terms within a town might be clustered\nAgain, this doesn’t bias \\(\\hat{\\beta}_1\\) but it can affect standard errors!"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#the-right-hand-side-1",
    "href": "2024/weeks/week05/slides.html#the-right-hand-side-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "The Right Hand Side",
    "text": "The Right Hand Side\nWe will look at three features of the right-hand side\n\nWhat if the variable is categorical or binary? (binary variables)\nWhat if the variable has a nonlinear effect on \\(Y\\) (polynomials and logarithms)\nWhat if the effect of one variable depends on the value of another variable? (interaction terms)"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#binary-data-1",
    "href": "2024/weeks/week05/slides.html#binary-data-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Binary Data",
    "text": "Binary Data\n\nA variable is binary if it only has two values - 0 or 1 (or “No” or “Yes”, etc.)\nBinary variables are super common in econometrics!\nDid you get the treatment? Yes / No\nDo you live in the US? Yes / No\nIs a floating exchange rate in effect? Yes / No"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#comparison-of-means",
    "href": "2024/weeks/week05/slides.html#comparison-of-means",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nWhen a binary variable is an independent variable, what we are often interested in doing is comparing means\nIs mean income higher inside the US or outside?\nIs mean height higher for kids who got a nutrition supplement or those who didn’t?\nIs mean GDP growth higher with or without a floating exchange rate?"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#comparison-of-means-1",
    "href": "2024/weeks/week05/slides.html#comparison-of-means-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nLet’s compare log earnings in 1993 between married people 30 or older vs. never-married people 30 or older\nSeems to be a slight favor to the married men\n\n\ndata(PSID, package = 'Ecdat')\nPSID &lt;- PSID %&gt;%\n  filter(age &gt;= 30, married %in% c('married','never married'), earnings &gt; 0) %&gt;%\n  mutate(married  = married == 'married')\nPSID %&gt;%\n  group_by(married) %&gt;%\n  summarize(log_earnings = mean(log(earnings)))\n\n# A tibble: 2 × 2\n  married log_earnings\n  &lt;lgl&gt;          &lt;dbl&gt;\n1 FALSE           9.26\n2 TRUE            9.47"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#comparison-of-means-2",
    "href": "2024/weeks/week05/slides.html#comparison-of-means-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#comparison-of-means-3",
    "href": "2024/weeks/week05/slides.html#comparison-of-means-3",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nThe difference between the means follows a t-distribution under the null that they’re identical\nSo of course we can do a hypothesis test of whether they’re different. But why bother trotting out a specific test when we can just do a regression?\n(In fact, a lot of specific tests can be replaced with basic regression, see this explainer)"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#comparison-of-means-4",
    "href": "2024/weeks/week05/slides.html#comparison-of-means-4",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nNotice:\n\nThe intercept gives the mean for the non-married group\nThe coefficient on marriedTRUE gives the married minus non-married difference\ni.e. the coefficient on a binary variable in a regression gives the difference in means"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#comparison-of-means-5",
    "href": "2024/weeks/week05/slides.html#comparison-of-means-5",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nWhy does OLS give us a comparison of means when you give it a binary variable?\n\nThe only \\(X\\) values are 0 (FALSE) and 1 (TRUE)\nBecause of this, OLS no longer really fits a line, it’s more of two separate means\nAnd when you’re estimating to minimize the sum of squared errors separately for each group, can’t do any better than to predict the mean!\nSo you get the mean of each group as each group’s prediction"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#binary-with-controls",
    "href": "2024/weeks/week05/slides.html#binary-with-controls",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Binary with Controls",
    "text": "Binary with Controls\n\nObviously this is handy for including binary controls, but why do this for binary treatments? Because we can add controls!\n\n\n\n                feols(log(earning..\nDependent Var.:       log(earnings)\n                                   \nConstant          8.740*** (0.1478)\nmarriedTRUE      0.3404*** (0.0579)\nkids            -0.2259*** (0.0159)\nage              0.0223*** (0.0038)\n_______________ ___________________\nS.E. type                       IID\nObservations                  2,803\nR2                          0.07609\nAdj. R2                     0.07510\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#multicollinearity",
    "href": "2024/weeks/week05/slides.html#multicollinearity",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nWhy is just one side of it on the regression? Why aren’t “married” and “not married” BOTH included?\nBecause regression couldn’t give an answer!"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#multicollinearity-1",
    "href": "2024/weeks/week05/slides.html#multicollinearity-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMean of married is \\(9.47\\) and of non-married is \\(9.26\\). \\[ \\log(Earnings) = 0 + 9.47Married + 9.26NonMarried \\] \\[ \\log(Earnings) = 3 + 6.47Married + 6.26NonMarried \\]\nThese all give the exact same predictions! OLS can’t pick between them. There’s no single best way to minimize squared residuals\nSo we pick one with convenient properties, setting one of the categories to have a coefficient of 0 (dropping it) and making the coefficient on the other the difference relative to the one we left out"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#more-than-two-categories-1",
    "href": "2024/weeks/week05/slides.html#more-than-two-categories-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nThat interpretation - dropping one and making the other relative to that, conveniently extends to multi-category variables\nWhy stop at binary categorical variables? There are plenty of categorical variables with more than two values\nWe can put these in a regression by turning each value into its own binary variable\n(and then dropping one so the coefficients on the others give you the difference with the omitted one)"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#concept-checks",
    "href": "2024/weeks/week05/slides.html#concept-checks",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nIf \\(X\\) is binary, in sentences interpret the coefficients from the estimated OLS equation \\(Y = 4 + 3X + 2Z\\)\nHow might a comparison of means come in handy if you wanted to analyze the results of a randomized policy experiment?\nIf you had a data set of people from every continent and added “continent” as a control, how many coefficients would this add to your model?\nIf in that regression you really wanted to compare Europe to Asia specifically, what might you do so that the regression made this easy?"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#interpreting-ols-1",
    "href": "2024/weeks/week05/slides.html#interpreting-ols-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interpreting OLS",
    "text": "Interpreting OLS\n\nTo think more about the right-hand-side, let’s go back to our original interpretation of an OLS coefficient \\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\nA one-unit change in \\(X\\) is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\nThis logic still works with binary variables since “a one-unit change in \\(X\\)” means “changing \\(X\\) from No to Yes”\nNotice that this assumes that a one-unit change in \\(X\\) always has the same effect on \\(\\beta_1\\) no matter what else is going on\nWhat if that’s not true?"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#functional-form",
    "href": "2024/weeks/week05/slides.html#functional-form",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Functional Form",
    "text": "Functional Form\n\nWe talked before about times when a linear model like standard OLS might not be sufficient\nHowever, as long as those non-linearities are on the right hand side, we can fix the problem easily but just having \\(X\\) enter non-linearly! Run it through a transformation!\nThe most common transformations by far are polynomials and logarithms"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#functional-form-1",
    "href": "2024/weeks/week05/slides.html#functional-form-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Functional Form",
    "text": "Functional Form\n\nWhy do this? Because sometimes a straight line is clearly not going to do the trick!"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#polynomials-1",
    "href": "2024/weeks/week05/slides.html#polynomials-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\n\\(\\beta_1X\\) is a “first order polynomial” - there’s one term\n\\(\\beta_1X + \\beta_2X^2\\) is a “second order polynomial” or a “quadratic” - two terms (note both included, it’s not just \\(X^2\\))\n\\(\\beta_1X + \\beta_2X^2 + \\beta_3X^3\\) is a third-order or cubic, etc."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#polynomials-2",
    "href": "2024/weeks/week05/slides.html#polynomials-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\nWhat do they do?\n\nThe more polynomial terms, the more flexible the line can be. With enough terms you can mimic any shape of relationship\nOf course, if you just add a whole buncha terms, it gets very noisy, and prediction out-of-sample gets very bad\nKeep it minimal - quadratics are almost always enough, unless you have reason to believe there’s a true more-complex relationship. You can try adding higher-order terms and see if they make a difference"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#polynomials-3",
    "href": "2024/weeks/week05/slides.html#polynomials-3",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\nThe true relationship is quadratic"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#polynomials-4",
    "href": "2024/weeks/week05/slides.html#polynomials-4",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\nHigher-order terms don’t do anything for us here (because a quadratic is sufficient!)"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#polynomials-5",
    "href": "2024/weeks/week05/slides.html#polynomials-5",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\nInterpret polynomials using the derivative\n\\(\\partial Y/\\partial X\\) will be different depending on the value of \\(X\\) (as it should! Notice in the graph that the slope changes for different values of \\(X\\))\n\n\\[ Y = \\beta_1X + \\beta_2X^2 \\] \\[ \\partial Y/\\partial X = \\beta_1 + 2\\beta_2X \\]\nSo at \\(X = 0\\), the effect of a one-unit change in \\(X\\) is \\(\\beta_1\\). At \\(X = 1\\), it’s \\(\\beta_1 + \\beta_2\\). At \\(X = 5\\) it’s \\(\\beta_1 + 5\\beta_2\\)."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#polynomials-6",
    "href": "2024/weeks/week05/slides.html#polynomials-6",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\nIMPORTANT: when you have a polynomial, the coefficients on each individual term mean very little on their own. You have to consider them alongisde the other coefficients from the polynomial! Never interpret \\(\\beta_1\\) here without thinking about \\(\\beta_2\\) alongside. Also, the significance of the individual terms doesn’t really matter - consider doing an F-test of all of them at once."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#concept-check",
    "href": "2024/weeks/week05/slides.html#concept-check",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat’s the effect of a one-unit change in \\(X\\) at \\(X = 0\\), \\(X = 1\\), and \\(X = 2\\) for each of these?\n\n\n\n                feols(Y ~ X, dat.. feols(Y ~ X + I(.. feols(Y ~ X + I(...1\nDependent Var.:                  Y                  Y                    Y\n                                                                          \nConstant         7.285*** (0.5660)   -0.1295 (0.3839)      0.0759 (0.5091)\nX               -8.934*** (0.1953)   0.9779* (0.3831)      0.4542 (0.9331)\nX square                           -2.003*** (0.0752)   -1.738*** (0.4368)\nX cube                                                    -0.0354 (0.0574)\n_______________ __________________ __________________   __________________\nS.E. type                      IID                IID                  IID\nObservations                   200                200                  200\nR2                         0.91357            0.98122              0.98126\nAdj. R2                    0.91313            0.98103              0.98097\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#logarithms-1",
    "href": "2024/weeks/week05/slides.html#logarithms-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms\n\nAnother common transformation, both for dependent and independent variables, is to take the logarithm\nThis has the effect of pulling in extreme values from strongly right-skewed data and making linear relationships pop out\nIncome, for example, is almost always used with a logarithm\nIt also gives the coefficients a nice percentage-based interpretation"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#logarithms-2",
    "href": "2024/weeks/week05/slides.html#logarithms-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#or-if-you-prefer",
    "href": "2024/weeks/week05/slides.html#or-if-you-prefer",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Or if you prefer…",
    "text": "Or if you prefer…\n\nNotice the change in axes"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#logarithms-3",
    "href": "2024/weeks/week05/slides.html#logarithms-3",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms\n\nHow can we interpret them?\nThe key is to remember that \\(\\log(X) + a \\approx \\log((1+a)X)\\), meaning that a \\(a\\)-unit change in \\(log(X)\\) is similar to a \\(a\\times100%\\) change in \\(X\\)\nSo, walk through our “one-unit change in the variable” logic from before, but whenever we hit a log, change that into a percentage!"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#logarithms-4",
    "href": "2024/weeks/week05/slides.html#logarithms-4",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms\n\n\\(Y = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1X\\) a one-unit change in \\(X\\) is associated with a \\(\\beta_1\\times 100\\)% change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(\\log(Y)\\), or a \\(\\beta_1\\times100\\)% change in \\(Y\\).\n(Try also with changes smaller than one unit - that’s usually more reasonable)"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#logarithms-5",
    "href": "2024/weeks/week05/slides.html#logarithms-5",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms\nDownsides:\n\nLogarithms require that all data be positive. No negatives or zeroes!\nFairly rare that a variable with negative values wants a log anyway\nBut zeroes are common! A common practice is to just do \\(log(X+1)\\) but this is pretty arbitrary"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#functional-form-2",
    "href": "2024/weeks/week05/slides.html#functional-form-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Functional Form",
    "text": "Functional Form\n\nIn general, you want the shape of your function to match the shape of the relationship in the data (or, even better, the true relationship)\nPolynomials and logs can usually get you there!\nWhich to use? Use logs for highly skewed data or variables with exponential relationships\nUse polynomials if it doesn’t look straight! Check that scatterplot and see how not-straight it is!"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#concept-checks-1",
    "href": "2024/weeks/week05/slides.html#concept-checks-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhich of the following variables would you likely want to log before using them? Income, height, wealth\nFor these, interpret the coefficient by filling in “A [blank] change in X is associated with a [blank] change in Y”:\n\n\\[ Y = 1 + 2\\log(X) \\] \\[ \\log(Y) = 3 + 2\\log(X) \\]\n\\[ \\log(Y) = 4 + 3X \\]"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#interactions",
    "href": "2024/weeks/week05/slides.html#interactions",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interactions",
    "text": "Interactions\n\nFor both polynomials and logarithms, the effect of a one-unit change in \\(X\\) differs depending on its current value (for logarithms, a 1-unit change in \\(X\\) is different percentage changes in \\(X\\) depending on current value)\nBut why stop there? Maybe the effect of \\(X\\) differs depending on the current value of other variables!\nEnter interaction terms!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z + \\varepsilon \\] - Interaction terms are a little tough but also extremely important."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#interactions-1",
    "href": "2024/weeks/week05/slides.html#interactions-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interactions",
    "text": "Interactions\nExpect to come back to these slides, as you’re almost certainly going to use interaction terms in both our assessment and the dissertation"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#interactions-2",
    "href": "2024/weeks/week05/slides.html#interactions-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interactions",
    "text": "Interactions\n\nChange in the value of a control can shift a regression line up and down\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z\\), estimated as \\(Y = .01 + 1.2X + .95Z\\):"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#interactions-3",
    "href": "2024/weeks/week05/slides.html#interactions-3",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interactions",
    "text": "Interactions\n\nBut an interaction can both shift the line up and down AND change its slope\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z\\), estimated as \\(Y = .035 + 1.14X + .94Z + 1.02X\\times Z\\):"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#interactions-4",
    "href": "2024/weeks/week05/slides.html#interactions-4",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interactions",
    "text": "Interactions\n\nHow can we interpret an interaction?\nThe idea is that the interaction shows how the effect of one variable changes as the value of the other changes\nThe derivative helps!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z \\] \\[ \\partial Y/\\partial X = \\beta_1 + \\beta_3 Z \\]\n\nThe effect of \\(X\\) is \\(\\beta_1\\) when \\(Z = 0\\), or \\(\\beta_1 + \\beta_3\\) when \\(Z = 1\\), or \\(\\beta_1 + 3\\beta_3\\) if \\(Z = 3\\)!"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#interactions-5",
    "href": "2024/weeks/week05/slides.html#interactions-5",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interactions",
    "text": "Interactions\n\nOften we are doing interactions with binary variables to see how an effect differs across groups\nNow, instead of the intercept giving the baseline and the binary coefficient giving the difference, the coefficient on \\(X\\) is the baseline effect of \\(X\\) and the interaction is the difference in the effect of \\(X\\)\nThe interaction coefficient becomes “the difference in the effect of \\(X\\) between the \\(Z\\) =”No” and \\(Z\\) = “Yes” groups”\n(What if it’s continuous? Mathematically the same but the thinking changes - the interaction term is the difference in the effect of \\(X\\) you get when increasing \\(Z\\) by one unit)"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#interactions-6",
    "href": "2024/weeks/week05/slides.html#interactions-6",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interactions",
    "text": "Interactions\n\nMarriage for those without a college degree raises earnings by 24%. A college degree reduces the marriage premium by 25%. Marriage for those with a college degree reduces earnings by .24 - .25 = -1%\n\n\n\n                          feols(log(earnin..\nDependent Var.:                log(earnings)\n                                            \nConstant                   9.087*** (0.0583)\nmarriedTRUE               0.2381*** (0.0638)\ncollegeTRUE               0.8543*** (0.1255)\nmarriedTRUE x collegeTRUE  -0.2541. (0.1363)\n_________________________ __________________\nS.E. type                                IID\nObservations                           2,803\nR2                                   0.06253\nAdj. R2                              0.06153\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#notes-on-interactions",
    "href": "2024/weeks/week05/slides.html#notes-on-interactions",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nLike with polynomials, the coefficients on their own now have little meaning and must be evaluated alongside each other. \\(\\beta_1\\) by itself is just “the effect of \\(X\\) when \\(Z = 0\\)”, not “the effect of \\(X\\)”\nYes, you do almost always want to include both variables in un-interacted form and interacted form. Otherwise the interpretation gets very thorny"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#notes-on-interactions-1",
    "href": "2024/weeks/week05/slides.html#notes-on-interactions-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nInteraction effects are poorly powered. You need a lot of data to be able to tell whether an effect is different in two groups. If \\(N\\) observations is adequate power to see if the effect itself is different from zero, you need a sample of roughly \\(16\\times N\\) to see if the difference in effects is nonzero. Sixteen times!!\nIt’s tempting to try interacting your effect with everything to see if it’s bigger/smaller/nonzero in some groups, but because it’s poorly powered, this is a bad idea! You’ll get a lot of false positives\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#bias-and-the-error-term",
    "href": "2024/weeks/week07/slides.html#bias-and-the-error-term",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Bias and the Error Term",
    "text": "Bias and the Error Term\n\nAll of the nice stuff we’ve gotten so far makes some assumptions about our true model\n\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nIn particular, we’ve made some assumptions about the error term \\(\\varepsilon\\)\nSo what is that error term exactly, and what are we assuming about it?"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#the-error-term",
    "href": "2024/weeks/week07/slides.html#the-error-term",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "The Error Term",
    "text": "The Error Term\n\nThe error term contains everything that isn’t in our model\nIf \\(Y\\) were a pure function of \\(X\\), for example if \\(Y\\) was “height in feet” and \\(X\\) was “height in inches”, we wouldn’t have an error term, because a straight line fully describes the relationship perfectly with no variation\nBut in most cases, the line is a simplification - we’re leaving other stuff out! That’s in the error term"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#the-error-term-1",
    "href": "2024/weeks/week07/slides.html#the-error-term-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "The Error Term",
    "text": "The Error Term\n\nConsider this data generating process:\n\n\\[ ClassGrade = \\beta_0 + \\beta_1 StudyTime + \\varepsilon \\]\n\nSurely StudyTime isn’t the only thing that determines your ClassGrade\nEverything else is in the error term!\nProfessorLeniency, InterestInSubject, Intelligence, and so on and so on…"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#the-error-term-2",
    "href": "2024/weeks/week07/slides.html#the-error-term-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "The Error Term",
    "text": "The Error Term\n\nIsn’t that really bad? We’re leaving out a bunch of important stuff!\nIf you want to predict \\(Y\\) as accurately as possible then we’re probably going to do a bad job of it\nBut if our real interest is *figuring out the relationship between \\(X\\) and \\(Y\\), then it’s fine to leave stuff out, as long as whatever’s left in the error term obeys a few important assumptions"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#error-term-assumptions",
    "href": "2024/weeks/week07/slides.html#error-term-assumptions",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Error Term Assumptions",
    "text": "Error Term Assumptions\n\nThe most important assumption about the error term is that it is unrelated to \\(X\\)\nIf \\(X\\) and \\(\\varepsilon\\) are correlated, \\(\\hat{\\beta}_1\\) will be biased - its distribution no longer has the true \\(\\beta_1\\) as its mean\nIn these cases we can say ” \\(X\\) is endogenous” or “we have omitted variable bias”\nNo amount of additional sample size will fix that problem!\n(what will fix the problem? We’ll get to that one later)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#omitted-variable-bias",
    "href": "2024/weeks/week07/slides.html#omitted-variable-bias",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\n\nWe can intuitively think about whether omitted variable bias is likely to make our estimates too high or too low\nThe sign of the bias will be the sign of the relationship between the omitted variable and \\(X\\), times the sign of the relationship between the omitted variable bias and \\(Y\\)\nInterestInSubject is positively related to both StudyTime and ClassGrade, and \\(+\\times+ = +\\), so our estimates are positively biased / too high\nMore precisely we have that the mean of the \\(\\hat{\\beta}_1\\) sampling distribution is \\[\\beta_1 + corr(X,\\varepsilon)\\frac{\\sigma_\\varepsilon}{\\sigma_X}\\]"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#thinking-through-this-bias",
    "href": "2024/weeks/week07/slides.html#thinking-through-this-bias",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Thinking Through this Bias",
    "text": "Thinking Through this Bias\n\nIf \\(Z\\) hangs around \\(X\\), but \\(Y\\) doesn’t know about it, then the coefficient on \\(X\\) will get all the credit for \\(Z\\)\nThis is a good way to keep that “direction of bias” problem in mind\nAnd you do want to keep it in mind! This is important for understanding general correlations you see in the wild, too\nAnd helps keep in line some things - for example, if \\(Z\\) is unrelated to \\(X\\), it won’t bias you!!"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#thinking-through-this-bias-1",
    "href": "2024/weeks/week07/slides.html#thinking-through-this-bias-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Thinking Through this Bias",
    "text": "Thinking Through this Bias\n\nThat means that when you’re thinking about the controls you need, that only includes things related to \\(X\\)\nAdding controls for things related to \\(Y\\) not \\(X\\) can make the model predict better and reduce standard errors, but won’t remove omitted variable bias"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#less-serious-error-concerns",
    "href": "2024/weeks/week07/slides.html#less-serious-error-concerns",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Less Serious Error Concerns",
    "text": "Less Serious Error Concerns\n\nOmitted variable bias can, well, bias us, which is very bad\nThere are some other assumptions that can fail that may also pose a problem to us but less so\nWe’ve assumed so far not just that \\(\\varepsilon\\) is unrelated to \\(X\\), but also that the variance of \\(\\varepsilon\\) is unrelated to \\(X\\), and that the \\(\\varepsilon\\)s are unrelated to each other\nIf these assumptions fail, our standard errors will be wrong, but we won’t be biased, and also there are ways to fix the standard errors\nWe will cover these only briefly, they’ll come back later"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#heteroskedasticity",
    "href": "2024/weeks/week07/slides.html#heteroskedasticity",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nIf the variance of the error term is different for different values of \\(X\\), then we have “heteroskedasticity”\nNotice in the below graph how the spread of the points around the line (the variance of the error term) is bigger on the right than the left"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#heteroskedasticity-1",
    "href": "2024/weeks/week07/slides.html#heteroskedasticity-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nWe can correct for this using heteroskedasticity-robust standard errors which sort of “squash down” the big variances and then re-estimate the standard errors\nWe can do this in feols with vcov = 'hetero' in R or with “robust” in STATA"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#correlated-errors",
    "href": "2024/weeks/week07/slides.html#correlated-errors",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Correlated Errors",
    "text": "Correlated Errors\n\nIf the error terms are correlated with each other, then our standard errors will also be wrong\nHow could this happen? For example, maybe you’ve surveyed a bunch of people in different towns - the error terms within a town might be clustered\nOr maybe you have time series data. If a term in the error term is “sticky” or has “momentum” it will likely be a similar error term a few time periods in a row, beign correlated across time, or autocorrelated\nAgain, this doesn’t bias \\(\\hat{\\beta}_1\\) but it can affect standard errors!"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#the-right-hand-side-1",
    "href": "2024/weeks/week07/slides.html#the-right-hand-side-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "The Right Hand Side",
    "text": "The Right Hand Side\nWe will look at three features of the right-hand side\n\nWhat if the variable is categorical or binary? (binary variables)\nWhat if the variable has a nonlinear effect on \\(Y\\) (polynomials and logarithms)\nWhat if the effect of one variable depends on the value of another variable? (interaction terms)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#binary-data-1",
    "href": "2024/weeks/week07/slides.html#binary-data-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Binary Data",
    "text": "Binary Data\n\nA variable is binary if it only has two values - 0 or 1 (or “No” or “Yes”, etc.)\nBinary variables are super common in econometrics!\nDid you get the treatment? Yes / No\nDo you live in the US? Yes / No\nIs a floating exchange rate in effect? Yes / No"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#comparison-of-means",
    "href": "2024/weeks/week07/slides.html#comparison-of-means",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nWhen a binary variable is an independent variable, what we are often interested in doing is comparing means\nIs mean income higher inside the US or outside?\nIs mean height higher for kids who got a nutrition supplement or those who didn’t?\nIs mean GDP growth higher with or without a floating exchange rate?"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#comparison-of-means-1",
    "href": "2024/weeks/week07/slides.html#comparison-of-means-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nLet’s compare log earnings in 1993 between married people 30 or older vs. never-married people 30 or older\nSeems to be a slight favor to the married men\n\n\ndata(PSID, package = 'Ecdat')\nPSID &lt;- PSID %&gt;%\n  filter(age &gt;= 30, married %in% c('married','never married'), earnings &gt; 0) %&gt;%\n  mutate(married  = married == 'married')\nPSID %&gt;%\n  group_by(married) %&gt;%\n  summarize(log_earnings = mean(log(earnings)))\n\n# A tibble: 2 × 2\n  married log_earnings\n  &lt;lgl&gt;          &lt;dbl&gt;\n1 FALSE           9.26\n2 TRUE            9.47"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#comparison-of-means-2",
    "href": "2024/weeks/week07/slides.html#comparison-of-means-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#comparison-of-means-3",
    "href": "2024/weeks/week07/slides.html#comparison-of-means-3",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nThe difference between the means follows a t-distribution under the null that they’re identical\nSo of course we can do a hypothesis test of whether they’re different. But why bother trotting out a specific test when we can just do a regression?\n(In fact, a lot of specific tests can be replaced with basic regression, see this explainer)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#comparison-of-means-4",
    "href": "2024/weeks/week07/slides.html#comparison-of-means-4",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nNotice:\n\nThe intercept gives the mean for the non-married group\nThe coefficient on marriedTRUE gives the married minus non-married difference\ni.e. the coefficient on a binary variable in a regression gives the difference in means\nIf we’d defined it the other way, with “not married” as the independent variable, the intercept would be the mean for the married group (i.e. “not married = 0”), and the coefficient would be the exact same but times \\(-1\\) (same difference, just opposite direction!)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#comparison-of-means-5",
    "href": "2024/weeks/week07/slides.html#comparison-of-means-5",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nWhy does OLS give us a comparison of means when you give it a binary variable?\n\nThe only \\(X\\) values are 0 (FALSE) and 1 (TRUE)\nBecause of this, OLS no longer really fits a line, it’s more of two separate means\nAnd when you’re estimating to minimize the sum of squared errors separately for each group, can’t do any better than to predict the mean!\nSo you get the mean of each group as each group’s prediction"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#binary-with-controls",
    "href": "2024/weeks/week07/slides.html#binary-with-controls",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Binary with Controls",
    "text": "Binary with Controls\n\nObviously this is handy for including binary controls, but why do this for binary treatments? Because we can add controls!\n\n\n\n                feols(log(earning..\nDependent Var.:       log(earnings)\n                                   \nConstant          8.740*** (0.1478)\nmarriedTRUE      0.3404*** (0.0579)\nkids            -0.2259*** (0.0159)\nage              0.0223*** (0.0038)\n_______________ ___________________\nS.E. type                       IID\nObservations                  2,803\nR2                          0.07609\nAdj. R2                     0.07510\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#multicollinearity",
    "href": "2024/weeks/week07/slides.html#multicollinearity",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nWhy is just one side of it on the regression? Why aren’t “married” and “not married” BOTH included?\nBecause regression couldn’t give an answer!"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#multicollinearity-1",
    "href": "2024/weeks/week07/slides.html#multicollinearity-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMean of married is \\(9.47\\) and of non-married is \\(9.26\\). \\[ \\log(Earnings) = 0 + 9.47Married + 9.26NonMarried \\] \\[ \\log(Earnings) = 3 + 6.47Married + 6.26NonMarried \\]\nThese (and infinitely many other options) all give the exact same predictions! OLS can’t pick between them. There’s no single best way to minimize squared residuals\nSo we pick one with convenient properties, setting one of the categories to have a coefficient of 0 (dropping it) and making the coefficient on the other the difference relative to the one we left out"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#more-than-two-categories-1",
    "href": "2024/weeks/week07/slides.html#more-than-two-categories-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nThat interpretation - dropping one and making the other relative to that, conveniently extends to multi-category variables\nWhy stop at binary categorical variables? There are plenty of categorical variables with more than two values\nWhat is your education level? What is your religious denomination? What continent are you on?\nWe can put these in a regression by turning each value into its own binary variable\n(and then dropping one so the coefficients on the others give you the difference with the omitted one)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#concept-checks",
    "href": "2024/weeks/week07/slides.html#concept-checks",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nIf \\(X\\) is binary, in sentences interpret the coefficients from the estimated OLS equation \\(Y = 4 + 3X + 2Z\\)\nHow might a comparison of means come in handy if you wanted to analyze the results of a randomized policy experiment?\nIf you had a data set of people from every continent and added “continent” as a control, how many coefficients would this add to your model?\nIf in that regression you really wanted to compare Europe to Asia specifically, what might you do so that the regression made this easy?"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#interpreting-ols-1",
    "href": "2024/weeks/week07/slides.html#interpreting-ols-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interpreting OLS",
    "text": "Interpreting OLS\n\nTo think more about the right-hand-side, let’s go back to our original interpretation of an OLS coefficient \\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\nA one-unit change in \\(X\\) is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\nThis logic still works with binary variables since “a one-unit change in \\(X\\)” means “changing \\(X\\) from No to Yes”\nNotice that this assumes that a one-unit change in \\(X\\) always has the same effect on \\(\\beta_1\\) no matter what else is going on\nWhat if that’s not true?"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#functional-form",
    "href": "2024/weeks/week07/slides.html#functional-form",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Functional Form",
    "text": "Functional Form\n\nWe talked before about times when a linear model like standard OLS might not be sufficient\nHowever, as long as those non-linearities are on the right hand side, we can fix the problem easily but just having \\(X\\) enter non-linearly! Run it through a transformation!\nThe most common transformations by far are polynomials and logarithms"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#functional-form-1",
    "href": "2024/weeks/week07/slides.html#functional-form-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Functional Form",
    "text": "Functional Form\n\nWhy do this? Because sometimes a straight line is clearly not going to do the trick!"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#polynomials-1",
    "href": "2024/weeks/week07/slides.html#polynomials-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\n\\(\\beta_1X\\) is a “first order polynomial” - there’s one term\n\\(\\beta_1X + \\beta_2X^2\\) is a “second order polynomial” or a “quadratic” - two terms (note both included, it’s not just \\(X^2\\))\n\\(\\beta_1X + \\beta_2X^2 + \\beta_3X^3\\) is a third-order or cubic, etc."
  },
  {
    "objectID": "2024/weeks/week07/slides.html#polynomials-2",
    "href": "2024/weeks/week07/slides.html#polynomials-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\nWhat do they do?\n\nThe more polynomial terms, the more flexible the line can be. With enough terms you can mimic any shape of relationship\nOf course, if you just add a whole buncha terms, it gets very noisy, and prediction out-of-sample gets very bad\nKeep it minimal - quadratics are almost always enough, unless you have reason to believe there’s a true more-complex relationship. You can try adding higher-order terms and see if they make a difference"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#polynomials-3",
    "href": "2024/weeks/week07/slides.html#polynomials-3",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\nThe true relationship is quadratic"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#polynomials-4",
    "href": "2024/weeks/week07/slides.html#polynomials-4",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\nHigher-order terms don’t do anything for us here (because a quadratic is sufficient!)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#polynomials-5",
    "href": "2024/weeks/week07/slides.html#polynomials-5",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\nInterpret polynomials using the derivative\n\\(\\partial Y/\\partial X\\) will be different depending on the value of \\(X\\) (as it should! Notice in the graph that the slope changes for different values of \\(X\\))\n\n\\[ Y = \\beta_1X + \\beta_2X^2 \\] \\[ \\partial Y/\\partial X = \\beta_1 + 2\\beta_2X \\]\nSo at \\(X = 0\\), the effect of a one-unit change in \\(X\\) is \\(\\beta_1\\). At \\(X = 1\\), it’s \\(\\beta_1 + \\beta_2\\). At \\(X = 5\\) it’s \\(\\beta_1 + 5\\beta_2\\)."
  },
  {
    "objectID": "2024/weeks/week07/slides.html#polynomials-6",
    "href": "2024/weeks/week07/slides.html#polynomials-6",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Polynomials",
    "text": "Polynomials\n\nIMPORTANT: when you have a polynomial, the coefficients on each individual term mean very little on their own. You have to consider them alongisde the other coefficients from the polynomial! Never interpret \\(\\beta_1\\) here without thinking about \\(\\beta_2\\) alongside. Also, the significance of the individual terms doesn’t really matter - consider doing an F-test of all of them at once."
  },
  {
    "objectID": "2024/weeks/week07/slides.html#concept-check",
    "href": "2024/weeks/week07/slides.html#concept-check",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat’s the effect of a one-unit change in \\(X\\) at \\(X = 0\\), \\(X = 1\\), and \\(X = 2\\) for each of these?\n\n\n\n                feols(Y ~ X, dat.. feols(Y ~ X + I(.. feols(Y ~ X + I(...1\nDependent Var.:                  Y                  Y                    Y\n                                                                          \nConstant         7.285*** (0.5660)   -0.1295 (0.3839)      0.0759 (0.5091)\nX               -8.934*** (0.1953)   0.9779* (0.3831)      0.4542 (0.9331)\nX square                           -2.003*** (0.0752)   -1.738*** (0.4368)\nX cube                                                    -0.0354 (0.0574)\n_______________ __________________ __________________   __________________\nS.E. type                      IID                IID                  IID\nObservations                   200                200                  200\nR2                         0.91357            0.98122              0.98126\nAdj. R2                    0.91313            0.98103              0.98097\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#logarithms-1",
    "href": "2024/weeks/week07/slides.html#logarithms-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms\n\nAnother common transformation, both for dependent and independent variables, is to take the logarithm\nThis has the effect of pulling in extreme values from strongly right-skewed data and making linear relationships pop out\nIncome, for example, is almost always used with a logarithm\nIt also gives the coefficients a nice percentage-based interpretation"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#logarithms-2",
    "href": "2024/weeks/week07/slides.html#logarithms-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#or-if-you-prefer",
    "href": "2024/weeks/week07/slides.html#or-if-you-prefer",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Or if you prefer…",
    "text": "Or if you prefer…\n\nNotice the change in axes"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#logarithms-3",
    "href": "2024/weeks/week07/slides.html#logarithms-3",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms\n\nHow can we interpret them?\nThe key is to remember that \\(\\log(X) + a \\approx \\log((1+a)X)\\), meaning that a \\(a\\)-unit change in \\(log(X)\\) is similar to a \\(a\\times100%\\) change in \\(X\\)\nSo, walk through our “one-unit change in the variable” logic from before, but whenever we hit a log, change that into a percentage!"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#logarithms-4",
    "href": "2024/weeks/week07/slides.html#logarithms-4",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms\n\n\\(Y = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1X\\) a one-unit change in \\(X\\) is associated with a \\(\\beta_1\\times 100\\)% change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(\\log(Y)\\), or a \\(\\beta_1\\times100\\)% change in \\(Y\\).\n(Try also with changes smaller than one unit - that’s usually more reasonable)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#logarithms-5",
    "href": "2024/weeks/week07/slides.html#logarithms-5",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Logarithms",
    "text": "Logarithms\nDownsides:\n\nLogarithms require that all data be positive. No negatives or zeroes!\nFairly rare that a variable with negative values wants a log anyway\nBut zeroes are common! A common practice is to just do \\(log(X+1)\\) but this is pretty arbitrary"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#functional-form-2",
    "href": "2024/weeks/week07/slides.html#functional-form-2",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Functional Form",
    "text": "Functional Form\n\nIn general, you want the shape of your function to match the shape of the relationship in the data (or, even better, the true relationship)\nPolynomials and logs can usually get you there!\nWhich to use? Use logs for highly skewed data or variables with exponential relationships\nUse polynomials if it doesn’t look straight! Check that scatterplot and see how not-straight it is!"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#concept-checks-1",
    "href": "2024/weeks/week07/slides.html#concept-checks-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhich of the following variables would you likely want to log before using them? Income, height, wealth, company size, home square footage\nIn each of the following estimated OLS lines, interpret the coefficient by filling in “A [blank] change in X is associated with a [blank] change in Y”:\n\n\\[ Y = 1 + 2\\log(X) \\] \\[ \\log(Y) = 3 + 2\\log(X) \\]\n\\[ \\log(Y) = 4 + 3X \\]"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#interactions",
    "href": "2024/weeks/week07/slides.html#interactions",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Interactions",
    "text": "Interactions\n\nFor both polynomials and logarithms, the effect of a one-unit change in \\(X\\) differs depending on its current value (for logarithms, a 1-unit change in \\(X\\) is different percentage changes in \\(X\\) depending on current value)\nBut why stop there? Maybe the effect of \\(X\\) differs depending on the current value of other variables! - Enter interaction terms!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z + \\varepsilon \\]\n\nInteraction terms are a little tough but also extremely important."
  },
  {
    "objectID": "2024/weeks/week07/slides.html#interactions-1",
    "href": "2024/weeks/week07/slides.html#interactions-1",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Interactions",
    "text": "Interactions\nExpect to come back to these slides, as you’re almost certainly going to use interaction terms in both our assessment and the dissertation"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#interactions-2",
    "href": "2024/weeks/week07/slides.html#interactions-2",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Interactions",
    "text": "Interactions\n\nChange in the value of a control can shift a regression line up and down\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z\\), estimated as \\(Y = .01 + 1.2X + .95Z\\):"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#interactions-3",
    "href": "2024/weeks/week07/slides.html#interactions-3",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Interactions",
    "text": "Interactions\n\nBut an interaction can both shift the line up and down AND change its slope\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z\\), estimated as \\(Y = .035 + 1.14X + .94Z + 1.02X\\times Z\\):"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#interactions-4",
    "href": "2024/weeks/week07/slides.html#interactions-4",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Interactions",
    "text": "Interactions\n\nHow can we interpret an interaction?\nThe idea is that the interaction shows how the effect of one variable changes as the value of the other changes\nThe derivative helps!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z \\] \\[ \\partial Y/\\partial X = \\beta_1 + \\beta_3 Z \\]\n\nThe effect of \\(X\\) is \\(\\beta_1\\) when \\(Z = 0\\), or \\(\\beta_1 + \\beta_3\\) when \\(Z = 1\\), or \\(\\beta_1 + 3\\beta_3\\) if \\(Z = 3\\)!"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#interactions-5",
    "href": "2024/weeks/week07/slides.html#interactions-5",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Interactions",
    "text": "Interactions\n\nOften we are doing interactions with binary variables to see how an effect differs across groups\nNow, instead of the intercept giving the baseline and the binary coefficient giving the difference, the coefficient on \\(X\\) is the baseline effect of \\(X\\) and the interaction is the difference in the effect of \\(X\\)\nThe interaction coefficient becomes “the difference in the effect of \\(X\\) between the \\(Z\\) =”No” and \\(Z\\) = “Yes” groups”\n(What if it’s continuous? Mathematically the same but the thinking changes - the interaction term is the difference in the effect of \\(X\\) you get when increasing \\(Z\\) by one unit)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#interactions-6",
    "href": "2024/weeks/week07/slides.html#interactions-6",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Interactions",
    "text": "Interactions\n\nMarriage for those without a college degree raises earnings by 24%. A college degree reduces the marriage premium by 25%. Marriage for those with a college degree reduces earnings by .24 - .25 = -1%\n\n\n\n                          feols(log(earnin..\nDependent Var.:                log(earnings)\n                                            \nConstant                   9.087*** (0.0583)\nmarriedTRUE               0.2381*** (0.0638)\ncollegeTRUE               0.8543*** (0.1255)\nmarriedTRUE x collegeTRUE  -0.2541. (0.1363)\n_________________________ __________________\nS.E. type                                IID\nObservations                           2,803\nR2                                   0.06253\nAdj. R2                              0.06153\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#notes-on-interactions",
    "href": "2024/weeks/week07/slides.html#notes-on-interactions",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nLike with polynomials, the coefficients on their own now have little meaning and must be evaluated alongside each other. \\(\\beta_1\\) by itself is just “the effect of \\(X\\) when \\(Z = 0\\)”, not “the effect of \\(X\\)”\nYes, you do almost always want to include both variables in un-interacted form and interacted form. Otherwise the interpretation gets very thorny"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#notes-on-interactions-1",
    "href": "2024/weeks/week07/slides.html#notes-on-interactions-1",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nInteraction effects are poorly powered. You need a lot of data to be able to tell whether an effect is different in two groups. If \\(N\\) observations is adequate power to see if the effect itself is different from zero, you need a sample of roughly \\(16\\times N\\) to see if the difference in effects is nonzero. Sixteen times!!\nIt’s tempting to try interacting your effect with everything to see if it’s bigger/smaller/nonzero in some groups, but because it’s poorly powered, this is a bad idea! You’ll get a lot of false positives"
  },
  {
    "objectID": "2024/weeks/week04/slides.html",
    "href": "2024/weeks/week04/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Linear Regression - Endogeneity\n\nLast week was all about handling sampling variation and avoiding inference error\nThis week we’re all about endogeneity!\nWhere it pops up and what we can do about it\nAt least as a starter (we’ll revisit this topic many times)\n\n\n\n\nRecap\n\nWe believe that our true model looks like this:\n\n\\[Y = \\beta_0 + \\beta_1X+\\varepsilon\\]\n\nWhere \\(\\varepsilon\\) is everything that determines \\(Y\\) other than \\(X\\)\nIf \\(X\\) is related to some of those things, we have endogeneity\nEstimating the above model by OLS, it will mistake the effect of those other things for the effect of \\(X\\), and our estimate of \\(\\hat{\\beta}_1\\) won’t represent the true \\(\\beta_1\\) no matter how many observations we have\n\n\n\n\nEndogeneity Recap\n\nFor example looking at income and corruption, the model\n\n\\[Income = \\beta_0 + \\beta_1Corruption + \\varepsilon\\]\n\nThe true \\(\\beta_1\\) is probably \\(0\\). But since \\(Political Stability\\) is in \\(\\varepsilon\\) and \\(olitical Stability\\) is related to \\(Corruption\\), OLS will mistakenly assign the effect of \\(olitical Stability\\) to the effect of \\(Corruption\\), making it look like there’s a positive effect when there isn’t one\nIf \\(olitical Stability\\) hangs around \\(Corruption\\), but OLS doesn’t know about it, OLS will give \\(Corruption\\) all the credit for \\(olitical Stability\\)’s impact on \\(Income\\)\nHere we’re mistakenly finding a positive effect when the truth is \\(0\\), but it could be anything - negative effect when truth is \\(0\\), positive effect when the truth is a bigger/smaller positive effect, negative effect when truth is positive, etc. etc.\n\n\n\n\nTo the Rescue\n\nOne way we can solve this problem is through the use of control variables\nWhat if \\(Political Stability\\) weren’t in \\(\\varepsilon\\)? Then we’d be fine! OLS would know how to separate out its effect from the \\(Corruption\\) effect. How do we take it out? Just put it in the model directly!\n\n\\[Income = \\beta_0 + \\beta_1Corruption + \\beta_2PoliticalStability + \\varepsilon\\]\n\nNow we have a multivariate regression model. Our estimate \\(\\hat{\\beta}_1\\) will not be biased by \\(Political Stability\\) because we’ve controlled for it\n\n(probably more accurate to say “covariates” or “variables to adjust for” than “control variables” and “adjust for” rather than “control for” but hey what are you gonna do, “control” is standard)\n\n\n\nTo the Rescue\n\nSo the task of solving our endogeneity problems in estimating \\(\\beta_1\\) using \\(\\hat{\\beta}_1\\) comes down to us finding all the elements of \\(\\varepsilon\\) that are related to \\(X\\) and adding them to the model\nAs we add them, they leave \\(\\varepsilon\\) and hopefully we end up with a version of \\(\\varepsilon\\) that is no longer related to \\(X\\)\nIf \\(cov(X,\\varepsilon) = 0\\) then we have an unbiased estimate!\n(of course, we have no way of checking if that’s true - it’s based on what we think the data generating process looks like)\n\n\n\n\nHow?\n\nHow does this actually work?\nControlling for a variable works by removing variation in \\(X\\) and \\(Y\\) that is explained by the control variable\nSo our estimate of \\(\\hat{\\beta}_1\\) is based on just the variation in \\(X\\) and \\(Y\\) that is unrelated to the control variable\nAny accidentally-assigning-the-value-of-PoliticalStability-to-Corruption can’t happen because we’ve removed the effect of \\(Political Stability\\) on \\(Corruption\\) as well as the effect of \\(Political Stability\\) on \\(Income\\)\nWe’re asking at that point, holding \\(Political Stability\\) constant, i.e. comparing two different countries with the same \\(Polities\\) , how is \\(Corruption\\) related to \\(Income\\)?\n\n\n\n\nExample\nThe true effect is \\(\\beta_1 = 3\\). Notice \\(Z\\) is binary and is related to \\(X\\) and \\(Y\\) but isn’t in the model!\n\ntib &lt;- tibble(Z = 1*(rnorm(1000) &gt; 0)) %&gt;%\n  mutate(X = Z + rnorm(1000)) %&gt;%\n  mutate(Y = 2 + 3*X + 2*Z + rnorm(1000))\nfeols(Y~X, data = tib) %&gt;%\n  etable()\n\n                                .\nDependent Var.:                 Y\n                                 \nConstant        2.756*** (0.0461)\nX               3.421*** (0.0383)\n_______________ _________________\nS.E. type                     IID\nObservations                1,000\nR2                        0.88897\nAdj. R2                   0.88886\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nExample\nTo remove what part of \\(X\\) and \\(Y\\) is explained by \\(Z\\), we can get the mean of \\(X\\) and \\(Y\\) by values of \\(Z\\)\n\ntib &lt;- tib %&gt;%\n  group_by(Z) %&gt;% \n  mutate(Y_mean = mean(Y), X_mean = mean(X))\nhead(tib)\n\n# A tibble: 6 × 5\n# Groups:   Z [2]\n      Z       X      Y Y_mean  X_mean\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1     1  0.855   5.98    6.91  0.967 \n2     1 -1.59   -0.226   6.91  0.967 \n3     1  0.806   6.08    6.91  0.967 \n4     1 -0.0302  2.94    6.91  0.967 \n5     0 -0.490  -0.997   1.95 -0.0121\n6     0 -0.512   0.383   1.95 -0.0121\n\n\n\n\n\nExample\nNow, Y_mean and X_mean are the mean of Y and X for the values of Z, i.e. the part of Y and X explained by Z. So subtract those parts out to get residuals Y_res and X_res!\n\ntib &lt;- tib %&gt;%\n  mutate(Y_res = Y - Y_mean, X_res = X - X_mean)\nhead(tib)\n\n# A tibble: 6 × 7\n# Groups:   Z [2]\n      Z       X      Y Y_mean  X_mean  Y_res  X_res\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1  0.855   5.98    6.91  0.967  -0.928 -0.113\n2     1 -1.59   -0.226   6.91  0.967  -7.14  -2.55 \n3     1  0.806   6.08    6.91  0.967  -0.827 -0.161\n4     1 -0.0302  2.94    6.91  0.967  -3.98  -0.997\n5     0 -0.490  -0.997   1.95 -0.0121 -2.95  -0.478\n6     0 -0.512   0.383   1.95 -0.0121 -1.57  -0.499\n\n\n\n\n\nExample\nWhat do we get now?\n\nfeols(Y_res ~ X_res, data = tib) %&gt;%\n  etable()\n\n                                .\nDependent Var.:             Y_res\n                                 \nConstant        5.73e-17 (0.0319)\nX_res           3.030*** (0.0319)\n_______________ _________________\nS.E. type                     IID\nObservations                1,000\nR2                        0.90060\nAdj. R2                   0.90050\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nExample\nCompare this to actually including Z as a control:\n\nfeols(Y ~ X + Z, data = tib) %&gt;%\n  etable()\n\n                                .\nDependent Var.:                 Y\n                                 \nConstant        1.988*** (0.0441)\nX               3.030*** (0.0319)\nZ               1.993*** (0.0712)\n_______________ _________________\nS.E. type                     IID\nObservations                1,000\nR2                        0.93782\nAdj. R2                   0.93769\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nGraphically\n\n\n\n\n\n\n\n\nControlling\n\nWe achieve all this just by adding the variable to the OLS equation!\nWe can, of course, include more than one control, or controls that aren’t binary\nUse OLS to predict \\(X\\) using all the controls, then take the residual (the part not explained by the controls)\nUse OLS to predict \\(Y\\) using all the controls, then take the residual (the part not explained by the controls)\nNow do OLS of just the \\(Y\\) residuals on just the \\(X\\) residuals\n\n\n\n\nA Continuous Control\n\n\n\n\n\n\nWhat do we get?\n\nWe can remove some of the relationship between \\(X\\) and \\(\\varepsilon\\)\nPotentially all of it, making \\(\\hat{\\beta}_1\\) us an unbiased (i.e. correct on average, but sampling variation doesn’t go away!) estimate of \\(\\beta_1\\)\nMaybe we can also get some estimates of \\(\\beta_2\\), \\(\\beta_3\\)… but be careful, they’re subject to the same identification and endogeneity problems!\nOften in econometrics we focus on getting one parameter, \\(\\hat{\\beta}_1\\), exactly right and don’t focus on parameters we haven’t put much effort into identifying\n\n\n\n\nConcept Checks\n\nSelene is a huge bore at parties, but sometimes brings her girlfriend Donna who is super fun. If you regressed \\(PartyFunRating\\) on \\(SeleneWasThere\\) but not \\(DonnaWasThere\\), what would the coefficient on \\(SeleneWasThere\\) look like and why?\nDescribe the steps necessary to estimate the effect of \\(Exports\\) on \\(GrowthRate\\) while controlling for \\(AmountofConflict\\) (a continuous variable). There are three “explain/regress” steps and two “subtract” steps.\nIf we estimate the same \\(\\hat{\\beta}_1\\) with or without \\(Z\\) added as a control, does that mean we have no endogeneity problem? What does it mean exactly?\n\n\n\n\nHave We Solved It?\n\nIncluding controls for every part of (what used to be) \\(\\varepsilon\\) that is related to \\(X\\) clears up any endogeneity problem we had with \\(X\\)\nSo… when we add a control, does that do it? How do we know?\nInconveniently, the data alone will never tell us if we’ve solved endogeniety\nWe can’t just check \\(X\\) against the remaining \\(\\varepsilon\\) because we never see \\(\\varepsilon\\) - what we have left over after a regression is the real-world residual, not the true-model error\n\n\n\n\nCausal Diagrams\n\n“What do I have to control for to solve the endogeneity problem” is an important and difficult question!\nTo answer it we need to think about the data-generating process\nOne way to do that is to draw a causal diagram\nA causal diagram describes the variables responsible for generating data and how they cause each other\nOnce we have written down our diagram, we’ll know what we need to control for\n(hopefully we have data on everything we need to control for! Often we don’t)\n\n\n\n\nDrawing a Diagram\n\nEndogeneity is all about the alternate reasons why two variables might be related *other than the causal effect you want\nWe can represent all the reasons two variables are related with a diagram\nPut down on paper how you think the world works, and where you think the data came from! This is economic modeling but with less math\n\n\nList out all the variables relevant to the DGP (including the ones we can’t measure or put our finger on!)\nDraw arrows between them reflecting what causes what else\nList all the paths from \\(X\\) to \\(Y\\) - these paths are reasons why \\(X\\) and \\(Y\\) are related!\nControl for at least one variable on each path you want to close (isn’t the effect you want)\n\n\n\n\nDrawing a Diagram\n\n\n\n\n\n\n\n\nDrawing a Diagram\n\nWe observe that, in the data, \\(ShortsWearing\\) and \\(IceCreamEating\\) are related. Why?\nMaybe, we theorize, that wearing shorts causes you to eat ice cream ( \\(ShortsWearing \\rightarrow IceCreamEating\\) )\nHowever, there’s another explanation/path: \\(Temperature\\) causes both ( \\(ShortsWearing \\leftarrow Temperature \\rightarrow IceCreamEating\\) )\nWe need to control for temperature to close this path!\nOnce it’s closed, the only path left is \\(ShortsWearing \\rightarrow IceCreamEating\\), so if we do see a relationship still in the data, we know we’ve identified the causal effect\n\n\n\n\nDetailing Paths\n\nThe goal is to list all the paths that go from the cause of our choice to the outcome variable (no loops)\nThat way we know what we need to control for to close the paths!\nControl for any one variable on the path, and suddenly there’s no variation from that variable any more - the causal chain is broken and the path is closed!\nA path counts no matter which direction the arrows point on it (the arrow direction matters but we’ll get to that next time)\nIf the path isn’t part of what answers our research question, it’s a back door we want to be closed\n\n\n\n\nPreschool and Adult Earnings\nDoes going to preschool improve your earnings as an adult?\n\n\n\n\n\n\n\n\nPaths\n\n\\(Preschool \\rightarrow Earnings\\)\n\\(Preschool \\rightarrow Skills \\rightarrow Earnings\\)\n\\(Preschool \\leftarrow Location \\rightarrow Earnings\\)\n\\(Preschool \\leftarrow Background \\rightarrow Earnings\\)\n\\(Preschool \\leftarrow Background \\rightarrow Skills \\rightarrow Earnings\\)\n\\(Preschool \\rightarrow Skills \\leftarrow Background \\rightarrow Earnings\\)\n\n\n\n\nClosing Paths\n\nWe want the ways that \\(Preschool\\) causes \\(Earnings\\) - that’s the first two, \\(Preschool \\rightarrow Earnings\\) and \\(Preschool \\rightarrow Skills \\rightarrow Earnings\\)\nThe rest we want to close! They’re back doors\n\\(Location\\) is on #3, so if we control for \\(Location\\), 3 is closed\n\\(Background\\) is on the rest, so if we control for \\(Background\\), the rest are closed\nSo if we estimate the below OLS equation, \\(\\hat{\\beta}_1\\) will be unbiased!\n\n\\[Earnings = \\beta_0 + \\beta_1Preschool + \\beta_2Location + \\beta_3Background+\\varepsilon\\]\n\n\n\nAnd the Bad News…\n\nThis assumes that the model we drew was accurate. Did we leave any important variables or arrows out? Think hard!\nWhat other variables might belong on this graph? Would they be on a path that gives an alternate explanation?\nJust because we say that’s the model doesn’t magically make it the actual model! It needs to be right! Use that economic theory and common sense to think about missing parts of the graph\nAlso, can we control for those things? What would it mean to assign a single number for \\(Background\\) to someone? Or if we’re representing \\(Background\\) with multiple variables - race, gender, parental income, etc., how do we know if we’ve fully covered it?\n\n\n\n\nAnd the Bad News…\n\nRegardless, this is the kind of thinking (whether or not you do that thinking with a causal diagram) we have to do to figure out how to identify things by controlling for variables\nThere’s no way to get around having to make these sorts of assumptions if we want to identify a causal effect\nReally! No way at all! Even experiments have assumptions\nThe key is not avoiding assumptions, but making sure they’re reasonable, and verifying those assumptions where you can\n\n\n\n\nAn Example\n\nLet’s back off of those concerns a moment and generate the data ourselves so we know the truth!\nIn the below data generating process, what is the true effect of \\(X\\) on \\(Y\\)?\nLet’s figure out how to draw the causal diagram for this data generating process!\n(note: U1, U2, etc., often stand in as an unobserved common cause for two variables that are correlated but we think neither causes the other)\n\n\ntib2 &lt;- data.frame(U1 = rnorm(1000), A = rnorm(1000), B = rnorm(1000)) %&gt;%\n  mutate(C = U1 + rnorm(1000), D = U1 + rnorm(1000)) %&gt;%\n  mutate(X = A + C + rnorm(1000)) %&gt;%\n  mutate(Y = 4*X + A + B + D + rnorm(1000))\nm1 &lt;- feols(Y~X, data = tib)\ncoef(m1)\n\n(Intercept)           X \n   2.756197    3.420627 \n\n\n\n\n\nThe Diagram\n\nHere’s the diagram we can draw from that information. What paths are there from X to Y?\n\n\n\n\n\n\n\n\n\nThe Paths\n\n\\(X \\rightarrow Y\\)\n\\(X \\leftarrow A \\rightarrow Y\\)\n\\(X \\leftarrow C \\leftarrow U_1 \\rightarrow D \\rightarrow Y\\)\n\nWhat do we need to control for to close all the paths we don’t want? Assume we can’t observe (and so can’t control for) \\(U_1\\)\n\n\n\nThe Adjusted Analysis\n\nRemember, the true \\(\\beta_1\\) was 4\n\n\n\n                               m1                 m2                 m3\nDependent Var.:                 Y                  Y                  Y\n                                                                       \nConstant        2.756*** (0.0461)   -0.0261 (0.0584)    0.0218 (0.0455)\nX               3.421*** (0.0383)  4.004*** (0.0600)  4.003*** (0.0290)\nA                                  1.044*** (0.0846) 0.9848*** (0.0549)\nC                                 0.4827*** (0.0739)                   \nD                                                    0.9827*** (0.0368)\n_______________ _________________ __________________ __________________\nS.E. type                     IID                IID                IID\nObservations                1,000              1,000              1,000\nR2                        0.88897            0.96089            0.97624\nAdj. R2                   0.88886            0.96077            0.97617\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nConcept Checks\n\nWhy did we only need to control for \\(C\\) or \\(D\\) in that last example?\nDraw a graph with five variables on it: \\(X\\), \\(Y\\), \\(A\\), \\(B\\), \\(C\\). Then draw arrows at them completely at random (except to ensure there’s no “loop” where you can follow an arrow path from arrow base to head and end up where you started). Then list every path from \\(X\\) to \\(Y\\) and say what you’d need to control for to identify the effect\nWhat would you need to control for to estimate the effect of “drinking a glass of wine a day” on “lifespan”? Draw a diagram."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#data-generating-process",
    "href": "2024/weeks/week02/slides.html#data-generating-process",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nTo avoid this, we think about the data generating process\nWhat is a data generating process?\nThe data generating process is the true set of laws that determine where our data comes from\nFor example, if you hold a rock and drop it, it falls to the floor\nWhat is the data we observe? (Hold the rock & Rock is up) and (Let go & Rock is down)\nWhat is the data generating process? Gravity makes the rock fall down when you drop it"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#regressions",
    "href": "2024/weeks/week02/slides.html#regressions",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Regressions",
    "text": "Regressions\n\nIn statistics, regression is the practice of line-fitting\nWe want to use one variable to predict another\nLet’s say using \\(X\\) to predict \\(Y\\)\nWe’d refer to \\(X\\) as the “independent variable”, and \\(Y\\) as the “dependent variable” (dependent on \\(X\\) that is)\nRegression is the idea that we should characterize the relationship between \\(X\\) and \\(Y\\) as a line, and use that line to predict \\(Y\\)"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#ols-simulator",
    "href": "2024/weeks/week02/slides.html#ols-simulator",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "OLS Simulator",
    "text": "OLS Simulator\n\nTake a look at this OLS simulator and play around with it: https://econometricsbysimulation.shinyapps.io/OLS-App/\nClick “Show Residuals” to turn that on\nTry different data generating processes and standard deviations\nWhat settings make the residuals small or large? Any guesses why?\nWhat happens if you take the intercept out? What does that make our line do?\nHow close does the line come to the data generating process? Intercept and slope are in the second table below the graph under “Estimate”"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#concept-checks-1",
    "href": "2024/weeks/week02/slides.html#concept-checks-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy might we want to minimize squared residuals rather than just residuals?\nWhat’s the difference between a residual and an error?\nIf I have the below OLS-fitted line from a dataset of children:\n\n\\[ Height (Inches) = 18 + 2*Age\\]\nAnd we have the kids Darryl who is 10 years old and 40 inches tall, and Bijetri who is 9 years old and 37 inches tall, what are each of their: (a) predicted values, (b) residuals, and then what is the sum of their squared residuals?"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#recap",
    "href": "2024/weeks/week02/slides.html#recap",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Recap",
    "text": "Recap\n\nRegression is fitting a shape to data to explain the relationship more generally\nOrdinary least squares fits a straight line that minimizes the sum of squared residuals\nThat line has an intercept \\(\\beta_0\\) (Our prediction \\(\\hat{Y}\\) when \\(X = 0\\)) and a slope \\(\\beta_1\\) (how much higher we predict \\(Y\\) will be when we look at an \\(X\\) one unit higher)\nThe residual is the difference between our prediction \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X\\) and the actual number \\(Y\\)\n\n\\[ Y = \\beta_0 + \\beta_1 X + \\varepsilon \\] \\[ \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X \\]"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#data-generating-process",
    "href": "2024/weeks/week01/slides.html#data-generating-process",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nTo avoid identification error, economists think closely about the data generating process\nWhat is a data generating process?\nThe data generating process is the true set of laws that determine where our data comes from\nFor example, if you hold a rock and drop it, it falls to the floor\nWhat is the data we observe? (Hold the rock & Rock is up) and (Let go & Rock is down)\nWhat is the data generating process? Gravity makes the rock fall down when you drop it"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#causality",
    "href": "2024/weeks/week01/slides.html#causality",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nA data generating process can be described by a series of equations that describe where the data comes from. For example:\n\n\\[ X = \\gamma_0 + \\gamma_1\\varepsilon + \\nu \\]\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nThis says ” \\(X\\) is caused by \\(\\varepsilon\\) and \\(\\nu\\), and \\(Y\\) is caused by \\(X\\) and \\(\\varepsilon\\)”\nThe truth is that an increase in \\(X\\) causally increases \\(Y\\) by \\(\\beta_1\\)\nThe goal of econometrics is to be able to estimate what \\(\\beta_1\\) is accurately"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#endogeneity",
    "href": "2024/weeks/week01/slides.html#endogeneity",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Endogeneity",
    "text": "Endogeneity\n\nSo “correlation isn’t causation” isn’t quite complete\nIt’s more “only certain correlations are causal”\nMany correlations are beset by these problems like endogeneity, i.e. the presence of another variable like \\(\\varepsilon\\) related to both \\(X\\) and \\(Y\\), giving the effect a “back door”\nSo the correlation reflects both the causal effect and also the influence of \\(\\varepsilon\\)"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#random-experiments",
    "href": "2024/weeks/week01/slides.html#random-experiments",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Random Experiments",
    "text": "Random Experiments\n\nOne way around this problem is to run a random experiment\nIf we can randomly assign \\(X\\), then we know we’re not in graph a, because our graph looks like this!"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#random-experiments-1",
    "href": "2024/weeks/week01/slides.html#random-experiments-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Random Experiments",
    "text": "Random Experiments\n\nFor this reason, random experiments are generally considered the “gold standard”\nThey have problems (i.e. experimental sample might not represent the population, people may act differently knowing they’re in an experiment, etc.)\nBut regardless, we’re looking here at questions for which we can’t run an experiment, becuase it’s impossible or infeasible or immoral\nSo one way we can think about solving this endogeneity problem with econometrics is to use our observational data in such a way that it behaves as though there were an experiment being run\nPlenty of ways to do this we’ll go over in this course!"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#concept-check",
    "href": "2024/weeks/week01/slides.html#concept-check",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat does it mean to say that \\(X\\) has a causal effect on \\(Y\\)?\nWhy might the relationship between \\(X\\) and \\(Y\\) in data not be the same as the causal effect?\nWhat is an example of observational data?\nConsider the question of “Does getting an MBA make you a better manager?” What are \\(X\\) and \\(Y\\) here? What would be in the error term \\(\\varepsilon\\)? Are we likely to have an endogeneity problem here?"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#spurious-correlations",
    "href": "2024/weeks/week01/slides.html#spurious-correlations",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Spurious Correlations",
    "text": "Spurious Correlations\n\nLet’s visit this site all about “spurious” correlations (i.e. correlations that almost certainly do not reveal a true effect of one variable on the other): https://tylervigen.com/spurious-correlations\nTake a look at how easy it is to find variables that are related statistically, even though clearly neither causes the other\nDo you think this correlation is an example of inferential error (just random chance) or identification error (truly related, but not because one causes the other)? Why?"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#identification-error",
    "href": "2024/weeks/week00/slides.html#identification-error",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Identification Error",
    "text": "Identification Error\n\nWhat is an identification error?\nIdentification is how you link the result you see with the conclusion you draw from it\nFor example, say you observe that kids who play video games are more aggressive in everyday life (result), and you conclude from that result that video games make kids more aggressive (conclusion)\nIf seeing that result is actually evidence for that conclusion, then we are properly identified"
  },
  {
    "objectID": "2024/weeks/week00/slides.html#identification-error-2",
    "href": "2024/weeks/week00/slides.html#identification-error-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Identification Error",
    "text": "Identification Error\n\nFor example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts, you have committed an identification error\nOne day in and all we can do is complain, eesh"
  },
  {
    "objectID": "2024/weeks/week04/page.html",
    "href": "2024/weeks/week04/page.html",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "",
    "text": "In this week, we will build on Linear regressions by expanding our analysis using multiple regressors."
  },
  {
    "objectID": "2024/weeks/week04/page.html#lecture-slides",
    "href": "2024/weeks/week04/page.html#lecture-slides",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week04/page.html#recommended-reading",
    "href": "2024/weeks/week04/page.html#recommended-reading",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week04/page.html#communication",
    "href": "2024/weeks/week04/page.html#communication",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/weeks/week03/page.html",
    "href": "2024/weeks/week03/page.html",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "",
    "text": "In this week, we start looking how we can use data to describe relationships between two variables and distinguish between alternative scenarios."
  },
  {
    "objectID": "2024/weeks/week03/page.html#lecture-slides",
    "href": "2024/weeks/week03/page.html#lecture-slides",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week03/page.html#recommended-reading",
    "href": "2024/weeks/week03/page.html#recommended-reading",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week03/page.html#communication",
    "href": "2024/weeks/week03/page.html#communication",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/weeks/week02/page.html",
    "href": "2024/weeks/week02/page.html",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "",
    "text": "In this week, we will explore how we can use formal procedures to examine if two opposing claims or hypothesis are true or not.\nIn this week, we start looking how we can use data to describe relationships between two variables and distinguish between alternative scenarios."
  },
  {
    "objectID": "2024/weeks/week02/page.html#lecture-slides",
    "href": "2024/weeks/week02/page.html#lecture-slides",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week02/page.html#recommended-reading",
    "href": "2024/weeks/week02/page.html#recommended-reading",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week02/page.html#communication",
    "href": "2024/weeks/week02/page.html#communication",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#linear-regression---endogeneity",
    "href": "2024/weeks/week04/slides.html#linear-regression---endogeneity",
    "title": "Controls",
    "section": "Linear Regression - Endogeneity",
    "text": "Linear Regression - Endogeneity\n\nLast week was all about handling sampling variation and avoiding inference error\nThis week we’re all about endogeneity!\nWhere it pops up and what we can do about it\nAt least as a starter (we’ll revisit this topic many times)"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#endogeneity-recap",
    "href": "2024/weeks/week04/slides.html#endogeneity-recap",
    "title": "Controls",
    "section": "Endogeneity Recap",
    "text": "Endogeneity Recap\n\nFor example looking at income and corruption, the model\n\n\\[Income = \\beta_0 + \\beta_1Corruption + \\varepsilon\\]\n\nTrue \\(\\beta_1\\) is probably \\(0\\). But since \\(Political\\) \\(Stability\\) is in \\(\\varepsilon\\) & it’s related to \\(Corruption\\), OLS will mistakenly assign the effect of \\(Pol-Stability\\) to the effect of \\(Corr\\), making it look like there’s a positive effect when there isn’t one\nHere we’re mistakenly finding a positive effect when the truth is \\(0\\), but it could be anything - negative effect when truth is \\(0\\), positive effect when the truth is a bigger/smaller positive effect, etc etc"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#to-the-rescue",
    "href": "2024/weeks/week04/slides.html#to-the-rescue",
    "title": "Controls",
    "section": "To the Rescue",
    "text": "To the Rescue\n\nOne way we can solve this problem is through the use of control variables\nWhat if \\(Political Stability\\) weren’t in \\(\\varepsilon\\)?\nOLS would know how to separate out its effect from the \\(Corruption\\) effect. How? Just put it in the model directly!\n\n\\[Income = \\beta_0 + \\beta_1Corruption + \\beta_2PoliticalStability + \\varepsilon\\]\n\nNow we have a multivariate regression model. Our estimate \\(\\hat{\\beta}_1\\) will not be biased by \\(Political Stability\\) because we’ve controlled for it\n\n(probably more accurate to say “covariates” or “variables to adjust for” than “control variables” and “adjust for” rather than “control for” but hey what are you gonna do, “control” is standard)"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#to-the-rescue-1",
    "href": "2024/weeks/week04/slides.html#to-the-rescue-1",
    "title": "Controls",
    "section": "To the Rescue",
    "text": "To the Rescue\n\nSo the task of solving our endogeneity problems in estimating \\(\\beta_1\\) using \\(\\hat{\\beta}_1\\) comes down to us finding all the elements of \\(\\varepsilon\\) that are related to \\(X\\) and adding them to the model\nAs we add them, they leave \\(\\varepsilon\\) and hopefully we end up with a version of \\(\\varepsilon\\) that is no longer related to \\(X\\)\nIf \\(cov(X,\\varepsilon) = 0\\) then we have an unbiased estimate!\n(of course, we have no way of checking if that’s true - it’s based on what we think the data generating process looks like)"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#how",
    "href": "2024/weeks/week04/slides.html#how",
    "title": "Controls",
    "section": "How?",
    "text": "How?\n\nHow does this actually work?\nControlling for a variable works by removing variation in \\(X\\) and \\(Y\\) that is explained by the control variable\nSo our estimate of \\(\\hat{\\beta}_1\\) is based on just the variation in \\(X\\) and \\(Y\\) that is unrelated to the control variable\nAny accidentally-assigning-the-value-of-PoliticalStability-to-Corruption can’t happen because we’ve removed the effect of \\(Political Stability\\) on \\(Corruption\\) as well as the effect of \\(Political Stability\\) on \\(Income\\)\nWe’re asking at that point, holding \\(Political Stability\\) constant, i.e. comparing two different countries with the same \\(Polities\\) , how is \\(Corruption\\) related to \\(Income\\)?"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#example",
    "href": "2024/weeks/week04/slides.html#example",
    "title": "Controls",
    "section": "Example",
    "text": "Example\nThe true effect is \\(\\beta_1 = 3\\). Notice \\(Z\\) is binary and is related to \\(X\\) and \\(Y\\) but isn’t in the model!\n\ntib &lt;- tibble(Z = 1*(rnorm(1000) &gt; 0)) %&gt;%\n  mutate(X = Z + rnorm(1000)) %&gt;%\n  mutate(Y = 2 + 3*X + 2*Z + rnorm(1000))\nfeols(Y~X, data = tib) %&gt;%\n  etable()\n\n                                .\nDependent Var.:                 Y\n                                 \nConstant        2.756*** (0.0461)\nX               3.421*** (0.0383)\n_______________ _________________\nS.E. type                     IID\nObservations                1,000\nR2                        0.88897\nAdj. R2                   0.88886\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#example-1",
    "href": "2024/weeks/week04/slides.html#example-1",
    "title": "Controls",
    "section": "Example",
    "text": "Example\nTo remove what part of \\(X\\) and \\(Y\\) is explained by \\(Z\\), we can get the mean of \\(X\\) and \\(Y\\) by values of \\(Z\\)\n\ntib &lt;- tib %&gt;%\n  group_by(Z) %&gt;% \n  mutate(Y_mean = mean(Y), X_mean = mean(X))\nhead(tib)\n\n# A tibble: 6 × 5\n# Groups:   Z [2]\n      Z       X      Y Y_mean  X_mean\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1     1  0.855   5.98    6.91  0.967 \n2     1 -1.59   -0.226   6.91  0.967 \n3     1  0.806   6.08    6.91  0.967 \n4     1 -0.0302  2.94    6.91  0.967 \n5     0 -0.490  -0.997   1.95 -0.0121\n6     0 -0.512   0.383   1.95 -0.0121"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#example-2",
    "href": "2024/weeks/week04/slides.html#example-2",
    "title": "Controls",
    "section": "Example",
    "text": "Example\nNow, Y_mean and X_mean are the mean of Y and X for the values of Z, i.e. the part of Y and X explained by Z. So subtract those parts out to get residuals Y_res and X_res!\n\ntib &lt;- tib %&gt;%\n  mutate(Y_res = Y - Y_mean, X_res = X - X_mean)\nhead(tib)\n\n# A tibble: 6 × 7\n# Groups:   Z [2]\n      Z       X      Y Y_mean  X_mean  Y_res  X_res\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1  0.855   5.98    6.91  0.967  -0.928 -0.113\n2     1 -1.59   -0.226   6.91  0.967  -7.14  -2.55 \n3     1  0.806   6.08    6.91  0.967  -0.827 -0.161\n4     1 -0.0302  2.94    6.91  0.967  -3.98  -0.997\n5     0 -0.490  -0.997   1.95 -0.0121 -2.95  -0.478\n6     0 -0.512   0.383   1.95 -0.0121 -1.57  -0.499"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#example-3",
    "href": "2024/weeks/week04/slides.html#example-3",
    "title": "Controls",
    "section": "Example",
    "text": "Example\nWhat do we get now?\n\nfeols(Y_res ~ X_res, data = tib) %&gt;%\n  etable()\n\n                                .\nDependent Var.:             Y_res\n                                 \nConstant        5.73e-17 (0.0319)\nX_res           3.030*** (0.0319)\n_______________ _________________\nS.E. type                     IID\nObservations                1,000\nR2                        0.90060\nAdj. R2                   0.90050\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#example-4",
    "href": "2024/weeks/week04/slides.html#example-4",
    "title": "Controls",
    "section": "Example",
    "text": "Example\nCompare this to actually including Z as a control:\n\nfeols(Y ~ X + Z, data = tib) %&gt;%\n  etable()\n\n                                .\nDependent Var.:                 Y\n                                 \nConstant        1.988*** (0.0441)\nX               3.030*** (0.0319)\nZ               1.993*** (0.0712)\n_______________ _________________\nS.E. type                     IID\nObservations                1,000\nR2                        0.93782\nAdj. R2                   0.93769\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#a-continuous-control",
    "href": "2024/weeks/week04/slides.html#a-continuous-control",
    "title": "Controls",
    "section": "A Continuous Control",
    "text": "A Continuous Control"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#what-do-we-get",
    "href": "2024/weeks/week04/slides.html#what-do-we-get",
    "title": "Controls",
    "section": "What do we get?",
    "text": "What do we get?\n\nWe can remove some of the relationship between \\(X\\) and \\(\\varepsilon\\)\nPotentially all of it, making \\(\\hat{\\beta}_1\\) us an unbiased (i.e. correct on average, but sampling variation doesn’t go away!) estimate of \\(\\beta_1\\)\nMaybe we can also get some estimates of \\(\\beta_2\\), \\(\\beta_3\\)… but be careful, they’re subject to the same identification and endogeneity problems!\nOften in econometrics we focus on getting one parameter, \\(\\hat{\\beta}_1\\), exactly right and don’t focus on parameters we haven’t put much effort into identifying"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#concept-checks",
    "href": "2024/weeks/week04/slides.html#concept-checks",
    "title": "Controls",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nSelene is a huge bore at parties, but sometimes brings her girlfriend Donna who is super fun. If you regressed \\(PartyFunRating\\) on \\(SeleneWasThere\\) but not \\(DonnaWasThere\\), what would the coefficient on \\(SeleneWasThere\\) look like and why?\nDescribe the steps necessary to estimate the effect of \\(Exports\\) on \\(GrowthRate\\) while controlling for \\(AmountofConflict\\) (a continuous variable). There are three “explain/regress” steps and two “subtract” steps.\nIf we estimate the same \\(\\hat{\\beta}_1\\) with or without \\(Z\\) added as a control, does that mean we have no endogeneity problem? What does it mean exactly?"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#have-we-solved-it",
    "href": "2024/weeks/week04/slides.html#have-we-solved-it",
    "title": "Controls",
    "section": "Have We Solved It?",
    "text": "Have We Solved It?\n\nIncluding controls for every part of (what used to be) \\(\\varepsilon\\) that is related to \\(X\\) clears up any endogeneity problem we had with \\(X\\)\nSo… when we add a control, does that do it? How do we know?\nInconveniently, the data alone will never tell us if we’ve solved endogeniety\nWe can’t just check \\(X\\) against the remaining \\(\\varepsilon\\) because we never see \\(\\varepsilon\\) - what we have left over after a regression is the real-world residual, not the true-model error"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#drawing-a-diagram",
    "href": "2024/weeks/week04/slides.html#drawing-a-diagram",
    "title": "Controls",
    "section": "Drawing a Diagram",
    "text": "Drawing a Diagram\n\nEndogeneity is all about the alternate reasons why two variables might be related *other than the causal effect\nWe can represent all the reasons two variables are related with a diagram\nPut down on paper how you think the world works, and where you think the data came from! This is economic modeling but with less math"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#drawing-a-diagram-1",
    "href": "2024/weeks/week04/slides.html#drawing-a-diagram-1",
    "title": "Controls",
    "section": "Drawing a Diagram",
    "text": "Drawing a Diagram\n\nList out all the variables relevant to the DGP (including the ones we can’t measure or put our finger on!)\nDraw arrows between them reflecting what causes what else\nList all the paths from \\(X\\) to \\(Y\\) - these paths are reasons why \\(X\\) and \\(Y\\) are related!\nControl for at least one variable on each path you want to close (isn’t the effect you want)"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#drawing-a-diagram-2",
    "href": "2024/weeks/week04/slides.html#drawing-a-diagram-2",
    "title": "Controls",
    "section": "Drawing a Diagram",
    "text": "Drawing a Diagram"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#detailing-paths",
    "href": "2024/weeks/week04/slides.html#detailing-paths",
    "title": "Controls",
    "section": "Detailing Paths",
    "text": "Detailing Paths\n\nThe goal is to list all the paths that go from the cause of our choice to the outcome variable (no loops)\nThat way we know what we need to control for to close the paths!\nControl for any one variable on the path, and suddenly there’s no variation from that variable any more - the causal chain is broken and the path is closed!\nA path counts no matter which direction the arrows point on it (the arrow direction matters but we’ll get to that next time)\nIf the path isn’t part of what answers our research question, it’s a back door we want to be closed"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#paths",
    "href": "2024/weeks/week04/slides.html#paths",
    "title": "Controls",
    "section": "Paths",
    "text": "Paths\n\n\\(Preschool \\rightarrow Earnings\\)\n\\(Preschool \\rightarrow Skills \\rightarrow Earnings\\)\n\\(Preschool \\leftarrow Location \\rightarrow Earnings\\)\n\\(Preschool \\leftarrow Background \\rightarrow Earnings\\)\n\\(Preschool \\leftarrow Background \\rightarrow Skills \\rightarrow Earnings\\)\n\\(Preschool \\rightarrow Skills \\leftarrow Background \\rightarrow Earnings\\)"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#closing-paths",
    "href": "2024/weeks/week04/slides.html#closing-paths",
    "title": "Controls",
    "section": "Closing Paths",
    "text": "Closing Paths\n\nWe want the ways that \\(Preschool\\) causes \\(Earnings\\) - that’s the first two, \\(Preschool \\rightarrow Earnings\\) and \\(Preschool \\rightarrow Skills \\rightarrow Earnings\\)\nThe rest we want to close! They’re back doors\n\\(Location\\) is on #3, so if we control for \\(Location\\), 3 is closed\n\\(Background\\) is on the rest, so if we control for \\(Background\\), the rest are closed\nSo if we estimate the below OLS equation, \\(\\hat{\\beta}_1\\) will be unbiased!\n\n\\[Earnings = \\beta_0 + \\beta_1Preschool + \\beta_2Location + \\beta_3Background+\\varepsilon\\]"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#and-the-bad-news",
    "href": "2024/weeks/week04/slides.html#and-the-bad-news",
    "title": "Controls",
    "section": "And the Bad News…",
    "text": "And the Bad News…\n\nThis assumes that the model we drew was accurate. Did we leave any important variables or arrows out? Think hard!\nWhat other variables might belong on this graph? Would they be on a path that gives an alternate explanation?\nJust because we say that’s the model doesn’t magically make it the actual model! It needs to be right! Use that economic theory and common sense to think about missing parts of the graph\nAlso, can we control for those things? What would it mean to assign a single number for \\(Background\\) to someone? Or if we’re representing \\(Background\\) with multiple variables - race, gender, parental income, etc., how do we know if we’ve fully covered it?"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#and-the-bad-news-1",
    "href": "2024/weeks/week04/slides.html#and-the-bad-news-1",
    "title": "Controls",
    "section": "And the Bad News…",
    "text": "And the Bad News…\n\nRegardless, this is the kind of thinking (whether or not you do that thinking with a causal diagram) we have to do to figure out how to identify things by controlling for variables\nThere’s no way to get around having to make these sorts of assumptions if we want to identify a causal effect\nReally! No way at all! Even experiments have assumptions\nThe key is not avoiding assumptions, but making sure they’re reasonable, and verifying those assumptions where you can"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#an-example",
    "href": "2024/weeks/week04/slides.html#an-example",
    "title": "Controls",
    "section": "An Example",
    "text": "An Example\n\nLet’s back off of those concerns a moment and generate the data ourselves so we know the truth!\nIn the below data generating process, what is the true effect of \\(X\\) on \\(Y\\)?\nLet’s figure out how to draw the causal diagram for this data generating process!\n(note: U1, U2, etc., often stand in as an unobserved common cause for two variables that are correlated but we think neither causes the other)\n\n\ntib2 &lt;- data.frame(U1 = rnorm(1000), A = rnorm(1000), B = rnorm(1000)) %&gt;%\n  mutate(C = U1 + rnorm(1000), D = U1 + rnorm(1000)) %&gt;%\n  mutate(X = A + C + rnorm(1000)) %&gt;%\n  mutate(Y = 4*X + A + B + D + rnorm(1000))\nm1 &lt;- feols(Y~X, data = tib)\ncoef(m1)\n\n(Intercept)           X \n   2.756197    3.420627"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#controlling",
    "href": "2024/weeks/week04/slides.html#controlling",
    "title": "Controls",
    "section": "Controlling",
    "text": "Controlling\n\nWe achieve all this just by adding the variable to the OLS equation!\nWe can, of course, include more than one control, or controls that aren’t binary\nUse OLS to predict \\(X\\) using all the controls, then take the residual (the part not explained by the controls)\nUse OLS to predict \\(Y\\) using all the controls, then take the residual (the part not explained by the controls)\nNow do OLS of just the \\(Y\\) residuals on just the \\(X\\) residuals"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#causal-diagrams",
    "href": "2024/weeks/week04/slides.html#causal-diagrams",
    "title": "Controls",
    "section": "Causal Diagrams",
    "text": "Causal Diagrams\n\n“What do I have to control for to solve the endogeneity problem” is an important and difficult question!\nTo answer it we need to think about the data-generating process\nOne way to do that is to draw a causal diagram\nA causal diagram describes the variables responsible for generating data and how they cause each other\nOnce we have written down our diagram, we’ll know what we need to control for\n(hopefully we have data on everything we need to control for! Often we don’t)"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#drawing-a-diagram-3",
    "href": "2024/weeks/week04/slides.html#drawing-a-diagram-3",
    "title": "Controls",
    "section": "Drawing a Diagram",
    "text": "Drawing a Diagram\n\nWe observe that, in the data, \\(ShortsWearing\\) and \\(IceCreamEating\\) are related. Why?\nMaybe, we theorize, that wearing shorts causes you to eat ice cream ( \\(ShortsWearing \\rightarrow IceCreamEating\\) )\nHowever, there’s another explanation/path: \\(Temperature\\) causes both ( \\(ShortsWearing \\leftarrow Temperature \\rightarrow IceCreamEating\\) )\nWe need to control for temperature to close this path!\nOnce it’s closed, the only path left is \\(ShortsWearing \\rightarrow IceCreamEating\\), so if we do see a relationship still in the data, we know we’ve identified the causal effect"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#preschool-and-adult-earnings",
    "href": "2024/weeks/week04/slides.html#preschool-and-adult-earnings",
    "title": "Controls",
    "section": "Preschool and Adult Earnings",
    "text": "Preschool and Adult Earnings\nDoes going to preschool improve your earnings as an adult?"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#the-diagram",
    "href": "2024/weeks/week04/slides.html#the-diagram",
    "title": "Controls",
    "section": "The Diagram",
    "text": "The Diagram\n\nHere is the diagram we can draw from that information. What paths are there from X to Y?"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#concept-checks-1",
    "href": "2024/weeks/week04/slides.html#concept-checks-1",
    "title": "Controls",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy did we only need to control for \\(C\\) or \\(D\\) in that last example?\nDraw a graph with five variables on it: \\(X\\), \\(Y\\), \\(A\\), \\(B\\), \\(C\\). Then draw arrows at them completely at random (except to ensure there’s no “loop” where you can follow an arrow path from arrow base to head and end up where you started). Then list every path from \\(X\\) to \\(Y\\) and say what you’d need to control for to identify the effect\nWhat would you need to control for to estimate the effect of “drinking a glass of wine a day” on “lifespan”? Draw a diagram.\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2024/weeks/week05/page.html",
    "href": "2024/weeks/week05/page.html",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "",
    "text": "In this week, we will focus on how we analyse and interpret binary independent variables."
  },
  {
    "objectID": "2024/weeks/week05/page.html#lecture-slides",
    "href": "2024/weeks/week05/page.html#lecture-slides",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week05/page.html#recommended-reading",
    "href": "2024/weeks/week05/page.html#recommended-reading",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week05/page.html#communication",
    "href": "2024/weeks/week05/page.html#communication",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/weeks/week06/page.html",
    "href": "2024/weeks/week06/page.html",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "",
    "text": "This is reading week. Look at moodle for recap lectures and recap seminar questions."
  },
  {
    "objectID": "2024/weeks/week06/page.html#lecture-slides",
    "href": "2024/weeks/week06/page.html#lecture-slides",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week06/page.html#recommended-reading",
    "href": "2024/weeks/week06/page.html#recommended-reading",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week06/page.html#communication",
    "href": "2024/weeks/week06/page.html#communication",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/weeks/week07/page.html",
    "href": "2024/weeks/week07/page.html",
    "title": "🗓️ Week 07 - Revision / Non Linearity",
    "section": "",
    "text": "In this week, we revisited slides from Week 5 and focused on Non-linearity and transformations. on how we analyse within variation and include fixed effects."
  },
  {
    "objectID": "2024/weeks/week07/page.html#lecture-slides",
    "href": "2024/weeks/week07/page.html#lecture-slides",
    "title": "🗓️ Week 07 - Revision / Non Linearity",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2024/weeks/week07/page.html#recommended-reading",
    "href": "2024/weeks/week07/page.html#recommended-reading",
    "title": "🗓️ Week 07 - Revision / Non Linearity",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2024/weeks/week07/page.html#communication",
    "href": "2024/weeks/week07/page.html#communication",
    "title": "🗓️ Week 07 - Revision / Non Linearity",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2024/communication.html",
    "href": "2024/communication.html",
    "title": "Announcements",
    "section": "",
    "text": "📢 Description\n\n\n\n\n\nJoin our Discord Channel for faster communication !!!\n\n\n\n\nRequirements\nDownload and install STATA or R. Throughout the class, we will rely on STATA for seminars but codes will be provided for R as well."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#recap",
    "href": "2024/weeks/week04/slides.html#recap",
    "title": "Controls",
    "section": "Recap",
    "text": "Recap\n\nWe believe that our true model looks like this:\n\n\\[Y = \\beta_0 + \\beta_1X+\\varepsilon\\]\n\nWhere \\(\\varepsilon\\) is everything that determines \\(Y\\) other than \\(X\\)\nIf \\(X\\) is related to some of those things, we have endogeneity\nEstimating this with OLS, will mistake the effect of those other things for the effect of \\(X\\), and our \\(\\hat{\\beta}_1\\) won’t represent the true \\(\\beta_1\\) no matter how many observations we have"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#how-does-this-actually-work",
    "href": "2024/weeks/week04/slides.html#how-does-this-actually-work",
    "title": "Controls",
    "section": "How does this actually work?",
    "text": "How does this actually work?\n\nControlling for a variable works by removing variation in \\(X\\) and \\(Y\\) that is explained by the control variable\nSo our estimate of \\(\\hat{\\beta}_1\\) is based on just the variation in \\(X\\) and \\(Y\\) that is unrelated to the control variable\nAny accidentally-assigning-the-value-of-PoliticalStability-to-Corruption can’t happen because we’ve removed the effect of \\(Political Stability\\) on \\(Corruption\\) as well as the effect of \\(Political Stability\\) on \\(Income\\)\nWe’re asking at that point, holding \\(Political Stability\\) constant, i.e. comparing two different countries with the same \\(Polities\\) , how is \\(Corruption\\) related to \\(Income\\)?"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#graphically",
    "href": "2024/weeks/week04/slides.html#graphically",
    "title": "Controls",
    "section": "Graphically",
    "text": "Graphically"
  },
  {
    "objectID": "2024/weeks/week05/slides.html#omitted-variable-bias-1",
    "href": "2024/weeks/week05/slides.html#omitted-variable-bias-1",
    "title": "🗓️ Week 5 Functional Forms",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\n\nMore precisely we have that the mean of the \\(\\hat{\\beta}_1\\) sampling distribution is\n\n\\[\\beta_1 + corr(X,\\varepsilon)\\frac{\\sigma_\\varepsilon}{\\sigma_X}\\]\n\nIf \\(Z\\) hangs around \\(X\\), but \\(Y\\) doesn’t know about it, then the coefficient on \\(X\\) will get all the credit for \\(Z\\)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#non-continuous-dependent-variables",
    "href": "2024/weeks/week07/slides.html#non-continuous-dependent-variables",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Non-Continuous Dependent Variables",
    "text": "Non-Continuous Dependent Variables\nWhen might dependent variables not be continuous and have infinite range?\n\nYears working at current job (can’t be negative)\nAre you self-employed? (Binary)\nNumber of children (must be a round number, can’t be negative)\nWhich brand of soda did you buy? (categorical)\nDid you recover from your disease? (binary)\nHow satisfied are you with your purchase on a 1-5 scale? (must be a round number from 1 to 5, and the difference between 1 and 2 isn’t necessarily the same as the difference between 2 and 3)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#binary-dependent-variables",
    "href": "2024/weeks/week07/slides.html#binary-dependent-variables",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Binary Dependent Variables",
    "text": "Binary Dependent Variables\n\nIn many cases, such as variables that must be round numbers, or can’t be negative, even though there are ways of properly handling these issues, people will usually ignore the problem and just use OLS, as long as the data is continuous-ish (i.e. doesn’t have a LOT of observations right at 0 next to the impossible negative values, or has lots of different values so the round number smooth out)\nHowever, the problems of using OLS are a bit worse for binary data, and so they’re the most common case in which we do something special to account for it\nBinary dependent variables are also really common! We’re often interested in whether a certain outcome happened or didn’t (if we want to know if a drug was effective, we are likely asking if you are cured or not!)\n\nSo, how can we deal with having a binary dependent variable, and why do they give OLS such problems?"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#the-linear-probability-model",
    "href": "2024/weeks/week07/slides.html#the-linear-probability-model",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "The Linear Probability Model",
    "text": "The Linear Probability Model\n\nFirst off, let’s ignore the completely unexplained warnings I’ve just given you and do it with OLS anyway, and see what happens\nRunning OLS with a binary dependent variable is called the “linear probability model” or LPM\n\n\\[ D = \\beta_0 + \\beta_1X + \\varepsilon \\]\nThroughout these slides, let’s use \\(D\\) to refer to a binary variable"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#the-linear-probability-model-1",
    "href": "2024/weeks/week07/slides.html#the-linear-probability-model-1",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "The Linear Probability Model",
    "text": "The Linear Probability Model\n\nIn terms of how we do it, the interpretation is the exact same as regular OLS, so you can bring in all your intuition\nThe only difference is that our interpretation of the dependent variable is now in probability terms\nIf \\(\\hat{\\beta}_1 = .03\\), that means that a one-unit increase in \\(X\\) is associated with a three percentage point increase in the probability that \\(D = 1\\)\n(percentage points! Not percentage - an increase from .1 to .13, say, not .1 to .103)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#the-linear-probability-model-2",
    "href": "2024/weeks/week07/slides.html#the-linear-probability-model-2",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "The Linear Probability Model",
    "text": "The Linear Probability Model\nSo what’s the problem?\nThe linear probability model can lead to…\n\nTerrible predictions\nIncorrect slopes that don’t acknowledge the boundaries of the data"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#terrible-predictions",
    "href": "2024/weeks/week07/slides.html#terrible-predictions",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Terrible Predictions",
    "text": "Terrible Predictions\n\nOLS fits a straight line. So if you increase or decrease \\(X\\) enough, eventually you’ll predict that the probability of \\(D = 1\\) is bigger than 1, or lower than 0. Impossible!\nWe can address part of this by just not trying to predict outside the range of the data, but if \\(X\\) has a lot of variation in it, we might get those impossible predictions even for values in our data. And what do we do with that?\n(Also, because errors tend to be small for certain ranges of \\(X\\) and large for others, we have to use heteroskedasticity-robust standard errors)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#terrible-predictins",
    "href": "2024/weeks/week07/slides.html#terrible-predictins",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Terrible Predictins",
    "text": "Terrible Predictins"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#incorrect-slopes",
    "href": "2024/weeks/week07/slides.html#incorrect-slopes",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Incorrect Slopes",
    "text": "Incorrect Slopes\n\nAlso, OLS requires that the slopes be constant\n(Not necessarily if you use a polynomial or logarithm, but the following critique still applies)\nThis is not what we want for binary data!\nAs the prediction gets really close to 0 or 1, the slope should flatten out to nothing\nIf we predict there’s a .50 chance of \\(D = 1\\), a one-unit increase in \\(X\\) with \\(\\hat{\\beta}_1 = .03\\) would increase that to .53\nIf we predict there’s a .99 chance of \\(D = 1\\), a one-unit increase in \\(X\\) with \\(\\hat{\\beta}_1 = .03\\) would increase that to 1.02…\nUh oh! The slope should be flatter near the edges. We need the slope to vary along the range of \\(X\\)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#incorrect-slopes-1",
    "href": "2024/weeks/week07/slides.html#incorrect-slopes-1",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Incorrect Slopes",
    "text": "Incorrect Slopes\n\nWe can see how much the OLS slopes are overstating changes in \\(D\\) as \\(X\\) changes near the edges by comparing an OLS fit to just regular ol’ local means, with no shape imposed at all\nWe’re not forcing the red line to flatten out - it’s doing that naturally as the mean can’t possibly go any lower! OLS barrels on through though"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#linear-probability-model",
    "href": "2024/weeks/week07/slides.html#linear-probability-model",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\nSo what can we make of the LPM?\n\nBad if we want to make predictions\nBad at estimating slope if we’re looking near the edges of 0 and 1\n(which means it’s especially bad if the average of \\(D\\) is near 0 or 1)\n\nWhen might we use it anyway?\n\nIt behaves better in small samples than methods estimated by maximum likelihood (which many other methods are)\nIf we only care about slopes far away from the boundaries\nIf alternate methods (like we’re about to go into) put too many other statistical demands on the data (OLS is very “easy” from a computational standpoint)\nIf we’re using lots of fixed effects (OLS deals with these far more easily than nonlinear methods)\nIf our right-hand side is just binary variables (if X has limited range it might not predict out of 0-1!)"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#generalized-linear-models",
    "href": "2024/weeks/week07/slides.html#generalized-linear-models",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nSo LPM has problems. What can we do instead?\nLet’s introduce the concept of the Generalized Linear Model\n\nHere’s an OLS equation:\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\nHere’s a GLM equation:\n\\[ E(Y | X) = F(\\beta_0 + \\beta_1X) \\]\nWhere \\(F()\\) is some function."
  },
  {
    "objectID": "2024/weeks/week07/slides.html#generalized-linear-models-1",
    "href": "2024/weeks/week07/slides.html#generalized-linear-models-1",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\\[ E(D | X) = F(\\beta_0 + \\beta_1X) \\]\n\nWe can call the \\(\\beta_0 + \\beta_1X\\) part, which is the same as in OLS, the index function. It’s a linear function of our variable \\(X\\) (plus whatever other controls we have in there), same as before\nBut to get our prediction of what \\(Y\\) will be conditional on what \\(X\\) is ( \\(D|X\\) ), we do one additional step of running it through a function \\(F()\\) first. We call this function a link function since it links the index function to the outcome\nIf \\(F(z) = z\\), then we’re basically back to OLS\nBut if \\(F()\\) is nonlinear, then we can account for all sorts of nonlinear dependent variables!\n\nSo in other words, our prediction of \\(D\\) is still based on the linear index, but we run it through some nonlinear function first to get our nonlinear output!"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#generalized-linear-models-2",
    "href": "2024/weeks/week07/slides.html#generalized-linear-models-2",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nWe can also think of this in terms of the latent variable interpretation\n\\[ D^* = \\beta_0 + \\beta_1X \\]\nWhere \\(D^*\\) is an unseen “latent” variable that can take any value, just like a regular OLS dependent variable (and roughly the same in concept as our index function)\nAnd we convert that latent variable to a proabability using some function\n\\[ E(D | X) = F(D^*) \\]\nand perhaps saying something like “if we estimate \\(Y^*\\) is above the number \\(c\\), then we predict \\(D = 1\\)”"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#probit-and-logit",
    "href": "2024/weeks/week07/slides.html#probit-and-logit",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Probit and Logit",
    "text": "Probit and Logit\n\nLet’s go back to our index-and-function interpretation. What function should we use?\n(many many different options depending on your dependent variable - poisson for count data, log link for nonnegative skewed values, multinomial logit for categorical data…)\nFor binary dependent variables the two most common link functions are the probit and logistic links. We often call a regression with a logistic link a “logit regression”\n\n\\[ Probit(index) = \\Phi(index) \\]\nwhere \\(\\Phi()\\) is the standard normal cumulative distribution function (i.e. the probability that a random standard normal value is less than or equal to \\(index\\) )\n\\[ Logistic(index) = \\frac{e^{index}}{1+e^{index}} \\]\nFor most purposes it doesn’t matter whether you use probit or logit, but logit is getting much more popular recently (due to its common use in data science - it’s computationally easier) so we’ll focus on that, and just know that pretty much all of this is the same with probit"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#logit",
    "href": "2024/weeks/week07/slides.html#logit",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Logit",
    "text": "Logit\n\nNotice that we can’t possibly predict a value outside of 0 or 1, no matter how wild \\(X\\) and our index get\nAs \\(index\\) goes to \\(-\\infty\\),\n\n\\[ Logistic(index) \\rightarrow  \\frac{0}{1+0} = 0 \\]\n\nAnd as \\(index\\) goes to \\(\\infty\\),\n\n\\[ Logistic(index) \\rightarrow  \\frac{\\infty}{1+\\infty } = 1 \\]"
  },
  {
    "objectID": "2024/weeks/week07/slides.html#logit-1",
    "href": "2024/weeks/week07/slides.html#logit-1",
    "title": "🗓️ Week 7 Interactions & beyond ols ",
    "section": "Logit",
    "text": "Logit\n\nAlso notice that, like the local means did, its slope flattens out near the edges"
  }
]