[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PB4A7 - Quantitative Applications for Behavioural Science",
    "section": "",
    "text": "📑 Course Brief\nDescription: In this course, students will immerse themselves in the world of causal inference methodologies, a cornerstone in behavioural science research. The curriculum guides learners through the essential processes of cleaning, analysing, and visualising secondary data, equipping them with the skills to conduct robust and insightful research. Through practical examples, students will learn to adeptly navigate various research designs, and develop the proficiency to communicate their findings effectively, fostering a deeper understanding and application of best practices in behavioural science.\nFocus: Understand the fundamentals of causal inference and its applications in Behavioural Science. Master statistical tools used by psychologists, political scientists, and economists. Recognize and address contemporary issues in behavioural science.\nDownload the full syllabus here\n🛠️ Requirements: For students who have no prior experience with statistics and/or STATA, the completion of the following Digital Skills class is highly recommended:\nIntroduction to STATA\n\n\n🎯 Learning Objectives\n\nHow to distinguish Causation from Correlation\nHow different research designs work\nHow to apply different research designs on Stata\nHow to effectively visualise your findings\n\n\n  Enter the class"
  },
  {
    "objectID": "2023/weeks/week01/slides.html",
    "href": "2023/weeks/week01/slides.html",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "",
    "text": "library(directlabels)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(grid)\nlibrary(ggplot2)\ntheme_metro &lt;- function(x) {\n  theme_classic() + \n  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),\n        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),\n        text = element_text(size = 16),\n        axis.title.x = element_text(hjust = 1),\n        axis.title.y = element_text(hjust = 1, angle = 0))\n}\ntheme_metro_regtitle &lt;- function(x) {\n  theme_classic() + \n  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),\n        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),\n        text = element_text(size = 16))\n}"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#your-lecturer",
    "href": "2023/weeks/week01/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#teaching-assistants",
    "href": "2023/weeks/week01/slides.html#teaching-assistants",
    "title": "🗓️ Week 01 Introduction",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\n\n\n\n\n\nOctopnuian Arveda  PhD Candidate at OctopusLab, OUN  MSc in Octopus Transportation 📧 \n\n\n\n\n\n\nOctopnuian Arveda  PhD Candidate at OctopusLab, UOS  MSc in Octopus Transportation 📧 \n\n\n\n\n\n\n\nMY_COURSE_CODE – MY_COURSE_NAME"
  },
  {
    "objectID": "2023/index.html",
    "href": "2023/index.html",
    "title": "PB4A7 - Quantitative Applications for Behavioural Science",
    "section": "",
    "text": "🧑🏻‍🏫 Our Team\n\nCourse ConvenorTeaching Staff\n\n\n\nDr George Melios  Research Fellow  London School of Economics and Political Science 📧 g.melios at lse dot ac dot uk\nOffice Hours:\n\nWhen: Mondays 11:30 - 12:30\nWhere: Zoom\nHow to book: Student Hub\n\n\n\n\nLazaros Chatzilazarou  PhD Candidate at City University  📧 L.A.Chatzilazarou at lse dot ac dot uk\nHelp Sessions:\n\nWhen: Tuesdays 11:00-12:00\nWhere: CBG.G.01\n\n\n\n\n\n\n📍 Lecture\nWednesdays 13:00-14:00 at MAR.1.10\n\n\n💻 Seminars\n\nGroup 01\n\n📆 Wednesdays\n⌚ 14:00 - 15:00\n📍 FAW.4.02\n\n\n\n\nGroup 02\n\n📆 Tuesdays\n⌚ 15:00 - 16:00\n📍 FAW.4.02\n\n\n\nGroup 03\n\n📆 Tuesdays\n⌚ 16:00 - 17:00\n📍 FAW.4.02"
  },
  {
    "objectID": "2023/weeks/week01/page.html",
    "href": "2023/weeks/week01/page.html",
    "title": "🗓️ Week 01 - Introduction",
    "section": "",
    "text": "This session will introduce you to the concept of applied quantitative research in general, give a brief outline of the course, and address organizational issues. We will then have a short followup to themes discussed during the presessional."
  },
  {
    "objectID": "2023/weeks/week01/page.html#lecture-slides",
    "href": "2023/weeks/week01/page.html#lecture-slides",
    "title": "🗓️ Week 01 - Introduction",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week01/page.html#recommended-reading",
    "href": "2023/weeks/week01/page.html#recommended-reading",
    "title": "🗓️ Week 01 - Introduction",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nChapters 1 & 2 from The Effect book\nInteresting reading: The Credibility Revolution in Empirical Economics: How Better Research Design Is Taking the Con out of Econometrics"
  },
  {
    "objectID": "2023/weeks/week01/page.html#communication",
    "href": "2023/weeks/week01/page.html#communication",
    "title": "🗓️ Week 01 - Introduction",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-is-this-class",
    "href": "2023/weeks/week01/slides.html#what-is-this-class",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What is this class",
    "text": "What is this class\n\nIt’s a research design course on quasi-experimental methods\nLet’s break it down:\n\nResearch Design -&gt; How you transform an idea / question about the world to applied research\nQuasi-Experiments -&gt; Not by designing new experiments with random assignment. (how? We will see over the next 11 weeks)"
  },
  {
    "objectID": "2023/weeks/week09/slides.html",
    "href": "2023/weeks/week09/slides.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#your-lecturer",
    "href": "2023/weeks/week09/slides.html#your-lecturer",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#what-is-this-class",
    "href": "2023/weeks/week09/slides.html#what-is-this-class",
    "title": "Instrumental Variables",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be\n\n\n\nSo what is econometrics?\nEconometrics focuses on the study of observational data\nObservational data are measurements of things that the researcher does not control\nGiven that we are working with observational data, we still want to understand the causes of things\nThe world is what it is, we are only here to study it"
  },
  {
    "objectID": "2023/weeks/week08/slides.html",
    "href": "2023/weeks/week08/slides.html",
    "title": "🗓️ Week 8  Within variation",
    "section": "",
    "text": "So far we’ve been learning about how to set up, run, and interpret an ordinary least squares regression\nThis is a key skill for anyone doing anything with data - even if you never run a regular ol’ linear regression again, pretty much everything else in applied stats builds off of it in some way\nAnother thing we’ve been doing is thinking about how to design and add controls to that regression to identify our effect of interest by closing back doors"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#your-lecturer",
    "href": "2023/weeks/week08/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#what-is-this-class",
    "href": "2023/weeks/week08/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be\n\n\n\nSo what is econometrics?\nEconometrics focuses on the study of observational data\nObservational data are measurements of things that the researcher does not control\nGiven that we are working with observational data, we still want to understand the causes of things\nThe world is what it is, we are only here to study it"
  },
  {
    "objectID": "2023/weeks/week07/slides.html",
    "href": "2023/weeks/week07/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#your-lecturer",
    "href": "2023/weeks/week07/slides.html#your-lecturer",
    "title": "Within Variation and Fixed Effects",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#what-is-this-class",
    "href": "2023/weeks/week07/slides.html#what-is-this-class",
    "title": "Within Variation and Fixed Effects",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week06/slides.html",
    "href": "2023/weeks/week06/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week06/slides.html#your-lecturer",
    "href": "2023/weeks/week06/slides.html#your-lecturer",
    "title": "Within Variation and Fixed Effects",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week06/slides.html#what-is-this-class",
    "href": "2023/weeks/week06/slides.html#what-is-this-class",
    "title": "Within Variation and Fixed Effects",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week05/slides.html",
    "href": "2023/weeks/week05/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#your-lecturer",
    "href": "2023/weeks/week05/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#what-is-this-class",
    "href": "2023/weeks/week05/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week04/slides.html",
    "href": "2023/weeks/week04/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week04/slides.html#your-lecturer",
    "href": "2023/weeks/week04/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week04/slides.html#what-is-this-class",
    "href": "2023/weeks/week04/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week03/slides.html",
    "href": "2023/weeks/week03/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "Dr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#your-lecturer",
    "href": "2023/weeks/week03/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#what-is-this-class",
    "href": "2023/weeks/week03/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week02/slides.html",
    "href": "2023/weeks/week02/slides.html",
    "title": "🗓️ Week 2 Fitting Lines",
    "section": "",
    "text": "What is identification error?\nIdentification is how you link the result you see with the conclusion you draw from it\nFor example, say you observe that kids who play video games are more aggressive in everyday life (result), and you conclude from that result that video games make kids more aggressive (conclusion)\nIf seeing that result is actually evidence for that conclusion, then we are properly identified\n\n\n\n\n\nAnother reason why we might see that result, i.e. if the same result could give us a different conclusion, like kids who are aggressive play more video games or people notice aggression more when kids play video games, then\nwe have made an identification error* - our result was not identified!*\nIdentification error is when your result in the data doesn’t actually have a clear theory (“why” or “because”)\nFor example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#your-lecturer",
    "href": "2023/weeks/week02/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\nDr. Melios Research Fellow\n\n\n\n\nPhD in Economics\nBackground: Economics, Political Science, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#what-is-this-class",
    "href": "2023/weeks/week02/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is econometrics\nEconometrics is a field that covers how economists think about statistical analysis\nMany other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week00/slides.html",
    "href": "2023/weeks/week00/slides.html",
    "title": "🗓️ Week 0 Presessionals",
    "section": "",
    "text": "library(directlabels)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(grid)\nlibrary(ggplot2)\ntheme_metro &lt;- function(x) {\n  theme_classic() + \n  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),\n        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),\n        text = element_text(size = 16),\n        axis.title.x = element_text(hjust = 1),\n        axis.title.y = element_text(hjust = 1, angle = 0))\n}\ntheme_metro_regtitle &lt;- function(x) {\n  theme_classic() + \n  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),\n        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),\n        text = element_text(size = 16))\n}"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#your-lecturer",
    "href": "2023/weeks/week00/slides.html#your-lecturer",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\nDr.  George Melios Research Fellow www.georgemelios.com\n\n\n\n\n\nPhD in Economics\nBackground: Political Economy, Behavioural Science\n\nPolitical Economy  Beliefs  Wellbeing"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#what-is-this-class",
    "href": "2023/weeks/week00/slides.html#what-is-this-class",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is this class",
    "text": "What is this class\n\nThis is an applied econometrics class for behavioural science\nEconometrics is a field that covers how economists think about statistical analysis\nWhy do we care about econometrics?\n\nMany other social science fields (even epidemiology) pick up econometric tools as well because of how useful they tend to be"
  },
  {
    "objectID": "2023/weeks/week00/page.html",
    "href": "2023/weeks/week00/page.html",
    "title": "Lecture",
    "section": "",
    "text": "In this pre-sessional week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments, and how we will interact throughout this course.\nIn addition to that we will revise some key concepts in statistics and econometrics."
  },
  {
    "objectID": "2023/weeks/week00/page.html#lecture-slides",
    "href": "2023/weeks/week00/page.html#lecture-slides",
    "title": "Lecture",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week00/page.html#recommended-reading",
    "href": "2023/weeks/week00/page.html#recommended-reading",
    "title": "Lecture",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck syllabus for reading suggestions!"
  },
  {
    "objectID": "2023/weeks/week00/page.html#communication",
    "href": "2023/weeks/week00/page.html#communication",
    "title": "Lecture",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week02/page.html",
    "href": "2023/weeks/week02/page.html",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "",
    "text": "In this week, we will explore how we can use formal procedures to examine if two opposing claims or hypothesis are true or not.\nIn this week, we start looking how we can use data to describe relationships between two variables and distinguish between alternative scenarios."
  },
  {
    "objectID": "2023/weeks/week02/page.html#lecture-slides",
    "href": "2023/weeks/week02/page.html#lecture-slides",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week02/page.html#recommended-reading",
    "href": "2023/weeks/week02/page.html#recommended-reading",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week02/page.html#communication",
    "href": "2023/weeks/week02/page.html#communication",
    "title": "🗓️ Week 02 - Linear Regression / OLS",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week03/page.html",
    "href": "2023/weeks/week03/page.html",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "",
    "text": "In this week, we start looking how we can use data to describe relationships between two variables and distinguish between alternative scenarios."
  },
  {
    "objectID": "2023/weeks/week03/page.html#lecture-slides",
    "href": "2023/weeks/week03/page.html#lecture-slides",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week03/page.html#recommended-reading",
    "href": "2023/weeks/week03/page.html#recommended-reading",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week03/page.html#communication",
    "href": "2023/weeks/week03/page.html#communication",
    "title": "🗓️ Week 03 - Linear Regressions",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week04/page.html",
    "href": "2023/weeks/week04/page.html",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "",
    "text": "In this week, we will build on Linear regressions by expanding our analysis using multiple regressors."
  },
  {
    "objectID": "2023/weeks/week04/page.html#lecture-slides",
    "href": "2023/weeks/week04/page.html#lecture-slides",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week04/page.html#recommended-reading",
    "href": "2023/weeks/week04/page.html#recommended-reading",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week04/page.html#communication",
    "href": "2023/weeks/week04/page.html#communication",
    "title": "🗓 ️Week 04: Linear Regressions with multiple regressors",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week05/page.html",
    "href": "2023/weeks/week05/page.html",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "",
    "text": "In this week, we will focus on how we analyse and interpret binary independent variables."
  },
  {
    "objectID": "2023/weeks/week05/page.html#lecture-slides",
    "href": "2023/weeks/week05/page.html#lecture-slides",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week05/page.html#recommended-reading",
    "href": "2023/weeks/week05/page.html#recommended-reading",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week05/page.html#communication",
    "href": "2023/weeks/week05/page.html#communication",
    "title": "🗓️ Week 05 - Binary Variables and Functional Form",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week06/page.html",
    "href": "2023/weeks/week06/page.html",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "",
    "text": "This is reading week. Look at moodle for recap lectures and recap seminar questions."
  },
  {
    "objectID": "2023/weeks/week06/page.html#lecture-slides",
    "href": "2023/weeks/week06/page.html#lecture-slides",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week06/page.html#recommended-reading",
    "href": "2023/weeks/week06/page.html#recommended-reading",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week06/page.html#communication",
    "href": "2023/weeks/week06/page.html#communication",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week07/page.html",
    "href": "2023/weeks/week07/page.html",
    "title": "🗓️ Week 07 - Revision / Non Linearity",
    "section": "",
    "text": "In this week, we revisited slides from Week 5 and focused on Non-linearity and transformations. on how we analyse within variation and include fixed effects."
  },
  {
    "objectID": "2023/weeks/week07/page.html#lecture-slides",
    "href": "2023/weeks/week07/page.html#lecture-slides",
    "title": "🗓️ Week 07 - Revision / Non Linearity",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week07/page.html#recommended-reading",
    "href": "2023/weeks/week07/page.html#recommended-reading",
    "title": "🗓️ Week 07 - Revision / Non Linearity",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week07/page.html#communication",
    "href": "2023/weeks/week07/page.html#communication",
    "title": "🗓️ Week 07 - Revision / Non Linearity",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week08/page.html",
    "href": "2023/weeks/week08/page.html",
    "title": "🗓️ Week 08 - Panel data / Within Variation",
    "section": "",
    "text": "In this week, we will look into how we analyse within variation and include fixed effects."
  },
  {
    "objectID": "2023/weeks/week08/page.html#lecture-slides",
    "href": "2023/weeks/week08/page.html#lecture-slides",
    "title": "🗓️ Week 08 - Panel data / Within Variation",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week08/page.html#recommended-reading",
    "href": "2023/weeks/week08/page.html#recommended-reading",
    "title": "🗓️ Week 08 - Panel data / Within Variation",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week08/page.html#communication",
    "href": "2023/weeks/week08/page.html#communication",
    "title": "🗓️ Week 08 - Panel data / Within Variation",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week09/page.html",
    "href": "2023/weeks/week09/page.html",
    "title": "🗓️ Week 09 - Regression Discontinuity Design",
    "section": "",
    "text": "In this week, we will look into the quasi-experimental Regression Discontinuity Design."
  },
  {
    "objectID": "2023/weeks/week09/page.html#lecture-slides",
    "href": "2023/weeks/week09/page.html#lecture-slides",
    "title": "🗓️ Week 09 - Regression Discontinuity Design",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week09/page.html#recommended-reading",
    "href": "2023/weeks/week09/page.html#recommended-reading",
    "title": "🗓️ Week 09 - Regression Discontinuity Design",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week09/page.html#communication",
    "href": "2023/weeks/week09/page.html#communication",
    "title": "🗓️ Week 09 - Regression Discontinuity Design",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/syllabus.html",
    "href": "2023/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Quantitative data collection is an integral component of behavioural science: Testing hypotheses requires designing experiments and analysing the data or performing statistical analyses on secondary data. Whereas another core course in this programme - Experimental Design and Methods for the Behavioural Science - covers best practices in designing and conducting experimental research, Quantitative Applications for Behavioural Science introduces the main statistical background of behavioural research from psychology and economics. The course will cover best practices and state of the art statistical tools that are used by psychologists and economists. All the analyses will be demonstrated on example behavioural science research, and students will learn how to identify, interpret, and evaluate appropriate analyses for different research designs, conduct their own data analysis for each of these designs as well as report the analysis for publication in a journal, and recognise and understand contemporary issues in data science analysis in psychology and economics that need to be considered for best research practices. Emphasis will be on teaching students how the same analyses are presented in psychology and economics journals so students can understand how to integrate research from these two fields that constitute behavioural science."
  },
  {
    "objectID": "2023/communication.html",
    "href": "2023/communication.html",
    "title": "Announcements",
    "section": "",
    "text": "📢 Description\n\n\n\n\n\nWe will use this space to make class announcements throughout the semester!\n\n\n\n\nRequirements\nDownload and install STATA or R. Throughout the class, we will rely on STATA for seminars but codes will be provided for R as well."
  },
  {
    "objectID": "2023/weeks/week00/slides.html#welcome-to-econometrics",
    "href": "2023/weeks/week00/slides.html#welcome-to-econometrics",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Welcome to Econometrics",
    "text": "Welcome to Econometrics\n\nThis is a great course (tough one but great)\nWhy?\nGives you the ability to think about and answer questions you are interested in\nAnd to better understand and judge the existing body of literature\n\nThe classic Tik-Tok, Instagram video that starts with “A new study says…”? You can now have an idea of how robust/serious their inferences are."
  },
  {
    "objectID": "2023/weeks/week00/slides.html#welcome-to-econometrics-1",
    "href": "2023/weeks/week00/slides.html#welcome-to-econometrics-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Welcome to Econometrics",
    "text": "Welcome to Econometrics\n\nThis is a great course (tough one but great)\nWhy?\nGives you the ability to think about and answer questions you are interested in\nAnd to better understand and judge existing body of literature\n\nThe classic Tik-Tok, Instagram video that starts with “A new study says…”? You can now have an idea of how robust/serious their inferences are."
  },
  {
    "objectID": "2023/weeks/week00/slides.html#teaching-assistant",
    "href": "2023/weeks/week00/slides.html#teaching-assistant",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\n\n\n\n\nDr. Lazaros Chatzilazarou PhD Candidate Website\n\n\n\n\n\nPhD in Economics (exp 2026)\nBackground: Economics\n\nExperimental Economics"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#so-what-is-econometrics",
    "href": "2023/weeks/week00/slides.html#so-what-is-econometrics",
    "title": "🗓️ Week 0 Presessionals",
    "section": "So what is econometrics?",
    "text": "So what is econometrics?\n\nEconometrics focuses on the study of observational data\nObservational data are measurements of things that the researcher does not control\nGiven that we are working with observational data, we still want to understand the causes of things\nThe world is what it is\nFrom next week onwards though, we will explore ways to study it"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#admin",
    "href": "2023/weeks/week00/slides.html#admin",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Admin",
    "text": "Admin\n\nReview the syllabus (and other materials on Moodle and the PB4A7 website). Reading assignments there\nOur textbook is The Effect, by Huntington-Klein, available online for free.\nAlso these slides\nProgramming in STATA (we will get to this on later)\nAssignment: End of term paper & poster"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-and-prediction-1",
    "href": "2023/weeks/week00/slides.html#causality-and-prediction-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nData scientists are generally concerned with prediction\nThey want to use the data at hand to predict what comes next\nThey generally don’t care why they’re making the prediction they are\nThis can be really handy for certain tasks - “is this picture a cat or a dog?” “what’s the probability that a customer with qualities X, Y, and Z will end up purchasing our good?” “do you have lymphoma?”"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-and-prediction-2",
    "href": "2023/weeks/week00/slides.html#causality-and-prediction-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nEconometricians, on the other hand, care almost exclusively about why\nData scientists want to minimize prediction error\nEconometricians want to minimize inference and identification error\nWe want to correctly understand the underlying data generating process"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#inference-error-and-randomness-1",
    "href": "2023/weeks/week00/slides.html#inference-error-and-randomness-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Inference Error and Randomness",
    "text": "Inference Error and Randomness\n\nSo if we look in a data set and see that \\(X\\) and \\(Y\\) appear to be positively related to each other…\nAre they actually positively related, or is that just a random chance?\nIf they are positively related, maybe we’re understating or overstating how positively related"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#inference-error-and-randomness-2",
    "href": "2023/weeks/week00/slides.html#inference-error-and-randomness-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Inference Error and Randomness",
    "text": "Inference Error and Randomness\n\nIf the true relationship is 0, then in the data we’ll see a positive relationship half the time, and a negative relationship half the time\nEven though the truth is 0!\nHow can we properly make an inference about whether the relationship is 0 or not (or positive, or negative, or how positive or negative), taking into account this randomness?\nThat’s being careful about inference. The statisticians teach us all about this!"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#identification-error-1",
    "href": "2023/weeks/week00/slides.html#identification-error-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Identification Error",
    "text": "Identification Error\n\nBut if there’s another reason why we might see that result, i.e. if the same result could give us a different conclusion, like kids who are aggressive play more video games or people notice aggression more when kids play video games, then we have made an identification error - our result was not identified!\nIdentification error is when your result in the data doesn’t actually have a clear theoretical (“why” or “because”) interpretation\nFor example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts, you have committed an identification error\nOne day in and all we can do is complain, eesh"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-generating-process-1",
    "href": "2023/weeks/week00/slides.html#data-generating-process-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nAnother example is a model of supply and demand\nWe observe prices and quantities in a competitive market\nWhat led to those being the prices and quantities we see?\nThe supply and demand model and its equilibrium, we theorize!"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-generating-process-2",
    "href": "2023/weeks/week00/slides.html#data-generating-process-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data Generating Process",
    "text": "Data Generating Process"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-generating-process-3",
    "href": "2023/weeks/week00/slides.html#data-generating-process-3",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nThe prices that we observe come from that theoretical construct\nWhen we see the prices and quantities moving, according to our theory, it’s because the S and D lines are moving\nBut we can’t see the S and D lines\nOur goal: use the observations we do see to infer what the theoretical model (data generating process) is"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-generating-process-4",
    "href": "2023/weeks/week00/slides.html#data-generating-process-4",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nHarder than it sounds. What inference about S and D can we draw from these observations?"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-1",
    "href": "2023/weeks/week00/slides.html#causality-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nWe can also represent this set of relationships as a graph, with arrows telling you what variables cause each other"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-2",
    "href": "2023/weeks/week00/slides.html#causality-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nWe do this because most of the relationships we’re interested in are causal - we want to know, if we could reach in and manipulate \\(X\\), would \\(Y\\) change as a result, and how much?\nDoes the minimum wage reduce employment?\nDoes quantitative easing avert recessions?\nDoes six-sigma improve business performance?\nDoes getting an MBA make you a better manager?"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-3",
    "href": "2023/weeks/week00/slides.html#causality-3",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nImagine this is the graph we see for minimum wage and employment"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-4",
    "href": "2023/weeks/week00/slides.html#causality-4",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nDoes that mean that the minimum wage harms employment?\nMaybe! But also maybe not\nWhat the graph shows us is a correlation\nAnd correlation is not the same thing as causation"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-5",
    "href": "2023/weeks/week00/slides.html#causality-5",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nA given correlation, like the negative relationship between minimum wage changes and employment changes, can be consistent with a number of different causal relationships\nAs econometricians, we need to figure out which one it is!\nHow can we narrow it down?\nHow many of the diagrams on the next page can be consistent with that negative relationship?"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#eight-possible-relationships",
    "href": "2023/weeks/week00/slides.html#eight-possible-relationships",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Eight Possible Relationships",
    "text": "Eight Possible Relationships"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-6",
    "href": "2023/weeks/week00/slides.html#causality-6",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nThe only ones we can eliminate are d, g, and h\nAll the rest are possible!\nIf f is correct, we see the negative relationship even though minimum wage has nothing to do with causing employment (like the ice cream and shorts example)\nIf a is correct, then even though we know minimum wage causes employment to change, the size or even direction of the relationship will be wrong (why?)"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-7",
    "href": "2023/weeks/week00/slides.html#causality-7",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality",
    "text": "Causality\n\nSo which of them is likely to be correct?\nThat depends on what we think \\(\\varepsilon\\) is\n\\(\\varepsilon\\) is everything that determines \\(Y\\) other than \\(X\\)\nPerhaps the health of the economy, or the policies that area has chosen\nSo we almost certainly have a graph with \\(\\varepsilon \\rightarrow Y\\)\nDo those things also affect the choice to raise the minimum wage? If so we’re in graph a. That downward relationship could be due to a null relationship, or even a positive one (or perhaps a more negative one?)"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-and-prediction-3",
    "href": "2023/weeks/week00/slides.html#causality-and-prediction-3",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nEconometricians, on the other hand, care almost exclusively about why\nData scientists want to minimize prediction error\nEconometricians want to minimize inference and identification error\nWe want to correctly understand the underlying data generating process"
  },
  {
    "objectID": "2023/syllabus.html#description",
    "href": "2023/syllabus.html#description",
    "title": "Syllabus",
    "section": "",
    "text": "Quantitative data collection is an integral component of behavioural science: Testing hypotheses requires designing experiments and analysing the data or performing statistical analyses on secondary data. Whereas another core course in this programme - Experimental Design and Methods for the Behavioural Science - covers best practices in designing and conducting experimental research, Quantitative Applications for Behavioural Science introduces the main statistical background of behavioural research from psychology and economics. The course will cover best practices and state of the art statistical tools that are used by psychologists and economists. All the analyses will be demonstrated on example behavioural science research, and students will learn how to identify, interpret, and evaluate appropriate analyses for different research designs, conduct their own data analysis for each of these designs as well as report the analysis for publication in a journal, and recognise and understand contemporary issues in data science analysis in psychology and economics that need to be considered for best research practices. Emphasis will be on teaching students how the same analyses are presented in psychology and economics journals so students can understand how to integrate research from these two fields that constitute behavioural science."
  },
  {
    "objectID": "2023/weeks/week01/page.html#recommended-reading-1",
    "href": "2023/weeks/week01/page.html#recommended-reading-1",
    "title": "🗓️ Week 01 - Introduction, Context & Key Concepts",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week00/slides.html#what-is-pb4a7",
    "href": "2023/weeks/week00/slides.html#what-is-pb4a7",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is PB4A7",
    "text": "What is PB4A7\n\nNot a maths course!\nNot a pure stats course!\nNot a theoretical econometrics course!\nNot a data science course!"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#what-is-pb4a7-1",
    "href": "2023/weeks/week00/slides.html#what-is-pb4a7-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "What is PB4A7",
    "text": "What is PB4A7"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#textbook",
    "href": "2023/weeks/week00/slides.html#textbook",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Textbook",
    "text": "Textbook"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#why-applications-and-not-econometrics",
    "href": "2023/weeks/week00/slides.html#why-applications-and-not-econometrics",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Why applications and not econometrics?",
    "text": "Why applications and not econometrics?\n\nPB4A7 and PB413 are applied courses. You need to know how to use statistics, and why you’re using them – you will not master the nuts or bolts of statistical theory!\nFor those who want more information\n\nWill provide material on the website of the class\nVisit me during the office hours\nWe will discuss additional courses to audit"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data",
    "href": "2023/weeks/week00/slides.html#data",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-1",
    "href": "2023/weeks/week00/slides.html#data-1",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#data-2",
    "href": "2023/weeks/week00/slides.html#data-2",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#an-example",
    "href": "2023/weeks/week00/slides.html#an-example",
    "title": "🗓️ Week 0 Presessionals",
    "section": "An example",
    "text": "An example"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#housekeeping",
    "href": "2023/weeks/week00/slides.html#housekeeping",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nMoodle (lecture videos and assignments)\nWebsite\nOffice Hours (Book Upfront!!!)"
  },
  {
    "objectID": "2023/Stata.html",
    "href": "2023/Stata.html",
    "title": "Stata",
    "section": "",
    "text": "📢 Description\n\n\n\n\n\nDear All,\nFor anyone that wants extra Intro (from scratch) material for STATA, I can propose the following. These should help you become STATA “experts” in little time. I know this sounds like these ads for “Build a six-pack in 10 steps” but STATA is actually much easier than that. Our seminars and a few of those videos should help you pick it up in no time.\nLSE - DATA SKILLS LAB COURSE\nYou can enrol here and follow at your own pace. It’s supposed to take 10-14 hours to complete in total and should give anyone and superbly good background in stata. You can follow it at your own pace.\nYouTube Channels\nStata Intro - STATA Company itself has a 2hour introduction to STATA that is superb\nEconometrics Academy - Econometrics Academy has a playlist on Introduction to STATA\nData for Development - Data for Development has a nice set of videos as well\nSee you all soon, George"
  },
  {
    "objectID": "2023/R.html",
    "href": "2023/R.html",
    "title": "Stata",
    "section": "",
    "text": "📢 Description\n\n\n\n\n\nDear All,\nHope this finds you well. This channel on the server is for those of you that already have some stata knowledge and want to spend the time you might have the first week(s) to expand more on programming and stats. It’s a mix of LSE courses you can attend and online material that are fantastic to help you progress.\nLSE Modules Courses on dept of methodology - here you can find courses on causal inference, advanced statistics and data science summary of courses here\nCourses from Digital Skills Lab intro to a lot of programming languages (R, STATA, Python)\nCourses from Data Science Institute Courses from B.Sc. in Data Science on Politics\nOnline Resources\n\nHertie Summer School Probably the best and most variable resource for everyone that wants to expand their knowledge on Data Science and statistics is the Hertie Summer School that is free online. Here is a course catalog and they offer courses from intro to Data science to Advanced Machine Learning, NLPs, Scraping data from the web and anything you can imagine. (the one from 2021 has more courses than the 2022 version)\nIsmail Khalil has a fantastic channel with Advanced R programming and Machine Learning on YouTube\n\nWill keep updating this on the relevant Discord Channel and here as much as possible throughout the semester.\nBest wishes, George"
  },
  {
    "objectID": "2023/weeks/week01/page1.html",
    "href": "2023/weeks/week01/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week01/page1.html#seminar-slides",
    "href": "2023/weeks/week01/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week01/page1.html#communication",
    "href": "2023/weeks/week01/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-in-r-1",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-in-r-1",
    "title": "🗓️ Week 8 Regresssion Discontinuity Designs",
    "section": "Regression Discontinuity in R",
    "text": "Regression Discontinuity in R\n\nWe can specify an RDD model by just telling it the dependent variable \\(Y\\), the running variable \\(X\\), and the cutoff \\(c\\).\nWe can also specify how many polynomials to us with p\n(it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)\nIt will also pick a window for us with h\nPlenty of other options\nIncluding a fuzzy option to specify actual treatment outside of the running variable/cutoff combo"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#rdrobust",
    "href": "2023/weeks/week08/slides.html#rdrobust",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nWe’ve gone through all kinds of procedures for doing RDD in R already using regression\nBut often, professional researchers won’t do it that way!\nWe’ll use packages and formulas that do things like “picking a bandwidth (window)” for us in a smart way, or not relying so strongly on linearity\nThe rdrobust package does just that!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-in-stata",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-in-stata",
    "title": "🗓️ Week 8 Regresssion Discontinuity Designs",
    "section": "Regression Discontinuity in STATA",
    "text": "Regression Discontinuity in STATA\n\nWe can specify an RDD model by just telling it the dependent variable \\(Y\\), the running variable \\(X\\), and the cutoff \\(c\\).\nWe can also specify how many polynomials to us with p\n(it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)\nIt will also pick a window for us with h\nPlenty of other options\nIncluding a fuzzy option to specify actual treatment outside of the running variable/cutoff combo"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-in-r",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-in-r",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity in R",
    "text": "Regression Discontinuity in R\n\nWe can specify an RDD model by just telling it the dependent variable \\(Y\\), the running variable \\(X\\), and the cutoff \\(c\\).\nWe can also specify how many polynomials to us with p\n(it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)\nIt will also pick a window for us with h\nPlenty of other options\nIncluding a fuzzy option to specify actual treatment outside of the running variable/cutoff combo"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#check-in",
    "href": "2023/weeks/week08/slides.html#check-in",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Check-in",
    "text": "Check-in\n\nWe’re thinking through ways that we can identify the effect of interest without having to control for everything\nOne way is by focusing on within variation - if all the endogeneity can be controlled for or only varies between-individuals, we can just focus on within variation to identify it\nPro: control for a bunch of stuff\nCon: washes out a lot of variation! Result can be noisier if there’s not much within-variation to work with\nAlso, this requires no endogenous variation over time\nThat might be a tricky assumption! Often there are plenty of back doors that shift over time"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-1",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\nThe basic idea is this:\n\nWe look for a treatment that is assigned on the basis of being above/below a cutoff value of a continuous variable\nFor example, if you get above a certain test score they let you into a “gifted and talented” program\nOr if you are just on one side of a time zone line, your day starts one hour earlier/later\nOr if a candidate gets 50.1% of the vote they’re in, 40.9% and they’re out\nOr if you’re 65 years old you get Medicaid, if you’re 64.99 years old you don’t\n\nWe call these continuous variables “Running variables” because we run along them until we hit the cutoff"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-2",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-2",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nBut wait, hold on, if treatment is driven by running variables, won’t we have a back door going through those very same running variables?? Yes!\nAnd we can’t just control for RunningVar because that’s where all the variation in treatment comes from. Uh oh!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-3",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-3",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nThe key here is realizing that the running variable affects treatment only when you go across the cutoff\nSo really the diagram looks like this!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-4",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-4",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nSo what does this mean?\nIf we can control for the running variable everywhere except the cutoff, then…\nWe will be controlling for the running variable, closing that back door\nBut leaving variation at the cutoff open, allowing for variation in treatment\nWe focus on just the variation around the treatment, narrowing the range of the running variable we use so sharply that it’s basically controlled for. Then the effect of cutoff on treatment is like an experiment!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-5",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-5",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nBasically, the idea is that right around the cutoff, treatment is randomly assigned\nIf you have a test score of 89.9 (not high enough for gifted-and-talented), you’re basically the same as someone who has a test score of 90.0 (just barely high enough)\nSo if we just focus around the cutoff, we close any back doors because it’s basically random which side of the line you’re on\nBut we get variation in treatment!\nThis specifically gives us the effect of treatment for people who are right around the cutoff a.k.a. a “local average treatment effect” (we still won’t know the effect of being put in gifted-and-talented for someone who gets a 30)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-6",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-6",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nA very basic idea of this, before we even get to regression, is to create a binned chart\nAnd see how the bin values jump at the cutoff\nA binned chart chops the Y-axis up into bins\nThen takes the average Y value within that bin. That’s it!\nThen, we look at how those X bins relate to the Y binned values.\nIf it looks like a pretty normal, continuous relationship… then JUMPS UP at the cutoff X-axis value, that tells us that the treatment itself must be doing something!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-discontinuity-7",
    "href": "2023/weeks/week08/slides.html#regression-discontinuity-7",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#concept-checks",
    "href": "2023/weeks/week08/slides.html#concept-checks",
    "title": "🗓️ Week 8  Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nFor each of these variables, would we expect them to have within variation, between variation, or both?\n(Individual = person) How a child’s height changes as they age.\n(Individual = person) In a data set tracking many people over many years, the variation in the number of children a person has in a given year.\n(Individual = city) Overall, Paris, France has more restaurants than Paris, Texas.\n(Individual = genre) The average pop music album sells more copies than the average jazz album\n(Individual = genre) Miles Davis’ Kind of Blue sold very well for a jazz album.\n(Individual = genre) Michael Jackson’s Thriller, a pop album, sold many more copies than Kind of Blue, a jazz album."
  },
  {
    "objectID": "2023/weeks/week08/slides.html#fitting-lines-in-rdd",
    "href": "2023/weeks/week08/slides.html#fitting-lines-in-rdd",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Fitting Lines in RDD",
    "text": "Fitting Lines in RDD\n\nLooking purely just at the cutoff and making no use of the space away from the cutoff throws out a lot of useful information\nWe know that the running variable is related to outcome, so we can probably improve our prediction of what the value on either side of the cutoff should be if we use data away from the cutoff to help with prediction than if we just use data near the cutoff, which is what that animation does\nWe can do this with good ol’ OLS.\nThe bin plot we did can help us pick a functional form for the slope"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#fitting-lines-in-rdd-1",
    "href": "2023/weeks/week08/slides.html#fitting-lines-in-rdd-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Fitting Lines in RDD",
    "text": "Fitting Lines in RDD\n\nTo be clear, producing the line(s) below is our goal. How can we do it?\nThe true model I’ve made is an RDD effect of .7, with a slope of 1 to the left of the cutoff and a slope of 1.5 to the right"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#regression-in-rdd",
    "href": "2023/weeks/week08/slides.html#regression-in-rdd",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Regression in RDD",
    "text": "Regression in RDD\n\nFirst, we need to transform our data\nWe need a “Treated” variable that’s TRUE when treatment is applied - above or below the cutoff\nThen, we are going to want a bunch of things to change at the cutoff. This will be easier if the running variable is centered around the cutoff. So we’ll turn our running variable \\(X\\) into \\(X - cutoff\\) and call that \\(XCentered\\)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#varying-slope",
    "href": "2023/weeks/week08/slides.html#varying-slope",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Varying Slope",
    "text": "Varying Slope\n\nTypically, you will want to let the slope vary to either side\nIn effect, we are fitting an entirely different regression line on each side of the cutoff\nWe can do this by interacting both slope and intercept with \\(treated\\)!\nCoefficient on Treated is how the intercept jumps - that’s our RDD effect. Coefficient on the interaction is how the slope changes\n\n\\[Y = \\beta_0 + \\beta_1Treated + \\beta_2XCentered + \\beta_3Treated\\times XCentered + \\varepsilon\\]\n\n\nOLS estimation, Dep. Var.: Y\nObservations: 1,000 \nStandard-errors: IID \n                        Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)            -0.011133   0.025999 -0.428225 0.66857986    \ntreatedTRUE             0.746688   0.037577 19.870777  &lt; 2.2e-16 ***\nX_centered              0.982500   0.090666 10.836522  &lt; 2.2e-16 ***\ntreatedTRUE:X_centered  0.446961   0.129613  3.448417 0.00058748 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.296605   Adj. R2: 0.847229"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#varying-slope-1",
    "href": "2023/weeks/week08/slides.html#varying-slope-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Varying Slope",
    "text": "Varying Slope\n(as an aside, sometimes the effect of interest is the interaction term - the change in slope! This answers the question “does the effect of \\(X\\) on \\(Y\\) change at the cutoff? This is called a”regression kink” design. We won’t go more into it here, but it is out there!)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#polynomial-terms",
    "href": "2023/weeks/week08/slides.html#polynomial-terms",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Polynomial Terms",
    "text": "Polynomial Terms\n\nWe don’t need to stop at linear slopes!\nJust like we brought in our knowledge of binary and interaction terms to understand the linear slope change, we can bring in polynomials too. Add a square maybe!\nDon’t get too wild with cubes, quartics, etc. - polynomials tend to be at their “weirdest” near the edges, and we don’t want super-weird predictions right at the cutoff. It could give us a mistaken result!\nA square term should be enough"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#polynomial-terms-1",
    "href": "2023/weeks/week08/slides.html#polynomial-terms-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Polynomial Terms",
    "text": "Polynomial Terms\n\nHow do we do this? Interactions again. Take any regression equation…\n\n\\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\\]\n\nAnd just center the \\(X\\) (let’s call it \\(XC\\), add on a set of the same terms multiplied by \\(Treated\\) (don’t forget \\(Treated\\) by itself - that’s \\(Treated\\) times the interaction!)\n\n\\[Y = \\beta_0 + \\beta_1XC + \\beta_2XC^2 + \\beta_3Treated + \\beta_4Treated\\times XC + \\beta_5Treated\\times XC^2 + \\varepsilon\\]\n\nThe coefficient on \\(Treated\\) remains our “jump at the cutoff” - our RDD estimate!\n\n\n\n                              feols(Y ~ X_cent..\nDependent Var.:                                Y\n                                                \nConstant                        -0.0340 (0.0385)\nX_centered                      0.6990. (0.3641)\ntreatedTRUE                   0.7677*** (0.0577)\nX_centered square               -0.5722 (0.7117)\nX_centered x treatedTRUE         0.7509 (0.5359)\ntreatedTRUE x I(X_centered^2)     0.5319 (1.034)\n_____________________________ __________________\nS.E. type                                    IID\nObservations                               1,000\nR2                                       0.84779\nAdj. R2                                  0.84702\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#concept-checks-1",
    "href": "2023/weeks/week08/slides.html#concept-checks-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy does subtracting the within-individual mean of each variable “control for individual”?\nIn a sentence, interpret the slope coefficient in the estimated model \\((Y_{it} - \\bar{Y}_i) = 2 + 3(X_{it} - \\bar{X}_i)\\) where \\(Y\\) is “blood pressure”, \\(X\\) is “stress at work”, and \\(i\\) is an individual person"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#assumptions",
    "href": "2023/weeks/week08/slides.html#assumptions",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Assumptions",
    "text": "Assumptions\n\nWe knew there must be some assumptions lurking around here\nSome are more obvious (we should be using the correct functional form)\nOthers are trickier. What are we assuming about the error term and endogeneity here?\nSpecifically, we are assuming that the only thing jumping at the cutoff is treatment\nSort of like parallel trends, but maybe more believable since we’ve narrowed in so far\nFor example, if having an income below 150% of the poverty line gets you access to food stamps AND to job training, then we can’t really use that cutoff to get the effect of just food stamps\nOr if the proportion of people who are self-employed jumps up just below 150% (based on reported income), that’s a back door too!\nThe only thing different about just above/just below should be treatment"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#graphically",
    "href": "2023/weeks/week08/slides.html#graphically",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Graphically",
    "text": "Graphically"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#windows-1",
    "href": "2023/weeks/week08/slides.html#windows-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Windows",
    "text": "Windows\n\nPay attention to the sample sizes, accuracy (true value .7) and standard errors!\n\n\nm1 &lt;- feols(Y~treated*X_centered, data = df)\nm2 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .25))\nm3 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .1))\nm4 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .05))\nm5 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .01))\netable(m1,m2,m3,m4,m5, keep = 'treatedTRUE')\n\n                                         m1                 m2\nDependent Var.:                           Y                  Y\n                                                              \ntreatedTRUE              0.7467*** (0.0376) 0.7723*** (0.0566)\ntreatedTRUE x X_centered 0.4470*** (0.1296)   0.6671. (0.4022)\n________________________ __________________ __________________\nS.E. type                               IID                IID\nObservations                          1,000                492\nR2                                  0.84769            0.74687\nAdj. R2                             0.84723            0.74531\n\n                                         m3                 m4              m5\nDependent Var.:                           Y                  Y               Y\n                                                                              \ntreatedTRUE              0.7086*** (0.0900) 0.6104*** (0.1467) 0.5585 (0.4269)\ntreatedTRUE x X_centered     -1.307 (1.482)      6.280 (4.789)   41.21 (72.21)\n________________________ __________________ __________________ _______________\nS.E. type                               IID                IID             IID\nObservations                            206                 93              15\nR2                                  0.69322            0.59825         0.48853\nAdj. R2                             0.68867            0.58470         0.34904\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#granular-running-variable-1",
    "href": "2023/weeks/week08/slides.html#granular-running-variable-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Granular Running Variable",
    "text": "Granular Running Variable\n\nNot a whole lot we can do about this\nThere are some fancy RDD estimators that allow for granular running variables\nBut in general, if this is what you’re facing, you might be in trouble\nBefore doing an RDD, think “is it plausible that someone with the highest value just below the cutoff, and someone with the lowest value just above the cutoff are only at different values because of random chance?”"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-1",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nIf there’s manipulation of the running variable around the cutoff, we can often see it in the presence of lumping\nI.e. if there’s a big cluster of observations to one side of the cutoff and a seeming gap missing on the other side"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-2",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-2",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nHere’s an example from the real world in medical research - statistically, p-values should be uniformly distributed\nBut it’s hard to get insignificant results published in some journals. So people might “p-hack” until they find some form of analysis that’s significant, and also we have heavy selection into publication based on \\(p &lt; .05\\). Can’t use that cutoff for an RDD!\n\n\np-value graph from Perneger & Combescure, 2017"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-3",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-3",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nHow can we look for this stuff?\nWe can look graphically by just checking for a jump at the cutoff in number of observations after binning\n\n\ndf_bin_count &lt;- df %&gt;%\n  # Select breaks so that one of hte breakpoints is the cutoff\n  mutate(X_bins = cut(X, breaks = 0:10/10)) %&gt;%\n  group_by(X_bins) %&gt;%\n  count()"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-4",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-4",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nThe first one looks pretty good. We have one that looks not-so-good on the right"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#looking-for-lumping-5",
    "href": "2023/weeks/week08/slides.html#looking-for-lumping-5",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nAnother thing we can do is do a “placebo test”\nCheck if variables other than treatment or outcome vary at the cutoff\nWe can do this by re-running our RDD but just swapping out some other variable for our outcome\nIf we get a significant jump, that’s bad! That tells us that other things are changing at the cutoff which implies some sort of manipulation (or just super lousy luck)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#rdrobust-1",
    "href": "2023/weeks/week08/slides.html#rdrobust-1",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5))\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  501          499\nEff. Number of Obs.             185          170\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.174        0.174\nBW bias (b)                   0.293        0.293\nrho (h/b)                     0.594        0.594\nUnique Obs.                     501          499\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.707     0.085     8.311     0.000     [0.540 , 0.874]     \n        Robust         -         -     6.762     0.000     [0.484 , 0.878]     \n============================================================================="
  },
  {
    "objectID": "2023/weeks/week08/slides.html#rdrobust-2",
    "href": "2023/weeks/week08/slides.html#rdrobust-2",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5, fuzzy = df$treatment))\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  501          499\nEff. Number of Obs.             185          170\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.174        0.174\nBW bias (b)                   0.293        0.293\nrho (h/b)                     0.594        0.594\nUnique Obs.                     501          499\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.707     0.085     8.311     0.000     [0.540 , 0.874]     \n        Robust         -         -     6.762     0.000     [0.484 , 0.878]     \n============================================================================="
  },
  {
    "objectID": "2023/weeks/week08/slides.html#rdrobust-3",
    "href": "2023/weeks/week08/slides.html#rdrobust-3",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nWe can even have it automatically make plots of our RDD! Same syntax\n\n\nrdplot(df$Y, df$X, c = .5)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#thats-it",
    "href": "2023/weeks/week08/slides.html#thats-it",
    "title": "🗓️ Week 8 Regression Discontinuity Designs",
    "section": "That’s it!",
    "text": "That’s it!\n\nThat’s what we have for RDD\nGo explore the regression discontinuity Seminar\nAnd the paper to read!\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-you-should-expect",
    "href": "2023/weeks/week01/slides.html#what-you-should-expect",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What you should expect",
    "text": "What you should expect\n\nConfidence: You will feel like you have a good understanding of design-based causal inference by the end such that it doesn’t feel mysterious or intimidating\nComprehension: You will have learned a lot both conceptually but also in various specifics, particularly with regards to issues around identification and estimation\nCompetency: You will have had some experience working together implementing these methods using code in Stata syntax"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#lectures-plan",
    "href": "2023/weeks/week01/slides.html#lectures-plan",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Lectures plan",
    "text": "Lectures plan\n\nWeek 2 - Linear regressions\nWeek 3 - Hypothesis testing\nWeek 4 – Linear regressions with multiple regressors / Non-linear functions\nWeek 5 – Regressions on Binary variables\nWeek 6 – BREAK!!! (eeeehm Reading Week)\nWeek 7 – Recap & POTENTIAL OUTCOMES\nWeek 8 – Panel regressions\nWeek 9 – Regression Discontinuity Designs\nWeek 10 – Instrumental Variables\nWeek 11 – Difference in Differences"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-is-a-good-research-question",
    "href": "2023/weeks/week01/slides.html#what-is-a-good-research-question",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What is a good research question?",
    "text": "What is a good research question?\n\nComing up with questions is easy.\nBut coming up with good ones, is tricky. Good RQ:"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs",
    "href": "2023/weeks/week01/slides.html#research-designs",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs",
    "text": "Research Designs\n\nQuantitative empirical analysis uses data to explore, test or estimate a relationship."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-is-a-good-research-question-1",
    "href": "2023/weeks/week01/slides.html#what-is-a-good-research-question-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What is a good research question?",
    "text": "What is a good research question?\n\nComing up with questions is easy.\nBut coming up with good ones, is tricky. Good RQ:\n\nA question that can be answered / Researchable:\nHow can you answer a question which is unanswerable?\nWhat versus why? Are you trying to determine what causes Y, or why something causes Y.\n\nImprove our understanding of the world:\n\nDoesn’t have to shake the foundations of science and human knowledge\nWhat if I find an unexpected result?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs-1",
    "href": "2023/weeks/week01/slides.html#research-designs-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs",
    "text": "Research Designs\n\nFrom a broad spectrum of methodologies, we will cover:"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causal-inference",
    "href": "2023/weeks/week01/slides.html#causal-inference",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nContemplating interventios that change behaviour:\n\nHow would littering parks change if we increase the severity of fines?\n\nIs public shaming more effective?\n\nWhat if we increase other types of fines (i.e. driving)?\n\nWill people commit less crimes?\n\n\n\nEach of these policies is asking what happens to some outcome if we make an intervention - keep everything the same but change one factor –"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#a-little-throwback",
    "href": "2023/weeks/week01/slides.html#a-little-throwback",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "A little throwback",
    "text": "A little throwback\n\nOctober 2021’s Nobel Prize in economics went to D. Card, J. Angrist and G. Imbens\nBut it’s arguably as much to Princeton’s mid 1980s Industrial Relations group as it’s ground zero for the credibility revolution\nStarts with Orley Ashenfelter, who had been working on job trainings programs\nKEY individuals: Orley Ashenfelter, David Card (Orley’s student), Josh Angrist (Card and Orley’s student), Alan Krueger (hired by Orley), Bob Lalonde (Card and Orley’s student) and then a generation of students (Levine, Currie, Pischke)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#a-little-throwback-1",
    "href": "2023/weeks/week01/slides.html#a-little-throwback-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "A little throwback",
    "text": "A little throwback\n\nAngrist started working on how randomization in Vietnam drafting can explain later outcomes (we will see this in Week 10)\nMeets Gibens and they both get mentored by Gary Chamberlain\nThey propose the potential outcomes framework\nThis course is about these people, their ideas, subsequent development and how the revolutionised modern empirical research with observational data"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-1",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-2",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment\nAliens come and orbit earth, in superposition.\n\nThey see sick people in hospitals\nWhat do they? think?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-3",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment\nAliens come and orbit earth, in superposition.\n\nThey see sick people in hospitals\nWhat do they? think?\nHospitals kill people. What is the difference? Doctors?\n\nThey kill the doctors, unplug patients from machines, throw open the doors – many more patients inexplicably die\nSounds ridiculous?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#three-types-of-errors",
    "href": "2023/weeks/week01/slides.html#three-types-of-errors",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Three types of errors",
    "text": "Three types of errors\n\nCorrelation =/= causation"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#three-types-of-errors-1",
    "href": "2023/weeks/week01/slides.html#three-types-of-errors-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Three types of errors",
    "text": "Three types of errors\n\nCorrelation =/= causation\nSomething Happening first may not imply causality (rooster)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#three-types-of-errors-2",
    "href": "2023/weeks/week01/slides.html#three-types-of-errors-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Three types of errors",
    "text": "Three types of errors\n\nCorrelation =/= causation\nSomething Happening first may not imply causality (rooster)\nNo correlation does not imply no causation"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-4",
    "href": "2023/weeks/week01/slides.html#introduction-to-counterfactuals-4",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Introduction to counterfactuals",
    "text": "Introduction to counterfactuals\n\nLet’s do a little thought experiment\nAliens come and orbit earth, in superposition.\n\nThey see sick people in hospitals\nWhat do they? think?\nHospitals kill people. What is the difference? Doctors?\n\nThey kill the doctors, unplug patients from machines, throw open the doors – many more patients inexplicably die\nSounds ridiculous?\nAren’t we all aliens in our research?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs-and-causality-1",
    "href": "2023/weeks/week01/slides.html#research-designs-and-causality-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs and Causality",
    "text": "Research Designs and Causality\nExample: If we want to know whether a vaccine works\n\nWe compare people who have gotten vaccinated and those who took a placebo instead"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs-and-causality-2",
    "href": "2023/weeks/week01/slides.html#research-designs-and-causality-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs and Causality",
    "text": "Research Designs and Causality\nExample: If we want to know whether a vaccine works\n\nWe compare people who have gotten vaccinated and those who took a placebo instead\nIn a classic clinical experiment, one applies a ‘treatment’ (0 = placebo, 1 = vaccine) to some set of n ‘subjects’ and observes some ‘outcome’ (Y).\nWe can then estimate:\n\nY = infection(0,1)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#research-designs-and-causality-3",
    "href": "2023/weeks/week01/slides.html#research-designs-and-causality-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Research Designs and Causality",
    "text": "Research Designs and Causality\n\nEach individual i is assigned into one of the treatment options (0 = placebo, 1 = vaccine)\nTherefore, each i as two potential outcomes:\n\nWhat would happen if they got the placebo? Yi (0)\nWhat would happen if they got the vaccine? Yi (1)\n\nDid vaccines prevent infection?\n\nTo answer this we need to know what happened to the individual if they got the vaccine and what happened to the same individual if they got the placebo."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#counterfactuals",
    "href": "2023/weeks/week01/slides.html#counterfactuals",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nWhat actually happened (i.e., the ‘factual’):\n\nI got the vaccine and did not get sick\nTreatment (X) = 1\nObserved outcome = Yi(1)\n\nThe counterfactual: (what would have happened)\n\nIf I did not get the vaccine, would I have fallen sick?\nCounterfactual treatment (X) = 0\nCounterfactual outcome = Yi(0)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#counterfactuals-1",
    "href": "2023/weeks/week01/slides.html#counterfactuals-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nAfter treatment is assigned there is potential for only one outcome to be observed"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#counterfactuals-2",
    "href": "2023/weeks/week01/slides.html#counterfactuals-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nBut ideally we would like to observe two:"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#fundamental-problem-of-causal-inference",
    "href": "2023/weeks/week01/slides.html#fundamental-problem-of-causal-inference",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\n\nOnce we observe one treatment for one individual, we cannot observe a different treatment for the same individual.\nThis is called the “fundamental problem of causal inference.” Each potential outcome is observable, but we can never observe all of them.” (Rubin, 2005, p. 323).\nThen, why are we discussing all these?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#fundamental-problem-of-causal-inference-1",
    "href": "2023/weeks/week01/slides.html#fundamental-problem-of-causal-inference-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\n\nOnce we observe one treatment for one individual, we cannot observe a different treatment for the same individual.\nThis is called the “fundamental problem of causal inference.” Each potential outcome is observable, but we can never observe all of them.” (Rubin, 2005, p. 323).\nThen, why are we discussing all these?\nWe can observe different treatments across different people.\nThis may be a way of solving the fundamental problem, but it introduces a new problem we must consider."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#selection-bias",
    "href": "2023/weeks/week01/slides.html#selection-bias",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Selection Bias",
    "text": "Selection Bias\n\nThis new problem arises because different people are… DIFFERENT!"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#selection-bias-1",
    "href": "2023/weeks/week01/slides.html#selection-bias-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Selection Bias",
    "text": "Selection Bias\n\nDifferences between people following a treatment may be because of the treatment, or they may be because of the differences in the people being treated.\nThis is selection bias.\nLet’s consider some other factors which may matter for selection bias."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#addressing-selection-bias",
    "href": "2023/weeks/week01/slides.html#addressing-selection-bias",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Addressing Selection Bias",
    "text": "Addressing Selection Bias\n\nSelect a large enough random sample and divide them into two groups.\n\nCharacteristics which contribute to selection bias should on average be distributed the same between both groups.\nTherefore, we expect that the treatment and control groups should differ only because of the treatment, and in absence of the treatment, would produce the same results."
  },
  {
    "objectID": "2023/weeks/week01/slides.html#addressing-selection-bias-1",
    "href": "2023/weeks/week01/slides.html#addressing-selection-bias-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Addressing Selection Bias",
    "text": "Addressing Selection Bias\n\nEach group differs within the group…\nBut, on average, the groups themselves are the same, and so are comparable.\nThe effect of treatment on average would then be:\nE(Y | T = 1) – E(Y | T = 0) = Average Treatment Effect (ATE)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#treatment-effect",
    "href": "2023/weeks/week01/slides.html#treatment-effect",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Treatment effect",
    "text": "Treatment effect\n\nThe effect of the intervention then would be:\n\nTreatment effect of intervention = Outvome of Treated - Outcome of Untreated + Selection Bias\nSelection bias is the difference in average outcomes between treatment and control groups due to factors other than the treatment status\nThe true treatment effect, selection bias needs to be eliminated, or shown to be reasonably assumed to be zero.\nTo eliminate selection bias, we need well designed experiments (Matteo’s class) and large enough samples"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#experiments-not-always-the-solution",
    "href": "2023/weeks/week01/slides.html#experiments-not-always-the-solution",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Experiments not always the solution",
    "text": "Experiments not always the solution\n\nTime consuming and expensive\nMay have ethical issues\nRequire large samples for the assumptions to hold\nSuffer from drop-out and non-compliance\nEstimated parameters in an experiment may different from the parameters in which the intervention will actually take place.\nNot very easy to observe ‘real’ behaviours or consequential behaviours because of the setting.\nAn interesting paper on the limits of RCTs from Deaton (Nobel laureate) and Cartwright (2017), if you’re interested!\nWhat do we do then?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causal-inference-1",
    "href": "2023/weeks/week01/slides.html#causal-inference-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causal inference",
    "text": "Causal inference\n\nWe design a strategy (Identification Strategy from now on) that allows us to:\n\nIdentify and isolate the random variation in treatment (i.e. a natural disaster)\nRely on institutional knowledge, theory and data to:\n\nReduce as much as possible Selection Bias\nIdentify outcomes for treated and untreated populations\nEstimate average treatment effects"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#what-follows",
    "href": "2023/weeks/week01/slides.html#what-follows",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "What follows?",
    "text": "What follows?\n\nSeminar today:\n\nIntro to Workflows and Stata\n\nWeek 2: Hypothesis testing\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#background-1",
    "href": "2023/weeks/week01/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Background",
    "text": "Background\n\nThink about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#code-and-software",
    "href": "2023/weeks/week01/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Code and Software",
    "text": "Code and Software\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week01/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week01/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week01/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Anna Karenina Principle",
    "text": "Anna Karenina Principle\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week01/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "What do we learn?",
    "text": "What do we learn?\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#workflow-1",
    "href": "2023/weeks/week01/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week01/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#checklist",
    "href": "2023/weeks/week01/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week01/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week01/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week01/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week01/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week01/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week01/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week01/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week01/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week01/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week01/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week01/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week01/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week01/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#data-generating-process-1",
    "href": "2023/weeks/week01/slides.html#data-generating-process-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nAnother example is a model of supply and demand\nWe observe prices and quantities in a competitive market\nWhat led to those being the prices and quantities we see?\nThe supply and demand model and its equilibrium, we theorize!"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#data-generating-process-2",
    "href": "2023/weeks/week01/slides.html#data-generating-process-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#data-generating-process-3",
    "href": "2023/weeks/week01/slides.html#data-generating-process-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nThe prices that we observe come from that theoretical construct\nWhen we see the prices and quantities moving, according to our theory, it’s because the S and D lines are moving\nBut we can’t see the S and D lines\nOur goal: use the observations we do see to infer what the theoretical model (data generating process) is"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#data-generating-process-4",
    "href": "2023/weeks/week01/slides.html#data-generating-process-4",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nHarder than it sounds. What inference about S and D can we draw from these observations?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-1",
    "href": "2023/weeks/week01/slides.html#causality-1",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nWe can also represent this set of relationships as a graph, with arrows telling you what variables cause each other"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-2",
    "href": "2023/weeks/week01/slides.html#causality-2",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nWe do this because most of the relationships we’re interested in are causal - we want to know, if we could reach in and manipulate \\(X\\), would \\(Y\\) change as a result, and how much?\nDoes the minimum wage reduce employment?\nDoes quantitative easing avert recessions?\nDoes six-sigma improve business performance?\nDoes getting an MBA make you a better manager?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-3",
    "href": "2023/weeks/week01/slides.html#causality-3",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nImagine this is the graph we see for minimum wage and employment"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-4",
    "href": "2023/weeks/week01/slides.html#causality-4",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nDoes that mean that the minimum wage harms employment?\nMaybe! But also maybe not\nWhat the graph shows us is a correlation\nAnd correlation is not the same thing as causation"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-5",
    "href": "2023/weeks/week01/slides.html#causality-5",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nA given correlation, like the negative relationship between minimum wage changes and employment changes, can be consistent with a number of different causal relationships\nAs econometricians, we need to figure out which one it is!\nHow can we narrow it down?\nHow many of the diagrams on the next page can be consistent with that negative relationship?"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#eight-possible-relationships",
    "href": "2023/weeks/week01/slides.html#eight-possible-relationships",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Eight Possible Relationships",
    "text": "Eight Possible Relationships"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-6",
    "href": "2023/weeks/week01/slides.html#causality-6",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nThe only ones we can eliminate are d, g, and h\nAll the rest are possible!\nIf f is correct, we see the negative relationship even though minimum wage has nothing to do with causing employment (like the ice cream and shorts example)\nIf a is correct, then even though we know minimum wage causes employment to change, the size or even direction of the relationship will be wrong (why?)"
  },
  {
    "objectID": "2023/weeks/week01/slides.html#causality-7",
    "href": "2023/weeks/week01/slides.html#causality-7",
    "title": "🗓️ Week 1 Introduction to counterfactuals",
    "section": "Causality",
    "text": "Causality\n\nSo which of them is likely to be correct?\nThat depends on what we think \\(\\varepsilon\\) is\n\\(\\varepsilon\\) is everything that determines \\(Y\\) other than \\(X\\)\nPerhaps the health of the economy, or the policies that area has chosen\nSo we almost certainly have a graph with \\(\\varepsilon \\rightarrow Y\\)\nDo those things also affect the choice to raise the minimum wage? If so we’re in graph a. That downward relationship could be due to a null relationship, or even a positive one (or perhaps a more negative one?)"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html",
    "href": "2023/weeks/week09/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#background-1",
    "href": "2023/weeks/week09/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#code-and-software",
    "href": "2023/weeks/week09/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week09/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week09/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week09/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week09/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#workflow-1",
    "href": "2023/weeks/week09/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week09/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#checklist",
    "href": "2023/weeks/week09/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week09/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week09/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week09/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week09/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week09/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week09/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week09/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week09/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week09/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week09/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week09/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week09/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week09/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html",
    "href": "2023/weeks/week08/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#background-1",
    "href": "2023/weeks/week08/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#code-and-software",
    "href": "2023/weeks/week08/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week08/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week08/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week08/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week08/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#workflow-1",
    "href": "2023/weeks/week08/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week08/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#checklist",
    "href": "2023/weeks/week08/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week08/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week08/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week08/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week08/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week08/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week08/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week08/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week08/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week08/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week08/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week08/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week08/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week08/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html",
    "href": "2023/weeks/week07/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#background-1",
    "href": "2023/weeks/week07/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Background",
    "text": "Background\n\nThink about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#code-and-software",
    "href": "2023/weeks/week07/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Code and Software",
    "text": "Code and Software\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week07/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week07/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week07/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Anna Karenina Principle",
    "text": "Anna Karenina Principle\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week07/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "What do we learn?",
    "text": "What do we learn?\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#workflow-1",
    "href": "2023/weeks/week07/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week07/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#checklist",
    "href": "2023/weeks/week07/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week07/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week07/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week07/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week07/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week07/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week07/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week07/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week07/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week07/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week07/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week07/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week07/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week07/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html",
    "href": "2023/weeks/week06/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#background-1",
    "href": "2023/weeks/week06/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Background",
    "text": "Background\n\nThink about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#code-and-software",
    "href": "2023/weeks/week06/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Code and Software",
    "text": "Code and Software\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week06/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week06/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Making mistakes",
    "text": "Making mistakes\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week06/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Anna Karenina Principle",
    "text": "Anna Karenina Principle\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week06/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "What do we learn?",
    "text": "What do we learn?\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#workflow-1",
    "href": "2023/weeks/week06/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week06/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#checklist",
    "href": "2023/weeks/week06/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week06/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week06/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week06/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week06/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week06/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week06/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week06/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week06/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week06/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week06/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week06/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week06/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week06/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html",
    "href": "2023/weeks/week05/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#background-1",
    "href": "2023/weeks/week05/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#code-and-software",
    "href": "2023/weeks/week05/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week05/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week05/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week05/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week05/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#workflow-1",
    "href": "2023/weeks/week05/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week05/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#checklist",
    "href": "2023/weeks/week05/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week05/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week05/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week05/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week05/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week05/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week05/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week05/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week05/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week05/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week05/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week05/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week05/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week05/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html",
    "href": "2023/weeks/week04/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#background-1",
    "href": "2023/weeks/week04/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#code-and-software",
    "href": "2023/weeks/week04/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week04/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week04/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week04/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week04/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#workflow-1",
    "href": "2023/weeks/week04/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week04/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#checklist",
    "href": "2023/weeks/week04/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week04/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week04/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week04/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week04/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week04/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week04/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week04/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week04/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week04/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week04/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week04/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week04/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week04/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html",
    "href": "2023/weeks/week03/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#background-1",
    "href": "2023/weeks/week03/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#code-and-software",
    "href": "2023/weeks/week03/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week03/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week03/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week03/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week03/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#workflow-1",
    "href": "2023/weeks/week03/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week03/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#checklist",
    "href": "2023/weeks/week03/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week03/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week03/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week03/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week03/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week03/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week03/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week03/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week03/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week03/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week03/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week03/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week03/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html",
    "href": "2023/weeks/week02/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#background-1",
    "href": "2023/weeks/week02/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#code-and-software",
    "href": "2023/weeks/week02/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week02/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week02/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week02/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week02/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#workflow-1",
    "href": "2023/weeks/week02/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week02/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#checklist",
    "href": "2023/weeks/week02/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week02/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week02/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week02/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week02/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week02/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week02/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week02/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week02/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week02/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week02/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week02/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week02/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week02/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week02/page1.html",
    "href": "2023/weeks/week02/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week02/page1.html#seminar-slides",
    "href": "2023/weeks/week02/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week02/page1.html#communication",
    "href": "2023/weeks/week02/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week03/page1.html",
    "href": "2023/weeks/week03/page1.html",
    "title": "💻 Seminar 3 - Linear Regressions / OLS",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week03/page1.html#seminar-slides",
    "href": "2023/weeks/week03/page1.html#seminar-slides",
    "title": "💻 Seminar 3 - Linear Regressions / OLS",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week03/page1.html#communication",
    "href": "2023/weeks/week03/page1.html#communication",
    "title": "💻 Seminar 3 - Linear Regressions / OLS",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week04/page1.html",
    "href": "2023/weeks/week04/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week04/page1.html#seminar-slides",
    "href": "2023/weeks/week04/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week04/page1.html#communication",
    "href": "2023/weeks/week04/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week05/page1.html",
    "href": "2023/weeks/week05/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week05/page1.html#seminar-slides",
    "href": "2023/weeks/week05/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week05/page1.html#communication",
    "href": "2023/weeks/week05/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week06/page1.html",
    "href": "2023/weeks/week06/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week06/page1.html#seminar-slides",
    "href": "2023/weeks/week06/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week06/page1.html#communication",
    "href": "2023/weeks/week06/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week07/page1.html",
    "href": "2023/weeks/week07/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week07/page1.html#seminar-slides",
    "href": "2023/weeks/week07/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week07/page1.html#communication",
    "href": "2023/weeks/week07/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week08/page1.html",
    "href": "2023/weeks/week08/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week08/page1.html#seminar-slides",
    "href": "2023/weeks/week08/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week08/page1.html#communication",
    "href": "2023/weeks/week08/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week09/page1.html",
    "href": "2023/weeks/week09/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week09/page1.html#seminar-slides",
    "href": "2023/weeks/week09/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week09/page1.html#communication",
    "href": "2023/weeks/week09/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#identification-error-1",
    "href": "2023/weeks/week02/slides.html#identification-error-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Identification Error",
    "text": "Identification Error\n\nAnother reason why we might see that result, i.e. if the same result could give us a different conclusion, like kids who are aggressive play more video games or people notice aggression more when kids play video games, then\nwe have made an identification error* - our result was not identified!*\nIdentification error is when your result in the data doesn’t actually have a clear theory (“why” or “because”)\nFor example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-generating-process-2",
    "href": "2023/weeks/week02/slides.html#data-generating-process-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-generating-process-3",
    "href": "2023/weeks/week02/slides.html#data-generating-process-3",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nThe prices that we observe come from that theoretical construct\nWhen we see the prices and quantities moving, according to our theory, it is because the S and D lines are moving\nBut we cannot see the S and D lines\nOur goal: use the observations we do see to infer what the theoretical model (data generating process) is"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-generating-process-4",
    "href": "2023/weeks/week02/slides.html#data-generating-process-4",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nHarder than it sounds. What inference about S and D can we draw from these observations?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality",
    "href": "2023/weeks/week02/slides.html#causality",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Causality",
    "text": "Causality\n\nA data generating process can be described by a series of equations that describe where the data comes from. For example:\n\n\\[ X = \\gamma_0 + \\gamma_1\\varepsilon + \\nu \\]\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nThis says ” \\(X\\) is caused by \\(\\varepsilon\\) and \\(\\nu\\), and \\(Y\\) is caused by \\(X\\) and \\(\\varepsilon\\)”\nThe truth is that an increase in \\(X\\) causally increases \\(Y\\) by \\(\\beta_1\\)\nThe goal of econometrics is to be able to estimate what \\(\\beta_1\\) is accurately"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-1",
    "href": "2023/weeks/week02/slides.html#causality-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Causality",
    "text": "Causality\n\nWe can also represent this set of relationships as a graph, with arrows telling you what variables cause each other"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-2",
    "href": "2023/weeks/week02/slides.html#causality-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Causality",
    "text": "Causality\n\nWe do this because most of the relationships we’re interested in are causal - we want to know, if we could reach in and manipulate \\(X\\), would \\(Y\\) change as a result, and how much?\nDoes the minimum wage reduce employment?\nDoes quantitative easing avert recessions?\nDoes six-sigma improve business performance?\nDoes getting an MBA make you a better manager?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-3",
    "href": "2023/weeks/week02/slides.html#causality-3",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Causality",
    "text": "Causality\n\nImagine this is the graph we see for minimum wage and employment"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-5",
    "href": "2023/weeks/week02/slides.html#causality-5",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Causality",
    "text": "Causality\n\nA given correlation, like the negative relationship between minimum wage changes and employment changes, can be consistent with a number of different causal relationships\nAs econometricians, we need to figure out which one it is!\nHow can we narrow it down?\nHow many of the diagrams on the next page can be consistent with that negative relationship?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#eight-possible-relationships",
    "href": "2023/weeks/week02/slides.html#eight-possible-relationships",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Eight Possible Relationships",
    "text": "Eight Possible Relationships"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-6",
    "href": "2023/weeks/week02/slides.html#causality-6",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Causality",
    "text": "Causality\n\nThe only ones we can eliminate are d, g, and h\nAll the rest are possible!\nIf f is correct, we see the negative relationship even though minimum wage has nothing to do with causing employment (like the ice cream and shorts example)\nIf a is correct, then even though we know minimum wage causes employment to change, the size or even direction of the relationship will be wrong (why?)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#causality-7",
    "href": "2023/weeks/week02/slides.html#causality-7",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Causality",
    "text": "Causality\n\nSo which of them is likely to be correct?\nThat depends on what we think \\(\\varepsilon\\) is\n\\(\\varepsilon\\) is everything that determines \\(Y\\) other than \\(X\\)\nPerhaps the health of the economy, or the policies that area has chosen\nSo we almost certainly have a graph with \\(\\varepsilon \\rightarrow Y\\)\nDo those things also affect the choice to raise the minimum wage? If so we’re in graph a. That downward relationship could be due to a null relationship, or even a positive one (or perhaps a more negative one?)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#endogeneity",
    "href": "2023/weeks/week02/slides.html#endogeneity",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Endogeneity",
    "text": "Endogeneity\n\nSo “correlation isn’t causation” isn’t quite complete\nIt’s more “only certain correlations are causal”\nMany correlations are beset by these problems like endogeneity, i.e. the presence of another variable like \\(\\varepsilon\\) related to both \\(X\\) and \\(Y\\), giving the effect a “back door”\nSo the correlation reflects both the causal effect and also the influence of \\(\\varepsilon\\)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#random-experiments-1",
    "href": "2023/weeks/week02/slides.html#random-experiments-1",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Random Experiments",
    "text": "Random Experiments\n\nFor this reason, random experiments are generally considered the “gold standard”\nAlthough they have their own problems, of course (your experimental sample might not represent the population well, there are plenty of statistical mistakes to make, people may act differently knowing they’re in an experiment, etc. etc.)\nBut regardless, we’re looking here at questions for which we can’t run an experiment, becuase it’s impossible or infeasible or immoral\nSo one way we can think about solving this endogeneity problem with econometrics is to use our observational data in such a way that it behaves as though there were an experiment being run\nPlenty of ways to do this we’ll go over in this course!"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#concept-check",
    "href": "2023/weeks/week02/slides.html#concept-check",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat does it mean to say that \\(X\\) has a causal effect on \\(Y\\)?\nWhy might the relationship between \\(X\\) and \\(Y\\) in data not be the same as the causal effect?\nWhat is an example of observational data?\nConsider the question of “Does getting an MBA make you a better manager?” What are \\(X\\) and \\(Y\\) here? What would be in the error term \\(\\varepsilon\\)? Are we likely to have an endogeneity problem here?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#spurious-correlations",
    "href": "2023/weeks/week02/slides.html#spurious-correlations",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Spurious Correlations",
    "text": "Spurious Correlations\n\nLet’s visit this site all about “spurious” correlations (i.e. correlations that almost certainly do not reveal a true effect of one variable on the other): https://tylervigen.com/spurious-correlations\nTake a look at how easy it is to find variables that are related statistically, even though clearly neither causes the other\nDo you think this correlation is an example of inferential error (just random chance) or identification error (truly related, but not because one causes the other)? Why?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-generating-process-1",
    "href": "2023/weeks/week02/slides.html#data-generating-process-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nAnother example is a model of supply and demand\nWe observe prices and quantities in a competitive market\nWhat led to those being the prices and quantities we see?\nThe supply and demand model and its equilibrium, we theorize!"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#x-and-y",
    "href": "2023/weeks/week02/slides.html#x-and-y",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "\\(X\\) and \\(Y\\)",
    "text": "\\(X\\) and \\(Y\\)\n\nI have an \\(X\\) value of 2.5 and want to predict what \\(Y\\) will be. What can I do?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#x-and-y-1",
    "href": "2023/weeks/week02/slides.html#x-and-y-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "\\(X\\) and \\(Y\\)",
    "text": "\\(X\\) and \\(Y\\)\n\nI can’t just say “just predict whatever values of \\(Y\\) we see for \\(X = 2.5\\), because there are multiple of those!\nPlus, what if we want to predict for a value we DON’T have any actual observations of, like \\(X = 4.3\\)?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#data-is-granular",
    "href": "2023/weeks/week02/slides.html#data-is-granular",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Data is Granular",
    "text": "Data is Granular\n\nIf I try to fit every point, I’ll get a mess that won’t really tell me the relationship between \\(X\\) and \\(Y\\)\nSo, we simplify the relationship into a shape: a line! The line smooths out those three points around 2.5 and fills in that gap around 4.3"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#isnt-this-worse",
    "href": "2023/weeks/week02/slides.html#isnt-this-worse",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Isn’t This Worse?",
    "text": "Isn’t This Worse?\n\nBy adding a line, we are necessarily simplifying our presentation of the data. We’re tossing out information!\nOur prediction of the data we have will be less accurate than if we just make predictions point-by-point\nHowever, we’ll do a better job predicting other data (avoiding “overfitting”)\nAnd, since a shape is something we can interpret, as opposed to a long list of predictions, which we can’t really, the line will do a better job of telling us about the true underlying relationship"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#the-line-does-a-few-things",
    "href": "2023/weeks/week02/slides.html#the-line-does-a-few-things",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "The Line Does a Few Things:",
    "text": "The Line Does a Few Things:\n\nWe can get a prediction of \\(Y\\) for a given value of \\(X\\) (If we follow \\(X = 2.5\\) up to our line we get \\(Y = 7.6\\))\nWe see the relationship: the line slopes up, telling us that “more \\(X\\) means more \\(Y\\) too!”"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#lines",
    "href": "2023/weeks/week02/slides.html#lines",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nThat line we get is the fit of our model\nA model “fit” means we’ve taken a shape (our line) and picked the one that best fits our data\nAll forms of regression do this\nOrdinary least squares specifically uses a straight line as its shape\nThe resulting line we get can also be written out as an actual line, i.e.\n\n\\[ Y = intercept + slope*X \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#lines-1",
    "href": "2023/weeks/week02/slides.html#lines-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nWe can use that line as… a line!\nIf we plug in a value of \\(X\\), we get a prediction for \\(Y\\)\nBecause these \\(Y\\) values are predictions, we’ll give them a hat \\(\\hat{Y}\\)\n\n\\[ Y = 3 + 4*X \\]\n\\[ \\hat{Y} = 3 + 4*(3.2) \\]\n\\[ \\hat{Y} = 15.8 \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#lines-2",
    "href": "2023/weeks/week02/slides.html#lines-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nWe can also use it to explain the relationship\nWhatever the intercept is, that’s what we predict for \\(Y\\) when \\(X = 0\\)\n\n\\[ Y = 3 + 4*X \\]\n\\[ \\hat{Y} = 3 + 4*0 \\]\n\\[ \\hat{Y} = 3 \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#lines-3",
    "href": "2023/weeks/week02/slides.html#lines-3",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Lines",
    "text": "Lines\n\nAnd as \\(X\\) increases, we know how much we expect \\(Y\\) to increase because of the slope\n\n\\[ Y = 3 + 4*X \\]\n\\[ \\hat{Y} = 3 + 4*3 = 15 \\]\n\\[ \\hat{Y} = 3 + 4*4 = 19 \\]\n\nWhen \\(X\\) increases by \\(1\\), \\(Y\\) increases by the slope (which is \\(4\\) here)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nRegression fits a shape to the data\nOrdinary least squares specifically fits a straight line to the data\nThe straight line is described using an \\(intercept\\) and a \\(slope\\)\nWhen we plug an \\(X\\) into the line, we get a prediction for \\(Y\\), which we call \\(\\hat{Y}\\)\nWhen \\(X = 0\\), we predict \\(\\hat{Y} = intercept\\)\nWhen \\(X\\) increases by \\(1\\), our prediction of \\(Y\\) increases by the \\(slope\\)\nIf \\(slope &gt; 0\\), \\(X\\) and \\(Y\\) are positively related/correlated\nIf \\(slope &lt; 0\\), \\(X\\) and \\(Y\\) are negatively related/correlated"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#concept-checks",
    "href": "2023/weeks/week02/slides.html#concept-checks",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nHow does producing a line let us use \\(X\\) to predict \\(Y\\)?\nIf our line is \\(Y = 5 - 2*X\\), explain what the \\(-2\\) means in a sentence\nNot all of the points are exactly on the line, meaning some of our predictions will be wrong! Should we be concerned? Why or why not?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#how",
    "href": "2023/weeks/week02/slides.html#how",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "How?",
    "text": "How?\n\nWe know that regression fits a line\nBut how does it do that exactly?\nIt picks the line that produces the smallest squares\nThus, “ordinary least squares”"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#predictions-and-residuals",
    "href": "2023/weeks/week02/slides.html#predictions-and-residuals",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Predictions and Residuals",
    "text": "Predictions and Residuals\n\nWhenever you make a prediction of any kind, you rarely get it exactly right\nThe difference between your prediction and the actual data is the residual\n\n\\[ Y = 3 + 4*X \\]\nIf we have a data point where \\(X = 4\\) and \\(Y = 18\\), then\n\\[ \\hat{Y} = 3 + 4*4 = 19 \\]\nThen the residual is \\(Y - \\hat{Y} = 18 - 19 = -1\\)."
  },
  {
    "objectID": "2023/weeks/week02/slides.html#predictions-and-residuals-1",
    "href": "2023/weeks/week02/slides.html#predictions-and-residuals-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Predictions and Residuals",
    "text": "Predictions and Residuals\nSo really, our relationship doesn’t look like this…\n\\[ Y = intercept + slope*X \\]\nInstead, it’s…\n\\[ Y = intercept + slope*X + residual \\]\nWe still use \\(intercept + slope*X\\) to predict \\(Y\\) though, so this is also\n\\[ Y = \\hat{Y} + residual \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-1",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-1",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nAs you’d guess, a good prediction should make the residuals as small as possible\nWe want to pick a line to do that\nAnd in particular, we’re going to square those residuals, so the really-big residuals count even more. We really don’t want to have points that are super far away from the line!\nThen, we pick a line to minimize those squared residuals (“least squares”)"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-2",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-2",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nStart with our data"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-3",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-3",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nLet’s just pick a line at random, not necessarily from OLS"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-4",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-4",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nThe vertical distance from point to line is the residual"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-5",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-5",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nNow square those residuals"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-6",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-6",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nCan we get the total area in the squares smaller with a different line?"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-7",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-7",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nOrdinary Least Squares, I can promise you, gets it the smallest"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-8",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-8",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nHow does it figure out which line makes the smallest squares?\nThere’s a mathematical formula for that!\nFirst, instead of thinking of \\(intercept\\) and \\(slope\\), we reframe the line as having parameters we can pick\n\n\\[ Y = intercept + slope*X + residual \\]\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]"
  },
  {
    "objectID": "2023/weeks/week02/slides.html#terminology-sidenote",
    "href": "2023/weeks/week02/slides.html#terminology-sidenote",
    "title": "🗓️ Week 2 Fitting Lines ",
    "section": "Terminology Sidenote",
    "text": "Terminology Sidenote\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nIn metrics, Greek letters represent “the truth” - in the true process by which the data is generated, a one-unit increase in \\(X\\) is related to a \\(\\beta_1\\) increase in \\(Y\\)\nWhen we put a hat on anything, that is our prediction or estimation of that true thing. \\(\\hat{Y}\\) is our prediction of \\(Y\\), and \\(\\hat{\\beta_1}\\) is our estimate of what we think the true \\(\\beta_1\\) is\nNote “residual” =/= \\(\\varepsilon\\) - residuals are what’s actually left over from our prediction with real data, but the error \\(\\varepsilon\\) is the true difference between our line and \\(Y\\)."
  },
  {
    "objectID": "2023/weeks/week02/slides.html#ordinary-least-squares-9",
    "href": "2023/weeks/week02/slides.html#ordinary-least-squares-9",
    "title": "🗓️ Week 2 Hypothesis Testing",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nNow that we have our line in parametric terms, we can pick our estimates of \\(\\beta_0\\) and \\(\\beta_1\\) in order to make the squared residuals as small as possible\nPick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize:\n\n\\[ \\sum_i (residual_i^2) \\]\n\\[ \\sum_i ((Y_i - \\hat{Y})^2) \\]\n\\[ \\sum_i ((Y_i - \\hat{\\beta_0} - \\hat{\\beta_1}X_i)^2) \\]\nWhere the \\(_i\\) refers to a particular observation. \\(\\sum_i\\) means “sum this up over all the observations”\n(Conveniently, you can pick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize that expression with basic calculus)"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#truth-and-reality-1",
    "href": "2023/weeks/week03/slides.html#truth-and-reality-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Truth and Reality",
    "text": "Truth and Reality\n\nToday we’ll be covering hypothesis testing, which is one approach to using reality to get closer to the truth\nIt works by subtraction\nWe test whether certain versions of the truth are likely or unlikely\nAnd if we find that they’re unlikely, we can reject that version of the truth, narrowing down what the actual possibilities are and getting closer and closer to the actual truth"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#truth-and-reality-2",
    "href": "2023/weeks/week03/slides.html#truth-and-reality-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Truth and Reality",
    "text": "Truth and Reality\n\nWhen we’re talking about the truth here, we’re referring to the true data generating process (DGP)\nFor example, if this is the true DGP:\n\n\\[ Wage_i = \\beta_0 + \\beta_1AdultHeight_i + \\varepsilon_i \\]\nwhere \\(cor(AdultHeight_i,\\varepsilon_i) = 0\\), then…\n\nPerson \\(i\\)’s wage is truly determined by a linear function of your height, plus an unrelated error term \\(\\varepsilon_i\\)\nWhy might someone have a high wage? Either they’re tall, or they have a high error term, or both. No other way!"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#truth-and-reality-3",
    "href": "2023/weeks/week03/slides.html#truth-and-reality-3",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Truth and Reality",
    "text": "Truth and Reality\n\nOur ability to estimate that true model depends on our ability to avoid inference and identification error\nIf we assume that’s the true DGP, there’s no endogeneity, and the relationship between \\(Wage\\) and \\(AdultHeight\\) is a straight line, so regular ’ol OLS of \\(Wage\\) on \\(AdultHeight\\) will not give us identification error\nBut we also need to be careful about inference error\nWhen we run that regression, what does our \\(\\hat{\\beta}_1\\) say about the true value \\(\\beta_1\\)?"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#sampling-variation-1",
    "href": "2023/weeks/week03/slides.html#sampling-variation-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Sampling Variation",
    "text": "Sampling Variation\n\nNotice that there is plenty of variation around the true value of \\(2\\)\nNow let’s imagine we don’t know that and are trying to answer the question “is the truth \\(\\beta_1 = 2\\)?”\nWe don’t have the full sampling distribution, we just have a single estimate:\n\n\n\n(Intercept)           X \n   2.798554    2.215880 \n\n\n\nAll we see is \\(\\hat{\\beta}_1 =\\) 2.22. So… is \\(\\beta_1 = 2\\)?"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#null-distribution",
    "href": "2023/weeks/week03/slides.html#null-distribution",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nThe “null distribution” is what the sampling distribution of the estimator would be if our null distribution were true\nWe can see that in the sampling distribution we have!\n\\(\\beta_1 = 2\\) is true, and here’s what the sampling distribution looks like! (although it would be smoother with more samples)"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#null-distribution-1",
    "href": "2023/weeks/week03/slides.html#null-distribution-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nSo the key question that a hypothesis test asks is: given this null distribution, how unlikely is it that we get the result we get?\nIf it’s super unlikely that the null is true and we get our result, well…\nWe definitely got our result…\nSo the null must be the part that’s wrong!\nThat’s when we reject the null - we find that the sampling distribution under the null hardly ever produces a result like ours, so that’s probably the wrong sampling distribution and thus the wrong null!"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#null-distribution-2",
    "href": "2023/weeks/week03/slides.html#null-distribution-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nHow does this work out with our estimate of 2.22?\nLet’s stick it on the graph"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#hypothesis-test",
    "href": "2023/weeks/week03/slides.html#hypothesis-test",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\nOur test comes down to: how weird would it be to get a result this far from the “truth” or farther?\nWe can figure this out by shading in the parts of the null distribution this far from the null truth or farther\nSo we shade 2.22 and above, and also 2 - abs(2.22 - 2) = 1.78 and below."
  },
  {
    "objectID": "2023/weeks/week03/slides.html#hypothesis-test-1",
    "href": "2023/weeks/week03/slides.html#hypothesis-test-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\nBased off of this sampling distribution, there’s a 31% + 26% = 57% chance of getting something as weird as we got or weirder (or for a one-tailed test, a 26% chance of getting something that high or higher) if the null of \\(\\beta_1 = 2\\) is true\nThat’s not too unlikely! So, we would fail to reject the null of \\(\\beta_1 = 2\\)\nThis doesn’t mean that we conclude that \\(\\beta_1 = 2\\) is true, it just means we can’t rule it out"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#the-null-distribution",
    "href": "2023/weeks/week03/slides.html#the-null-distribution",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\nOf course, we generated this null distribution by just randomly creating a few random samples\nWe also happen to know that if we had infinite samples, the sampling distribution of OLS would be a normal distribution with the mean at the true value and the standard deviation determined by \\(var(X)\\), the variance of the residual, and the sample size. The real null distribution looks like this:"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#the-null-distribution-1",
    "href": "2023/weeks/week03/slides.html#the-null-distribution-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\nSo with the estimate we made from the sample we got (2.22), we can’t reject a null \\(\\beta_1 = 2\\)\nWhich is good!! That’s the truth. We don’t want to reject it!\nHow about other nulls? Can we reject those?\nCan we reject a null that \\(\\beta_1 = 0\\)?\n(by default, most null hypotheses are that the parameter is 0)\nLet’s follow the same steps!"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#the-null-distribution-2",
    "href": "2023/weeks/week03/slides.html#the-null-distribution-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\nNow that’s unlikely. We can reject that the true value is 0."
  },
  {
    "objectID": "2023/weeks/week03/slides.html#p-values-1",
    "href": "2023/weeks/week03/slides.html#p-values-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "p-values",
    "text": "p-values\n\nThe lower the p-value, the less likely it is that we got our result AND the null is true\nAnd since we definitely got our result, a really low p-value says we should reject the null\nHow low does it need to be to reject the null?\nWell…"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#p-values-and-.05",
    "href": "2023/weeks/week03/slides.html#p-values-and-.05",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "p-values and .05",
    "text": "p-values and .05\n\nIt’s common practice to decide on a confidence level and a corresponding \\(\\alpha\\), most commonly a 95% confidence level \\(\\rightarrow \\alpha = .05\\), and reject the null if the p-value is lower than \\(\\alpha\\)\nWhy 95%? Completely arbitrary. Someone picked it out of thin air a hundred years ago as a just-for-instance and we still use it 🤷\nHaving a hard-and-fast threshold like this is not a great idea ( \\(p=.04\\) is rejection, but \\(p=.06\\) is not?), but it’s a very hard habit to break\nKey takeaway: get familiar with the concept of rejecting the null when the p-value is lower than .05, because you’ll see it\nBUT don’t get too hung up on black-and-white rejection in general"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#power-1",
    "href": "2023/weeks/week03/slides.html#power-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Power",
    "text": "Power\n\nBecause the smaller we make our confidence level, the less likely we are to reject the null in general\nWhich means we’ll also fail to reject it if it’s actually false (a “false negative”)\nWe want a low false-positive rate, but also a low false-negative rate\nThe false negative rate is called “power.” If we will reject the null 80% of the time when it’s actually false, we have 80% power\nAs \\(\\alpha\\) shrinks, false-positive rates decline, but false-negative rates increase\nStrike a balance!\n\n(minor sidenote: “false positive” and “false negative” are sometimes referred to as “Type I Error” and “Type II Error” - these are not great terms because they are hard to remember! If you encounter them, just remember that in “The boy who cried wolf” the townspeople think there’s a wolf when there’s not, then think there’s not a wolf when there is, committing Type I and II error in that order)"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#what-we-just-did",
    "href": "2023/weeks/week03/slides.html#what-we-just-did",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "What we just did…",
    "text": "What we just did…\n\nThis is how we thought of it - we picked a null, figured out the null distribution (the sampling distribution of the estimator assuming the null was true), and checked if our estimate was unlikely enough that we could reject the null"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#instead",
    "href": "2023/weeks/week03/slides.html#instead",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Instead…",
    "text": "Instead…\n\nBy thinking about standard errors, we instead center the sampling distribution around our estimate. If the null is far away, we reject that null (result should be the same)!"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#confidence-intervals",
    "href": "2023/weeks/week03/slides.html#confidence-intervals",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nSo what we’re thinking now is not “is our estimate close to the null?” but rather “is that null close to our estimate?”\nWe can go one step further and ask “which nulls are close to our estimate?” and figure out which nulls we can think about rejecting from there\nA confidence interval shows the range of nulls that would not be rejected by our estimate\nEverything outside that range can be rejected"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#confidence-intervals-1",
    "href": "2023/weeks/week03/slides.html#confidence-intervals-1",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThis takes the form of\n\n\\[ \\hat{\\beta}_1 \\pm Z(s.e.) \\]\n\nWhere \\(Z\\) is some value from our distribution that gives us the \\(1-\\alpha\\) percentile (for a 95% confidence interval with a normal sampling distribution, \\(Z = 1.96\\))\nand \\(s.e.\\) is the standard error of \\(\\hat{\\beta}_1\\)"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#confidence-intervals-2",
    "href": "2023/weeks/week03/slides.html#confidence-intervals-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThinking back to our estimate:\n\n\n\n\n\n\n\nModel 1\n\n\n(Intercept)\n2.80 \n\n\n\n(0.21)\n\n\nX\n2.22 \n\n\n\n(0.37)\n\n\nN\n100    \n\n\n\n\n\n\n\n\nOur estimate of the coefficient is 2.22, and our estimate of the standard error is 0.37\nSo for a 95% confidence interval, assuming normality, we get 2.22 \\(\\pm\\) 1.96* 0.37, or [1.49,2.94]"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#confidence-intervals-3",
    "href": "2023/weeks/week03/slides.html#confidence-intervals-3",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nWe can see this graphically as well - we should only reject nulls in those 5% tails for a 95% confidence interval"
  },
  {
    "objectID": "2023/weeks/week03/slides.html#concept-checks-2",
    "href": "2023/weeks/week03/slides.html#concept-checks-2",
    "title": "🗓️ Week 3 Hypothesis Testing",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhat feature of the sampling distribution of the \\(\\beta\\) does the standard error describe?\nWhy do we get the same reject/don’t reject result if we center the sampling distribution around the null as around our estimate?\nWe perform an estimate of \\(\\hat{\\beta}_1\\) and get a 95% confidence interval of [-1.3, 2.1]. Describe what this means in a sentence.\nIn the above confidence interval, can we reject the null of \\(\\beta_1 = 0\\)?\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#ols-1",
    "href": "2023/weeks/week03/slides_sem.html#ols-1",
    "title": "💻 Seminar 3 Hypothesis testing & OLS",
    "section": "OLS",
    "text": "OLS\n\nDuring this seminar:\n\nWe will go through Seminar 2 & 3 exercises.\nPlease conduct your analyses on your do-files and keep a log-file.\nSee how we can run OLS on STATA and test hypotheses"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#tasks",
    "href": "2023/weeks/week03/slides_sem.html#tasks",
    "title": "💻 Seminar 3 Hypothesis testing & OLS",
    "section": "Tasks",
    "text": "Tasks\n\nRegress education on yearsexp. Interpret the coefficient. What does the result tell us?\nCreate a set of dummy variables to capture the different levels of education on yearsexp."
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#tasks-1",
    "href": "2023/weeks/week03/slides_sem.html#tasks-1",
    "title": "💻 Seminar 3 Hypothesis testing & OLS",
    "section": "Tasks",
    "text": "Tasks\n\nTest the null hypothesis that there is no effect of earning a college degree on yearsexp using an OLS regression model. State the alternate hypothesis.\n\nInterpret the estimated coefficient.\nBased on the results, do you reject the null hypothesis? Explain using t-statistic, p-values and confidence intervals.\nWhat is the prediction for an applicant who has not earned a a college degree?\nWhat is the prediction for an applicant who has earned a a college degree?"
  },
  {
    "objectID": "2023/weeks/week03/slides_sem.html#tasks-2",
    "href": "2023/weeks/week03/slides_sem.html#tasks-2",
    "title": "💻 Seminar 3 Hypothesis testing & OLS",
    "section": "Tasks",
    "text": "Tasks\n\nTest the null hypothesis that there is no effect of earning a college degree on yearsexp using an OLS regression model using robust Standard Errors. Do the coefficient values change?\nTest the null hypothesis that there is no effect of earning a college degree on yearsexp using ANOVA.\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#the-right-hand-side-1",
    "href": "2023/weeks/week05/slides.html#the-right-hand-side-1",
    "title": "Binary Variables and Functional Form",
    "section": "The Right Hand Side",
    "text": "The Right Hand Side\nWe will look at three features of the right-hand side\n\nWhat if the variable is categorical or binary? (binary variables)\nWhat if the variable has a nonlinear effect on \\(Y\\) (polynomials and logarithms)\nWhat if the effect of one variable depends on the value of another variable? (interaction terms)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means",
    "href": "2023/weeks/week05/slides.html#comparison-of-means",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nWhen a binary variable is an independent variable, what we are often interested in doing is comparing means\nIs mean income higher inside the US or outside?\nIs mean height higher for kids who got a nutrition supplement or those who didn’t?\nIs mean GDP growth higher with or without a floating exchange rate?"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-1",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-1",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nLet’s compare log earnings in 1993 between married people 30 or older vs. never-married people 30 or older\nSeems to be a slight favor to the married men\n\n\ndata(PSID, package = 'Ecdat')\nPSID &lt;- PSID %&gt;%\n  filter(age &gt;= 30, married %in% c('married','never married'), earnings &gt; 0) %&gt;%\n  mutate(married  = married == 'married')\nPSID %&gt;%\n  group_by(married) %&gt;%\n  summarize(log_earnings = mean(log(earnings)))\n\n# A tibble: 2 × 2\n  married log_earnings\n  &lt;lgl&gt;          &lt;dbl&gt;\n1 FALSE           9.26\n2 TRUE            9.47"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-2",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-2",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-3",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-3",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nThe difference between the means follows a t-distribution under the null that they’re identical\nSo of course we can do a hypothesis test of whether they’re different. But why bother trotting out a specific test when we can just do a regression?\n(In fact, a lot of specific tests can be replaced with basic regression, see this explainer)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-4",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-4",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nNotice:\n\nThe intercept gives the mean for the non-married group\nThe coefficient on marriedTRUE gives the married minus non-married difference\ni.e. the coefficient on a binary variable in a regression gives the difference in means\nIf we’d defined it the other way, with “not married” as the independent variable, the intercept would be the mean for the married group (i.e. “not married = 0”), and the coefficient would be the exact same but times \\(-1\\) (same difference, just opposite direction!)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#comparison-of-means-5",
    "href": "2023/weeks/week05/slides.html#comparison-of-means-5",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nWhy does OLS give us a comparison of means when you give it a binary variable?\n\nThe only \\(X\\) values are 0 (FALSE) and 1 (TRUE)\nBecause of this, OLS no longer really fits a line, it’s more of two separate means\nAnd when you’re estimating to minimize the sum of squared errors separately for each group, can’t do any better than to predict the mean!\nSo you get the mean of each group as each group’s prediction"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#binary-with-controls",
    "href": "2023/weeks/week05/slides.html#binary-with-controls",
    "title": "Binary Variables and Functional Form",
    "section": "Binary with Controls",
    "text": "Binary with Controls\n\nObviously this is handy for including binary controls, but why do this for binary treatments? Because we can add controls!\n\n\n\n                feols(log(earning..\nDependent Var.:       log(earnings)\n                                   \nConstant          8.740*** (0.1478)\nmarriedTRUE      0.3404*** (0.0579)\nkids            -0.2259*** (0.0159)\nage              0.0223*** (0.0038)\n_______________ ___________________\nS.E. type                       IID\nObservations                  2,803\nR2                          0.07609\nAdj. R2                     0.07510\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#multicollinearity",
    "href": "2023/weeks/week05/slides.html#multicollinearity",
    "title": "Binary Variables and Functional Form",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nWhy is just one side of it on the regression? Why aren’t “married” and “not married” BOTH included?\nBecause regression couldn’t give an answer!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#multicollinearity-1",
    "href": "2023/weeks/week05/slides.html#multicollinearity-1",
    "title": "Binary Variables and Functional Form",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMean of married is \\(9.47\\) and of non-married is \\(9.26\\). \\[ \\log(Earnings) = 0 + 9.47Married + 9.26NonMarried \\] \\[ \\log(Earnings) = 3 + 6.47Married + 6.26NonMarried \\]\nThese (and infinitely many other options) all give the exact same predictions! OLS can’t pick between them. There’s no single best way to minimize squared residuals\nSo we pick one with convenient properties, setting one of the categories to have a coefficient of 0 (dropping it) and making the coefficient on the other the difference relative to the one we left out"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#more-than-two-categories",
    "href": "2023/weeks/week05/slides.html#more-than-two-categories",
    "title": "Binary Variables and Functional Form",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nThat interpretation - dropping one and making the other relative to that, conveniently extends to multi-category variables\nWhy stop at binary categorical variables? There are plenty of categorical variables with more than two values\nWhat is your education level? What is your religious denomination? What continent are you on?\nWe can put these in a regression by turning each value into its own binary variable\n(and then dropping one so the coefficients on the others give you the difference with the omitted one)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#more-than-two-categories-1",
    "href": "2023/weeks/week05/slides.html#more-than-two-categories-1",
    "title": "Binary Variables and Functional Form",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nThat interpretation - dropping one and making the other relative to that, conveniently extends to multi-category variables\nWhy stop at binary categorical variables? There are plenty of categorical variables with more than two values\nWhat is your education level? What is your religious denomination? What continent are you on?\nWe can put these in a regression by turning each value into its own binary variable\n(and then dropping one so the coefficients on the others give you the difference with the omitted one)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#more-than-two-categories-2",
    "href": "2023/weeks/week05/slides.html#more-than-two-categories-2",
    "title": "Binary Variables and Functional Form",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nBy changing the reference group, the coefficients change because they’re “different from” a different group!\nAnd notice that, as before, the intercept is the mean of the omitted group (although this changes once you add controls; the intercept is the predicted mean when all right-hand-side variables are 0)\n\n\ntib &lt;- tib %&gt;% mutate(group = factor(group, levels = c('B','A','C','D')))\nfeols(Y ~ group, data = tib)\n\nOLS estimation, Dep. Var.: Y\nObservations: 10,000 \nStandard-errors: IID \n             Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept)  1.976151   0.020241  97.6327 &lt; 2.2e-16 ***\ngroupA      -0.983373   0.028318 -34.7257 &lt; 2.2e-16 ***\ngroupC       1.031905   0.028575  36.1118 &lt; 2.2e-16 ***\ngroupD       2.001948   0.028425  70.4286 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.00187   Adj. R2: 0.557065"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#more-than-two-categories-3",
    "href": "2023/weeks/week05/slides.html#more-than-two-categories-3",
    "title": "Binary Variables and Functional Form",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nSome Interpretations: Controlling for number of kids and age, people with a high school degree have log earnings .324 higher than those without a high school degree (earnings 32.4% higher). BA-holders have earnings 84.8% higher than those without a HS degree\nControlling for kids and age, a graduate degree earns (.976 - .848 =) 12.8% more than someone with a BA (glht() could help!)\n\n\n\n\nNo High School Degree    High School Degree          Some College \n                  333                  1105                   708 \n    Bachelor's Degree       Graduate Degree \n                  356                   301 \n\n\nOLS estimation, Dep. Var.: log(earnings)\nObservations: 2,803 \nStandard-errors: IID \n                             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept)                  8.360245   0.156293 53.49092  &lt; 2.2e-16 ***\neducationHigh School Degree  0.323807   0.067175  4.82036 1.5096e-06 ***\neducationSome College        0.576478   0.071976  8.00934 1.6758e-15 ***\neducationBachelor's Degree   0.848200   0.082697 10.25666  &lt; 2.2e-16 ***\neducationGraduate Degree     0.976291   0.086842 11.24212  &lt; 2.2e-16 ***\nkids                        -0.149679   0.015689 -9.54045  &lt; 2.2e-16 ***\nage                          0.023137   0.003741  6.18453 7.1388e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.05675   Adj. R2: 0.123473"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#concept-checks",
    "href": "2023/weeks/week05/slides.html#concept-checks",
    "title": "Binary Variables and Functional Form",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nIf \\(X\\) is binary, in sentences interpret the coefficients from the estimated OLS equation \\(Y = 4 + 3X + 2Z\\)\nHow might a comparison of means come in handy if you wanted to analyze the results of a randomized policy experiment?\nIf you had a data set of people from every continent and added “continent” as a control, how many coefficients would this add to your model?\nIf in that regression you really wanted to compare Europe to Asia specifically, what might you do so that the regression made this easy?"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interpreting-ols",
    "href": "2023/weeks/week05/slides.html#interpreting-ols",
    "title": "Binary Variables and Functional Form",
    "section": "Interpreting OLS",
    "text": "Interpreting OLS\n\nTo think more about the right-hand-side, let’s go back to our original interpretation of an OLS coefficient \\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\nA one-unit change in \\(X\\) is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\nThis logic still works with binary variables since “a one-unit change in \\(X\\)” means “changing \\(X\\) from No to Yes”\nNotice that this assumes that a one-unit change in \\(X\\) always has the same effect on \\(\\beta_1\\) no matter what else is going on\nWhat if that’s not true?"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#functional-form",
    "href": "2023/weeks/week05/slides.html#functional-form",
    "title": "Binary Variables and Functional Form",
    "section": "Functional Form",
    "text": "Functional Form\n\nWe talked before about times when a linear model like standard OLS might not be sufficient\nHowever, as long as those non-linearities are on the right hand side, we can fix the problem easily but just having \\(X\\) enter non-linearly! Run it through a transformation!\nThe most common transformations by far are polynomials and logarithms"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#functional-form-1",
    "href": "2023/weeks/week05/slides.html#functional-form-1",
    "title": "Binary Variables and Functional Form",
    "section": "Functional Form",
    "text": "Functional Form\n\nWhy do this? Because sometimes a straight line is clearly not going to do the trick!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials",
    "href": "2023/weeks/week05/slides.html#polynomials",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\n\\(\\beta_1X\\) is a “first order polynomial” - there’s one term\n\\(\\beta_1X + \\beta_2X^2\\) is a “second order polynomial” or a “quadratic” - two terms (note both included, it’s not just \\(X^2\\))\n\\(\\beta_1X + \\beta_2X^2 + \\beta_3X^3\\) is a third-order or cubic, etc."
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-1",
    "href": "2023/weeks/week05/slides.html#polynomials-1",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\n\\(\\beta_1X\\) is a “first order polynomial” - there’s one term\n\\(\\beta_1X + \\beta_2X^2\\) is a “second order polynomial” or a “quadratic” - two terms (note both included, it’s not just \\(X^2\\))\n\\(\\beta_1X + \\beta_2X^2 + \\beta_3X^3\\) is a third-order or cubic, etc."
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-2",
    "href": "2023/weeks/week05/slides.html#polynomials-2",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\nWhat do they do?\n\nThe more polynomial terms, the more flexible the line can be. With enough terms you can mimic any shape of relationship\nOf course, if you just add a whole buncha terms, it gets very noisy, and prediction out-of-sample gets very bad\nKeep it minimal - quadratics are almost always enough, unless you have reason to believe there’s a true more-complex relationship. You can try adding higher-order terms and see if they make a difference"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-3",
    "href": "2023/weeks/week05/slides.html#polynomials-3",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nThe true relationship is quadratic"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-in-r",
    "href": "2023/weeks/week05/slides.html#polynomials-in-r",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials in R",
    "text": "Polynomials in R\n\nWe can add an I() function to our regression to do a calculation on a variable before including it. So I(X^2) adds a squared term\nThere’s also a poly() function but avoid it - it does something slightly different\n\n\n# Linear\nfeols(Y ~ X, data = df)\n# Quadratic\nfeols(Y ~ X + I(X^2), data = df)\n# Cubic\nfeols(Y ~ X + I(X^2) + I(X^3), data = df)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#concept-check",
    "href": "2023/weeks/week05/slides.html#concept-check",
    "title": "Binary Variables and Functional Form",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat’s the effect of a one-unit change in \\(X\\) at \\(X = 0\\), \\(X = 1\\), and \\(X = 2\\) for each of these?\n\n\n\n                feols(Y ~ X, dat.. feols(Y ~ X + I(.. feols(Y ~ X + I(...1\nDependent Var.:                  Y                  Y                    Y\n                                                                          \nConstant         7.285*** (0.5660)   -0.1295 (0.3839)      0.0759 (0.5091)\nX               -8.934*** (0.1953)   0.9779* (0.3831)      0.4542 (0.9331)\nX square                           -2.003*** (0.0752)   -1.738*** (0.4368)\nX cube                                                    -0.0354 (0.0574)\n_______________ __________________ __________________   __________________\nS.E. type                      IID                IID                  IID\nObservations                   200                200                  200\nR2                         0.91357            0.98122              0.98126\nAdj. R2                    0.91313            0.98103              0.98097\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms",
    "href": "2023/weeks/week05/slides.html#logarithms",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\nAnother common transformation, both for dependent and independent variables, is to take the logarithm\nThis has the effect of pulling in extreme values from strongly right-skewed data and making linear relationships pop out\nIncome, for example, is almost always used with a logarithm\nIt also gives the coefficients a nice percentage-based interpretation"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms-1",
    "href": "2023/weeks/week05/slides.html#logarithms-1",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\nAnother common transformation, both for dependent and independent variables, is to take the logarithm\nThis has the effect of pulling in extreme values from strongly right-skewed data and making linear relationships pop out\nIncome, for example, is almost always used with a logarithm\nIt also gives the coefficients a nice percentage-based interpretation"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#or-if-you-prefer",
    "href": "2023/weeks/week05/slides.html#or-if-you-prefer",
    "title": "Binary Variables and Functional Form",
    "section": "Or if you prefer…",
    "text": "Or if you prefer…\n\nNotice the change in axes"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms-2",
    "href": "2023/weeks/week05/slides.html#logarithms-2",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms-3",
    "href": "2023/weeks/week05/slides.html#logarithms-3",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\nHow can we interpret them?\nThe key is to remember that \\(\\log(X) + a \\approx \\log((1+a)X)\\), meaning that a \\(a\\)-unit change in \\(log(X)\\) is similar to a \\(a\\times100%\\) change in \\(X\\)\nSo, walk through our “one-unit change in the variable” logic from before, but whenever we hit a log, change that into a percentage!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms-4",
    "href": "2023/weeks/week05/slides.html#logarithms-4",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\n\\(Y = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1X\\) a one-unit change in \\(X\\) is associated with a \\(\\beta_1\\times 100\\)% change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(\\log(Y)\\), or a \\(\\beta_1\\times100\\)% change in \\(Y\\).\n(Try also with changes smaller than one unit - that’s usually more reasonable)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#functional-form-2",
    "href": "2023/weeks/week05/slides.html#functional-form-2",
    "title": "Binary Variables and Functional Form",
    "section": "Functional Form",
    "text": "Functional Form\n\nIn general, you want the shape of your function to match the shape of the relationship in the data (or, even better, the true relationship)\nPolynomials and logs can usually get you there!\nWhich to use? Use logs for highly skewed data or variables with exponential relationships\nUse polynomials if it doesn’t look straight! Check that scatterplot and see how not-straight it is!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#concept-checks-1",
    "href": "2023/weeks/week05/slides.html#concept-checks-1",
    "title": "Binary Variables and Functional Form",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhich of the following variables would you likely want to log before using them? Income, height, wealth, company size, home square footage\nIn each of the following estimated OLS lines, interpret the coefficient by filling in “A [blank] change in X is associated with a [blank] change in Y”:\n\n\\[ Y = 1 + 2\\log(X) \\] \\[ \\log(Y) = 3 + 2\\log(X) \\]\n\\[ \\log(Y) = 4 + 3X \\]"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions",
    "href": "2023/weeks/week05/slides.html#interactions",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nFor both polynomials and logarithms, the effect of a one-unit change in \\(X\\) differs depending on its current value (for logarithms, a 1-unit change in \\(X\\) is different percentage changes in \\(X\\) depending on current value)\nBut why stop there? Maybe the effect of \\(X\\) differs depending on the current value of other variables!\nEnter interaction terms!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z + \\varepsilon \\] - Interaction terms are a little tough but also extremely important."
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-1",
    "href": "2023/weeks/week05/slides.html#interactions-1",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\nExpect to come back to these slides, as you’re almost certainly going to use interaction terms in both our assessment and the dissertation"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-2",
    "href": "2023/weeks/week05/slides.html#interactions-2",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nChange in the value of a control can shift a regression line up and down\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z\\), estimated as \\(Y = .01 + 1.2X + .95Z\\):"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-3",
    "href": "2023/weeks/week05/slides.html#interactions-3",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nBut an interaction can both shift the line up and down AND change its slope\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z\\), estimated as \\(Y = .035 + 1.14X + .94Z + 1.02X\\times Z\\):"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-4",
    "href": "2023/weeks/week05/slides.html#interactions-4",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nHow can we interpret an interaction?\nThe idea is that the interaction shows how the effect of one variable changes as the value of the other changes\nThe derivative helps!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z \\] \\[ \\partial Y/\\partial X = \\beta_1 + \\beta_3 Z \\]\n\nThe effect of \\(X\\) is \\(\\beta_1\\) when \\(Z = 0\\), or \\(\\beta_1 + \\beta_3\\) when \\(Z = 1\\), or \\(\\beta_1 + 3\\beta_3\\) if \\(Z = 3\\)!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-5",
    "href": "2023/weeks/week05/slides.html#interactions-5",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nOften we are doing interactions with binary variables to see how an effect differs across groups\nNow, instead of the intercept giving the baseline and the binary coefficient giving the difference, the coefficient on \\(X\\) is the baseline effect of \\(X\\) and the interaction is the difference in the effect of \\(X\\)\nThe interaction coefficient becomes “the difference in the effect of \\(X\\) between the \\(Z\\) =”No” and \\(Z\\) = “Yes” groups”\n(What if it’s continuous? Mathematically the same but the thinking changes - the interaction term is the difference in the effect of \\(X\\) you get when increasing \\(Z\\) by one unit)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#notes-on-interactions",
    "href": "2023/weeks/week05/slides.html#notes-on-interactions",
    "title": "Binary Variables and Functional Form",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nLike with polynomials, the coefficients on their own now have little meaning and must be evaluated alongside each other. \\(\\beta_1\\) by itself is just “the effect of \\(X\\) when \\(Z = 0\\)”, not “the effect of \\(X\\)”\nYes, you do almost always want to include both variables in un-interacted form and interacted form. Otherwise the interpretation gets very thorny"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#in-r",
    "href": "2023/weeks/week05/slides.html#in-r",
    "title": "Binary Variables and Functional Form",
    "section": "In R!",
    "text": "In R!\n\nBinary variables in R (on the right-hand-side) you can just treat as normal variables\nCategorical variables too (although if it’s numeric you may need to run it through factor() first, or i() in feols())\nIn feols() you can specify which group gets dropped using i() and setting ref in it\n\n\n# drops married = FALSE\nfeols(log(earnings) ~ married, data = PSID)\n# drops married = TRUE\nfeols(log(earnings) ~ i(married, ref = 'TRUE'), data = PSID)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#binary-variables-1",
    "href": "2023/weeks/week05/slides.html#binary-variables-1",
    "title": "Binary Variables and Functional Form",
    "section": "Binary Variables",
    "text": "Binary Variables\n\n\n                            feols(log(earning..\nDependent Var.:                   log(earnings)\n                                               \nConstant                      10.06*** (0.0701)\neducationGraduateDegree        0.1673* (0.0842)\neducationHighSchoolDegree   -0.5433*** (0.0658)\neducationNoHighSchoolDegree -0.9404*** (0.0826)\neducationSomeCollege        -0.2893*** (0.0699)\nI(kids&gt;0)TRUE               -0.2922*** (0.0548)\n___________________________ ___________________\nS.E. type                                   IID\nObservations                              2,803\nR2                                      0.09907\nAdj. R2                                 0.09746\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-and-logarithms",
    "href": "2023/weeks/week05/slides.html#polynomials-and-logarithms",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials and Logarithms",
    "text": "Polynomials and Logarithms\n\nAs previously discussed, \\(I()\\) will let us do functions like \\(X^2\\)\nWe can also do log() straight in the regression.\n\n\nlm(Y ~ X + I(X^2) + I(X^3), data = df)\nlm(log(Y) ~ log(X), data = df)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-6",
    "href": "2023/weeks/week05/slides.html#interactions-6",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nMarriage for those without a college degree raises earnings by 24%. A college degree reduces the marriage premium by 25%. Marriage for those with a college degree reduces earnings by .24 - .25 = -1%\n\n\n\n                          feols(log(earnin..\nDependent Var.:                log(earnings)\n                                            \nConstant                   9.087*** (0.0583)\nmarriedTRUE               0.2381*** (0.0638)\ncollegeTRUE               0.8543*** (0.1255)\nmarriedTRUE x collegeTRUE  -0.2541. (0.1363)\n_________________________ __________________\nS.E. type                                IID\nObservations                           2,803\nR2                                   0.06253\nAdj. R2                              0.06153\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#tests",
    "href": "2023/weeks/week05/slides.html#tests",
    "title": "Binary Variables and Functional Form",
    "section": "Tests",
    "text": "Tests\n\nwald() can be handy for testing groups of binary variables for a categorical\nAlso good for testing all the polynomial terms, or testing if the effect of \\(X\\) is significant at a certain value of \\(Z\\)\n\n\n# Is the education effect zero overall?\nm1 &lt;- feols(log(earnings)~educatn, data = PSID)\nwald(m1, 'educatn')\n\n# Does X have any effect?\nm2 &lt;- feols(Y ~ X + I(X^2) + I(X^3), data = df)\nwald(m2, 'X') # Gets all coefficients with an 'X' anywhere in the name - check this is right!\n\n# Is the effect of X significant when Z = 5?\nlibrary(multcomp)\nm3 &lt;- lm(Y ~ X*Z, data = df)\nglht(m3, 'X + 5*X:Z= 0') %&gt;% summary()\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-4",
    "href": "2023/weeks/week05/slides.html#polynomials-4",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nHigher-order terms don’t do anything for us here (because a quadratic is sufficient!)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-5",
    "href": "2023/weeks/week05/slides.html#polynomials-5",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nInterpret polynomials using the derivative\n\\(\\partial Y/\\partial X\\) will be different depending on the value of \\(X\\) (as it should! Notice in the graph that the slope changes for different values of \\(X\\))\n\n\\[ Y = \\beta_1X + \\beta_2X^2 \\] \\[ \\partial Y/\\partial X = \\beta_1 + 2\\beta_2X \\]\nSo at \\(X = 0\\), the effect of a one-unit change in \\(X\\) is \\(\\beta_1\\). At \\(X = 1\\), it’s \\(\\beta_1 + \\beta_2\\). At \\(X = 5\\) it’s \\(\\beta_1 + 5\\beta_2\\)."
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interactions-7",
    "href": "2023/weeks/week05/slides.html#interactions-7",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nX*Z will include X, Z, and also their interaction\nIf necessary, X:Z is the interaction only, but you rarely need this. However, it’s handy for referring to the interaction term in linearHypothesis!\nIn feols() the i() function is a very powerful way of doing interactions\n\n\nfeols(Y ~ X*Z, data = df)\nfeols(Y ~ X + X:Z, data = df)\nfeols(Y ~ i(Z, X), data = df)\nfeols(Y ~ i(Z, X), data = df) |&gt; iplot()"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#notes-on-interactions-1",
    "href": "2023/weeks/week05/slides.html#notes-on-interactions-1",
    "title": "Binary Variables and Functional Form",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nInteraction effects are poorly powered. You need a lot of data to be able to tell whether an effect is different in two groups. If \\(N\\) observations is adequate power to see if the effect itself is different from zero, you need a sample of roughly \\(16\\times N\\) to see if the difference in effects is nonzero. Sixteen times!!\nIt’s tempting to try interacting your effect with everything to see if it’s bigger/smaller/nonzero in some groups, but because it’s poorly powered, this is a bad idea! You’ll get a lot of false positives"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#bias-and-the-error-term",
    "href": "2023/weeks/week05/slides.html#bias-and-the-error-term",
    "title": "Binary Variables and Functional Form",
    "section": "Bias and the Error Term",
    "text": "Bias and the Error Term\n\nAll of the nice stuff we’ve gotten so far makes some assumptions about our true model\n\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nIn particular, we’ve made some assumptions about the error term \\(\\varepsilon\\)\nSo what is that error term exactly, and what are we assuming about it?"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#the-error-term",
    "href": "2023/weeks/week05/slides.html#the-error-term",
    "title": "Binary Variables and Functional Form",
    "section": "The Error Term",
    "text": "The Error Term\n\nThe error term contains everything that isn’t in our model\nIf \\(Y\\) were a pure function of \\(X\\), for example if \\(Y\\) was “height in feet” and \\(X\\) was “height in inches”, we wouldn’t have an error term, because a straight line fully describes the relationship perfectly with no variation\nBut in most cases, the line is a simplification - we’re leaving other stuff out! That’s in the error term"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#the-error-term-1",
    "href": "2023/weeks/week05/slides.html#the-error-term-1",
    "title": "Binary Variables and Functional Form",
    "section": "The Error Term",
    "text": "The Error Term\n\nConsider this data generating process:\n\n\\[ ClassGrade = \\beta_0 + \\beta_1 StudyTime + \\varepsilon \\]\n\nSurely StudyTime isn’t the only thing that determines your ClassGrade\nEverything else is in the error term!\nProfessorLeniency, InterestInSubject, Intelligence, and so on and so on…"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#the-error-term-2",
    "href": "2023/weeks/week05/slides.html#the-error-term-2",
    "title": "Binary Variables and Functional Form",
    "section": "The Error Term",
    "text": "The Error Term\n\nIsn’t that really bad? We’re leaving out a bunch of important stuff!\nIf you want to predict \\(Y\\) as accurately as possible then we’re probably going to do a bad job of it\nBut if our real interest is *figuring out the relationship between \\(X\\) and \\(Y\\), then it’s fine to leave stuff out, as long as whatever’s left in the error term obeys a few important assumptions"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#error-term-assumptions",
    "href": "2023/weeks/week05/slides.html#error-term-assumptions",
    "title": "Binary Variables and Functional Form",
    "section": "Error Term Assumptions",
    "text": "Error Term Assumptions\n\nThe most important assumption about the error term is that it is unrelated to \\(X\\)\nIf \\(X\\) and \\(\\varepsilon\\) are correlated, \\(\\hat{\\beta}_1\\) will be biased - its distribution no longer has the true \\(\\beta_1\\) as its mean\nIn these cases we can say ” \\(X\\) is endogenous” or “we have omitted variable bias”\nNo amount of additional sample size will fix that problem!\n(what will fix the problem? We’ll get to that one later)"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#endogeneity",
    "href": "2023/weeks/week05/slides.html#endogeneity",
    "title": "Binary Variables and Functional Form",
    "section": "Endogeneity",
    "text": "Endogeneity\n\nWhy will this bias \\(\\hat{\\beta}_1\\)?\nLet’s say the data generating process looks like the below diagram, but we estimate \\(ClassGrade = \\beta_0 + \\beta_1 StudyTime + \\varepsilon\\)\nBecause InterestInSubject affects both ClassGrade and StudyTime, you’ll see people with both high ClassGrade and high StudyTime, but not becuase StudyTime caused ClassGrade, rather because InterestInSubject caused both!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#omitted-variable-bias",
    "href": "2023/weeks/week05/slides.html#omitted-variable-bias",
    "title": "Binary Variables and Functional Form",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\n\nWe can intuitively think about whether omitted variable bias is likely to make our estimates too high or too low\nThe sign of the bias will be the sign of the relationship between the omitted variable and \\(X\\), times the sign of the relationship between the omitted variable bias and \\(Y\\)\nInterestInSubject is positively related to both StudyTime and ClassGrade, and \\(+\\times+ = +\\), so our estimates are positively biased / too high\nMore precisely we have that the mean of the \\(\\hat{\\beta}_1\\) sampling distribution is \\[\\beta_1 + corr(X,\\varepsilon)\\frac{\\sigma_\\varepsilon}{\\sigma_X}\\]"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#thinking-through-this-bias",
    "href": "2023/weeks/week05/slides.html#thinking-through-this-bias",
    "title": "Binary Variables and Functional Form",
    "section": "Thinking Through this Bias",
    "text": "Thinking Through this Bias\n\nIf \\(Z\\) hangs around \\(X\\), but \\(Y\\) doesn’t know about it, then the coefficient on \\(X\\) will get all the credit for \\(Z\\)\nThis is a good way to keep that “direction of bias” problem in mind\nAnd you do want to keep it in mind! This is important for understanding general correlations you see in the wild, too\nAnd helps keep in line some things - for example, if \\(Z\\) is unrelated to \\(X\\), it won’t bias you!!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#less-serious-error-concerns",
    "href": "2023/weeks/week05/slides.html#less-serious-error-concerns",
    "title": "Binary Variables and Functional Form",
    "section": "Less Serious Error Concerns",
    "text": "Less Serious Error Concerns\n\nOmitted variable bias can, well, bias us, which is very bad\nThere are some other assumptions that can fail that may also pose a problem to us but less so\nWe’ve assumed so far not just that \\(\\varepsilon\\) is unrelated to \\(X\\), but also that the variance of \\(\\varepsilon\\) is unrelated to \\(X\\), and that the \\(\\varepsilon\\)s are unrelated to each other\nIf these assumptions fail, our standard errors will be wrong, but we won’t be biased, and also there are ways to fix the standard errors\nWe will cover these only briefly, they’ll come back later"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#heteroskedasticity",
    "href": "2023/weeks/week05/slides.html#heteroskedasticity",
    "title": "Binary Variables and Functional Form",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nIf the variance of the error term is different for different values of \\(X\\), then we have “heteroskedasticity”\nNotice in the below graph how the spread of the points around the line (the variance of the error term) is bigger on the right than the left"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#heteroskedasticity-1",
    "href": "2023/weeks/week05/slides.html#heteroskedasticity-1",
    "title": "Binary Variables and Functional Form",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nWe can correct for this using heteroskedasticity-robust standard errors which sort of “squash down” the big variances and then re-estimate the standard errors\nWe can do this in feols with vcov = 'hetero' in R or with “robust” in STATA"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#correlated-errors",
    "href": "2023/weeks/week05/slides.html#correlated-errors",
    "title": "Binary Variables and Functional Form",
    "section": "Correlated Errors",
    "text": "Correlated Errors\n\nIf the error terms are correlated with each other, then our standard errors will also be wrong\nHow could this happen? For example, maybe you’ve surveyed a bunch of people in different towns - the error terms within a town might be clustered\nOr maybe you have time series data. If a term in the error term is “sticky” or has “momentum” it will likely be a similar error term a few time periods in a row, beign correlated across time, or autocorrelated\nAgain, this doesn’t bias \\(\\hat{\\beta}_1\\) but it can affect standard errors!"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#thinking-through-this-bias-1",
    "href": "2023/weeks/week05/slides.html#thinking-through-this-bias-1",
    "title": "Binary Variables and Functional Form",
    "section": "Thinking Through this Bias",
    "text": "Thinking Through this Bias\n\nThat means that when you’re thinking about the controls you need, that only includes things related to \\(X\\)\nAdding controls for things related to \\(Y\\) not \\(X\\) can make the model predict better and reduce standard errors, but won’t remove omitted variable bias"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#binary-data-1",
    "href": "2023/weeks/week05/slides.html#binary-data-1",
    "title": "Binary Variables and Functional Form",
    "section": "Binary Data",
    "text": "Binary Data\n\nA variable is binary if it only has two values - 0 or 1 (or “No” or “Yes”, etc.)\nBinary variables are super common in econometrics!\nDid you get the treatment? Yes / No\nDo you live in the US? Yes / No\nIs a floating exchange rate in effect? Yes / No"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#interpreting-ols-1",
    "href": "2023/weeks/week05/slides.html#interpreting-ols-1",
    "title": "Binary Variables and Functional Form",
    "section": "Interpreting OLS",
    "text": "Interpreting OLS\n\nTo think more about the right-hand-side, let’s go back to our original interpretation of an OLS coefficient \\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\nA one-unit change in \\(X\\) is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\nThis logic still works with binary variables since “a one-unit change in \\(X\\)” means “changing \\(X\\) from No to Yes”\nNotice that this assumes that a one-unit change in \\(X\\) always has the same effect on \\(\\beta_1\\) no matter what else is going on\nWhat if that’s not true?"
  },
  {
    "objectID": "2023/weeks/week05/slides.html#polynomials-6",
    "href": "2023/weeks/week05/slides.html#polynomials-6",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nIMPORTANT: when you have a polynomial, the coefficients on each individual term mean very little on their own. You have to consider them alongisde the other coefficients from the polynomial! Never interpret \\(\\beta_1\\) here without thinking about \\(\\beta_2\\) alongside. Also, the significance of the individual terms doesn’t really matter - consider doing an F-test of all of them at once."
  },
  {
    "objectID": "2023/weeks/week05/slides.html#logarithms-5",
    "href": "2023/weeks/week05/slides.html#logarithms-5",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\nDownsides:\n\nLogarithms require that all data be positive. No negatives or zeroes!\nFairly rare that a variable with negative values wants a log anyway\nBut zeroes are common! A common practice is to just do \\(log(X+1)\\) but this is pretty arbitrary"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#a-pickle",
    "href": "2023/weeks/week07/slides.html#a-pickle",
    "title": "🗓️ Week 7 Within variation",
    "section": "A Pickle",
    "text": "A Pickle\n\nSo obviously this is a problem, and it’s not one we can reason or trick our way out of\nIf we don’t have the variable we need to control for, we don’t have it\n… or do we?"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-rest-of-the-term",
    "href": "2023/weeks/week07/slides.html#the-rest-of-the-term",
    "title": "🗓️ Week 7 Within variation",
    "section": "The Rest of the Term",
    "text": "The Rest of the Term\n\nMuch of the rest of the term is going to be focused on finding ways to control for stuff that we can’t measure\nSeems impossible! But it is possible, at least in some circumstances\nToday, we will be talking about within variation and between variation, and the ability to control for all between variation using fixed effects"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#panel-data-1",
    "href": "2023/weeks/week07/slides.html#panel-data-1",
    "title": "🗓️ Week 7 Within variation",
    "section": "Panel Data",
    "text": "Panel Data\n\nHere’s what (a few rows from) a panel data set looks like - a variable for individual (county), a variable for time (year), and then the data\n\n\n\n\n\n\nCounty\nYear\nCrimeRate\nProbofArrest\n\n\n\n\n1\n81\n0.0398849\n0.289696\n\n\n1\n82\n0.0383449\n0.338111\n\n\n1\n83\n0.0303048\n0.330449\n\n\n1\n84\n0.0347259\n0.362525\n\n\n1\n85\n0.0365730\n0.325395\n\n\n1\n86\n0.0347524\n0.326062\n\n\n1\n87\n0.0356036\n0.298270\n\n\n3\n81\n0.0163921\n0.202899\n\n\n3\n82\n0.0190651\n0.162218\n\n\n\n 9 rows out of 630. \"Prob. of Arrest\" is estimated probability of being arrested when you commit a crime"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#between-and-within-1",
    "href": "2023/weeks/week07/slides.html#between-and-within-1",
    "title": "🗓️ Week 7 Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nIf we look at the overall variation, just pretending this is all together, we get this"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#between-and-within-2",
    "href": "2023/weeks/week07/slides.html#between-and-within-2",
    "title": "🗓️ Week 7 Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nBETWEEN variation is what we get if we look at the relationship between the means of each county"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#between-and-within-3",
    "href": "2023/weeks/week07/slides.html#between-and-within-3",
    "title": "🗓️ Week 7 Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nAnd I mean it! Only look at those means! The individual year-to-year variation within county doesn’t matter."
  },
  {
    "objectID": "2023/weeks/week07/slides.html#between-and-within-4",
    "href": "2023/weeks/week07/slides.html#between-and-within-4",
    "title": "🗓️ Week 7 Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nWithin variation goes the other way - it treats those orange crosses as their own individualized sets of axes and looks at variation within county from year-to-year only!\nWe basically slide the crosses over on top of each other and then analyze that data"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#between-and-within-5",
    "href": "2023/weeks/week07/slides.html#between-and-within-5",
    "title": "🗓️ Week 7 Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nWe can clearly see that between counties there’s a strong positive relationship\nBut if you look within a given county, the relationship isn’t that strong, and actually seems to be negative\nWhich would make sense - if you think your chances of getting arrested are high, that should be a deterrent to crime\nBut what are we actually doing here? Let’s think about the causal diagram / data-generating process!\nWhat goes into the probability of arrest and the crime rate? Lots of stuff!"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-crime-rate",
    "href": "2023/weeks/week07/slides.html#the-crime-rate",
    "title": "🗓️ Week 7 Within variation",
    "section": "The Crime Rate",
    "text": "The Crime Rate\n\n“LocalStuff” is just all the things unique to that area\n“LawAndOrder” is how committed local politicians are to “Law and Order Politics”"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#between-and-within-6",
    "href": "2023/weeks/week07/slides.html#between-and-within-6",
    "title": "🗓️ Week 7 Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nFor each of these variables we can ask if they vary between groups and/or within groups\nLocalStuff is all the stuff unique to that county - geography, landmarks, the quality of the schools, almost by definition this only varies between groups. It’s not like the things that make your county unique are different each year (or at least not very different)\nWhether the county has LawAndOrder and how many CivilRights you’re allowed might change a bit year to year, but in general, political climates like that change pretty slowly. At a bit of a stretch we can call that something that only varies between groups too\nPolice budgets (and thus number of police on the streets) and Poverty (which varies with the economy) vary both between counties, but also within counties from year to year\nVariables with between variation only (by our assumption): LocalStuff, LawAndOrder, CivilRights\nVariables with both between and within variation: Police, Poverty"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#between-and-within-7",
    "href": "2023/weeks/week07/slides.html#between-and-within-7",
    "title": "🗓️ Week 7 Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nLet’s simplify our graph!\nSome of the variables only vary between counties\nSo, we can replace those variables on the graph with the variable County\nRight? That’s where all the variation is anyway"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-crime-rate-1",
    "href": "2023/weeks/week07/slides.html#the-crime-rate-1",
    "title": "🗓️ Week 7 Within variation",
    "section": "The Crime Rate",
    "text": "The Crime Rate\n\n“LocalStuff” is just all the things unique to that area\n“LawAndOrder” is how committed local politicians are to “Law and Order Politics”"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#between-and-within-8",
    "href": "2023/weeks/week07/slides.html#between-and-within-8",
    "title": "🗓️ Week 7 Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nNow the task of identifying ProbArrest \\(\\rightarrow\\) CrimeRate becomes much simpler!\nIf we control for County, that will close a lot of back doors for us\n(based on the diagram, all we need to control for is County and Poverty!)\nConveniently, we can control for County just like it was any other variable!\nAnd when we do, we automatically control for all variables that only have between variation, whatever they are, even if we can’t measure them directly or didn’t think about them\nAll that’s left is the within variation"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#concept-checks",
    "href": "2023/weeks/week07/slides.html#concept-checks",
    "title": "Binary Variables and Functional Form",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nIf \\(X\\) is binary, in sentences interpret the coefficients from the estimated OLS equation \\(Y = 4 + 3X + 2Z\\)\nHow might a comparison of means come in handy if you wanted to analyze the results of a randomized policy experiment?\nIf you had a data set of people from every continent and added “continent” as a control, how many coefficients would this add to your model?\nIf in that regression you really wanted to compare Europe to Asia specifically, what might you do so that the regression made this easy?"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#removing-between-variation",
    "href": "2023/weeks/week07/slides.html#removing-between-variation",
    "title": "🗓️ Week 7 Within variation",
    "section": "Removing Between Variation",
    "text": "Removing Between Variation\n\nOkay so that’s the concept\nRemove all the between variation so that all that’s left is within variation\nAnd in the process control for any variables that are made up only of between variation\nHow can we actually do this? And what’s really going on?\nLet’s first talk about the regression model itself that this implies\nThen let’s actually do the thing. There are two main ways: de-meaning and binary variables (they give the same result, for balanced panels anyway)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#estimation-vs.-design",
    "href": "2023/weeks/week07/slides.html#estimation-vs.-design",
    "title": "🗓️ Week 7 Within variation",
    "section": "Estimation vs. Design",
    "text": "Estimation vs. Design\n\nTo be clear, this is exactly 0% different from what we’ve done before in terms of controlling for stuff\nAnd in fact we’re about to do the exact same thing we did before by just adding a categorical control variable for county or whatever\n(and in fact the “within” thing holds with other categorical controls - a categorical control for education isolates variation “within education levels”)\nThe difference is the reason we’re doing it. It’s fixed effects because a categorical control for individual controls for a lot of stuff, and we think closes a lot of back doors for us, not just one, and not just ones we can measure"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-model",
    "href": "2023/weeks/week07/slides.html#the-model",
    "title": "🗓️ Week 7 Within variation",
    "section": "The Model",
    "text": "The Model\nThe \\(it\\) subscript says this variable varies over individual \\(i\\) and time \\(t\\)\n\\[Y_{it} = \\beta_0 + \\beta_1 X_{it} + \\varepsilon_{it}\\]\n\n\\(X_{it}\\) is related to LocalStuff which is not in the model and thus in the error term!\nRegular ol’ omitted variable bias. If we don’t adjust for the individual effect, we get a biased \\(\\hat{\\beta}_1\\)\n(this bias is called “pooling bias” although it’s really just a form of omitted variable bias)\nWe really have this then: \\[Y_{it} = \\beta_0 + \\beta_1 X_{it} + (\\alpha_i + \\varepsilon_{it})\\]"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#de-meaning",
    "href": "2023/weeks/week07/slides.html#de-meaning",
    "title": "🗓️ Week 7 Within variation",
    "section": "De-meaning",
    "text": "De-meaning\n\nLet’s do de-meaning first, since it’s most closely and obviously related to the “removing between variation” explanation we’ve been going for\nThe process here is simple!\n\n\nFor each variable \\(X_{it}\\), \\(Y_{it}\\), etc., get the mean value of that variable for each individual \\(\\bar{X}_i, \\bar{Y}_i\\)\nSubtract out that mean to get residuals \\((X_{it} - \\bar{X}_i), (Y_{it} - \\bar{Y}_i)\\)\nWork with those residuals\n\n\nThat’s it!"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#how-does-this-work",
    "href": "2023/weeks/week07/slides.html#how-does-this-work",
    "title": "🗓️ Week 7 Within variation",
    "section": "How does this work?",
    "text": "How does this work?\n\nThat \\(\\alpha_i\\) term gets absorbed\nThe residuals are, by construction, no longer related to the \\(\\alpha_i\\), so it no longer goes in the residuals!\n\n\\[(Y_{it} - \\bar{Y}_i) = \\beta_0 + \\beta_1(X_{it} - \\bar{X}_i) + \\varepsilon_{it}\\]"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#lets-do-it",
    "href": "2023/weeks/week07/slides.html#lets-do-it",
    "title": "🗓️ Week 7 Within variation",
    "section": "Let’s do it!",
    "text": "Let’s do it!\n\nWe can use group_by to get means-within-groups and subtract them out\n\n\ndata(crime4, package = 'wooldridge')\ncrime4 &lt;- crime4 %&gt;%\n  ## Filter to the data points from our graph\n  filter(county %in% c(1,3,7, 23),\n         prbarr &lt; .5) %&gt;%\n  group_by(county) %&gt;%\n  mutate(mean_crime = mean(crmrte),\n         mean_prob = mean(prbarr)) %&gt;%\n  mutate(demeaned_crime = crmrte - mean_crime,\n         demeaned_prob = prbarr - mean_prob)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#and-regress",
    "href": "2023/weeks/week07/slides.html#and-regress",
    "title": "🗓️ Week 7 Within variation",
    "section": "And Regress!",
    "text": "And Regress!\n\norig_data &lt;- feols(crmrte ~ prbarr, data = crime4)\nde_mean &lt;- feols(demeaned_crime ~ demeaned_prob, data = crime4)\netable(orig_data, de_mean)\n\n                        orig_data           de_mean\nDependent Var.:            crmrte    demeaned_crime\n                                                   \nConstant         0.0118* (0.0050) 1.41e-18 (0.0004)\nprbarr          0.0486** (0.0167)                  \ndemeaned_prob                     -0.0305* (0.0117)\n_______________ _________________ _________________\nS.E. type                     IID               IID\nObservations                   27                27\nR2                        0.25308           0.21445\nAdj. R2                   0.22321           0.18303\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#interpreting-a-within-relationship",
    "href": "2023/weeks/week07/slides.html#interpreting-a-within-relationship",
    "title": "🗓️ Week 7 Within variation",
    "section": "Interpreting a Within Relationship",
    "text": "Interpreting a Within Relationship\n\nHow can we interpret that slope of -0.03?\nThis is all within variation so our interpretation must be within-county\nSo, “comparing a county in year A where its arrest probability is 1 (100 percentage points) higher than it is in year B, we expect the number of crimes per person to drop by .03”\nOr if we think we’ve causally identified it (and want to work on a more realistic scale), “raising the arrest probability by 1 percentage point in a county reduces the number of crimes per person in that county by .0003”.\nWe’re basically “controlling for county” (and will do that explicitly in a moment)\nSo your interpretation should think of it in that way - holding county constant i.e. comparing two observations with the same value of county i.e. comparing a county to itself at a different point in time"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#concept-checks-1",
    "href": "2023/weeks/week07/slides.html#concept-checks-1",
    "title": "Binary Variables and Functional Form",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhich of the following variables would you likely want to log before using them? Income, height, wealth, company size, home square footage\nIn each of the following estimated OLS lines, interpret the coefficient by filling in “A [blank] change in X is associated with a [blank] change in Y”:\n\n\\[ Y = 1 + 2\\log(X) \\] \\[ \\log(Y) = 3 + 2\\log(X) \\]\n\\[ \\log(Y) = 4 + 3X \\]"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-least-squares-dummy-variable-approach",
    "href": "2023/weeks/week07/slides.html#the-least-squares-dummy-variable-approach",
    "title": "🗓️ Week 7 Within variation",
    "section": "The Least Squares Dummy Variable Approach",
    "text": "The Least Squares Dummy Variable Approach\n\nDe-meaning the data isn’t the only way to do it!\nYou can also use the least squares dummy variable (another word for “binary variable”) method\nWe just treat “individual” like the categorical variable it is and add it as a control! Again, the regression approach is exactly the same as with any categorical control, but the research design reason for doing it is different"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#lets-do-it-1",
    "href": "2023/weeks/week07/slides.html#lets-do-it-1",
    "title": "🗓️ Week 7 Within variation",
    "section": "Let’s do it!",
    "text": "Let’s do it!\n\nlsdv &lt;- feols(crmrte ~ prbarr + factor(county), data = crime4)\netable(orig_data, de_mean, lsdv, keep = c('prbarr', 'demeaned_prob'))\n\n                        orig_data           de_mean              lsdv\nDependent Var.:            crmrte    demeaned_crime            crmrte\n                                                                     \nprbarr          0.0486** (0.0167)                   -0.0305* (0.0124)\ndemeaned_prob                     -0.0305* (0.0117)                  \n_______________ _________________ _________________ _________________\nS.E. type                     IID               IID               IID\nObservations                   27                27                27\nR2                        0.25308           0.21445           0.94114\nAdj. R2                   0.22321           0.18303           0.93044\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-same",
    "href": "2023/weeks/week07/slides.html#the-same",
    "title": "🗓️ Week 7 Within variation",
    "section": "The same!",
    "text": "The same!\n\nThe result is the same, as it should be\nExcept for that \\(R^2\\) - What is that “within R2”?\nBecause de-meaning takes out the part explained by the fixed effects ( \\(\\alpha_i\\) ) before running the regression, while LSDV does it in the regression\nSo the .94 is the portion of crmrte explained by prbarr and county, whereas the .21 is the “within - \\(R^2\\)” - the portion of the within variation that’s explained by prbarr\nNeither is wrong (and the .94 isn’t “better”), they’re just measuring different things"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#why-lsdv",
    "href": "2023/weeks/week07/slides.html#why-lsdv",
    "title": "🗓️ Week 7 Within variation",
    "section": "Why LSDV?",
    "text": "Why LSDV?\n\nA benefit of the LSDV approach is that it calculates the fixed effects \\(\\alpha_i\\) for you\nWe left those out of the table with the coefs argument of export_summs (we rarely want them) but here they are:\n\n\n\nOLS estimation, Dep. Var.: crmrte\nObservations: 27 \nStandard-errors: IID \n                  Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)       0.045631   0.004116  11.08640 1.7906e-10 ***\nprbarr           -0.030491   0.012442  -2.45068 2.2674e-02 *  \nfactor(county)3  -0.025308   0.002165 -11.68996 6.5614e-11 ***\nfactor(county)7  -0.009870   0.001418  -6.96313 5.4542e-07 ***\nfactor(county)23 -0.008587   0.001258  -6.82651 7.3887e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.001933   Adj. R2: 0.930441\n\n\n\nInterpretation is exactly the same as with a categorical variable - we have an omitted county, and these show the difference relative to that omitted county"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#why-lsdv-1",
    "href": "2023/weeks/week07/slides.html#why-lsdv-1",
    "title": "🗓️ Week 7 Within variation",
    "section": "Why LSDV?",
    "text": "Why LSDV?\n\nThis also makes clear another element of what’s happening! Just like with a categorical var, the line is moving up and down to meet the counties\nGraphically, de-meaning moves all the points together in the middle to draw a line, while LSDV moves the line up and down to meet the points"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#why-not-lsdv",
    "href": "2023/weeks/week07/slides.html#why-not-lsdv",
    "title": "🗓️ Week 7 Within variation",
    "section": "Why Not LSDV?",
    "text": "Why Not LSDV?\n\nLSDV is computationally expensive\nIf there are a lot of individuals, or big data, or if you have many sets of fixed effects (yes you can do more than just individual - we’ll get to that next time!), it can be very slow\nMost professionally made fixed-effects commands use de-meaning, but then adjust the standard errors properly\n(They also leave the fixed effects coefficients off the regression table by default)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#concept-checks-2",
    "href": "2023/weeks/week07/slides.html#concept-checks-2",
    "title": "🗓️ Week 7 Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy can’t we use individual-person fixed effects to study the impact of race on traffic stops?\nThe within \\(R^2\\) from is .3, and the overall \\(R^2\\) is .5. Interpret these two numbers in sentences\nIn a sentence, interpret the slope coefficient in the estimated model \\((Y_{it} - \\bar{Y}_i) = 1 + .5(X_{it} - \\bar{X}_i)\\) where \\(Y\\) is “school funding per child” and \\(X\\) is “population growth”, and \\(i\\) is city\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html",
    "href": "2023/weeks/week08/slides11.html",
    "title": "🗓️ Week 8  Within variation",
    "section": "",
    "text": "So far we’ve been learning about how to set up, run, and interpret an ordinary least squares regression\nThis is a key skill for anyone doing anything with data - even if you never run a regular ol’ linear regression again, pretty much everything else in applied stats builds off of it in some way\nAnother thing we’ve been doing is thinking about how to design and add controls to that regression to identify our effect of interest by closing back doors"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#a-pickle",
    "href": "2023/weeks/week08/slides11.html#a-pickle",
    "title": "🗓️ Week 8  Within variation",
    "section": "A Pickle",
    "text": "A Pickle\n\nSo obviously this is a problem, and it’s not one we can reason or trick our way out of\nIf we don’t have the variable we need to control for, we don’t have it\n… or do we?"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#the-rest-of-the-term",
    "href": "2023/weeks/week08/slides11.html#the-rest-of-the-term",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Rest of the Term",
    "text": "The Rest of the Term\n\nMuch of the rest of the term is going to be focused on finding ways to control for stuff that we can’t measure\nSeems impossible! But it is possible, at least in some circumstances\nToday, we will be talking about within variation and between variation, and the ability to control for all between variation using fixed effects"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#panel-data-1",
    "href": "2023/weeks/week08/slides11.html#panel-data-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Panel Data",
    "text": "Panel Data\n\nHere’s what (a few rows from) a panel data set looks like - a variable for individual (county), a variable for time (year), and then the data\n\n\n\n\n\n\nCounty\nYear\nCrimeRate\nProbofArrest\n\n\n\n\n1\n81\n0.0398849\n0.289696\n\n\n1\n82\n0.0383449\n0.338111\n\n\n1\n83\n0.0303048\n0.330449\n\n\n1\n84\n0.0347259\n0.362525\n\n\n1\n85\n0.0365730\n0.325395\n\n\n1\n86\n0.0347524\n0.326062\n\n\n1\n87\n0.0356036\n0.298270\n\n\n3\n81\n0.0163921\n0.202899\n\n\n3\n82\n0.0190651\n0.162218\n\n\n\n 9 rows out of 630. \"Prob. of Arrest\" is estimated probability of being arrested when you commit a crime"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#between-and-within-1",
    "href": "2023/weeks/week08/slides11.html#between-and-within-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nIf we look at the overall variation, just pretending this is all together, we get this"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#between-and-within-2",
    "href": "2023/weeks/week08/slides11.html#between-and-within-2",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nBETWEEN variation is what we get if we look at the relationship between the means of each county"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#between-and-within-3",
    "href": "2023/weeks/week08/slides11.html#between-and-within-3",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nAnd I mean it! Only look at those means! The individual year-to-year variation within county doesn’t matter."
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#between-and-within-4",
    "href": "2023/weeks/week08/slides11.html#between-and-within-4",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nWithin variation goes the other way - it treats those orange crosses as their own individualized sets of axes and looks at variation within county from year-to-year only!\nWe basically slide the crosses over on top of each other and then analyze that data"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#between-and-within-5",
    "href": "2023/weeks/week08/slides11.html#between-and-within-5",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nWe can clearly see that between counties there’s a strong positive relationship\nBut if you look within a given county, the relationship isn’t that strong, and actually seems to be negative\nWhich would make sense - if you think your chances of getting arrested are high, that should be a deterrent to crime\nBut what are we actually doing here? Let’s think about the causal diagram / data-generating process!\nWhat goes into the probability of arrest and the crime rate? Lots of stuff!"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#the-crime-rate",
    "href": "2023/weeks/week08/slides11.html#the-crime-rate",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Crime Rate",
    "text": "The Crime Rate\n\n“LocalStuff” is just all the things unique to that area\n“LawAndOrder” is how committed local politicians are to “Law and Order Politics”"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#between-and-within-6",
    "href": "2023/weeks/week08/slides11.html#between-and-within-6",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nFor each of these variables we can ask if they vary between groups and/or within groups\nLocalStuff is all the stuff unique to that county - geography, landmarks, the quality of the schools, almost by definition this only varies between groups. It’s not like the things that make your county unique are different each year (or at least not very different)\nWhether the county has LawAndOrder and how many CivilRights you’re allowed might change a bit year to year, but in general, political climates like that change pretty slowly. At a bit of a stretch we can call that something that only varies between groups too\nPolice budgets (and thus number of police on the streets) and Poverty (which varies with the economy) vary both between counties, but also within counties from year to year\nVariables with between variation only (by our assumption): LocalStuff, LawAndOrder, CivilRights\nVariables with both between and within variation: Police, Poverty"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#between-and-within-7",
    "href": "2023/weeks/week08/slides11.html#between-and-within-7",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nLet’s simplify our graph!\nSome of the variables only vary between counties\nSo, we can replace those variables on the graph with the variable County\nRight? That’s where all the variation is anyway"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#the-crime-rate-1",
    "href": "2023/weeks/week08/slides11.html#the-crime-rate-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Crime Rate",
    "text": "The Crime Rate\n\n“LocalStuff” is just all the things unique to that area\n“LawAndOrder” is how committed local politicians are to “Law and Order Politics”"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#between-and-within-8",
    "href": "2023/weeks/week08/slides11.html#between-and-within-8",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nNow the task of identifying ProbArrest \\(\\rightarrow\\) CrimeRate becomes much simpler!\nIf we control for County, that will close a lot of back doors for us\n(based on the diagram, all we need to control for is County and Poverty!)\nConveniently, we can control for County just like it was any other variable!\nAnd when we do, we automatically control for all variables that only have between variation, whatever they are, even if we can’t measure them directly or didn’t think about them\nAll that’s left is the within variation"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#concept-checks",
    "href": "2023/weeks/week08/slides11.html#concept-checks",
    "title": "🗓️ Week 8  Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nFor each of these variables, would we expect them to have within variation, between variation, or both?\n(Individual = person) How a child’s height changes as they age.\n(Individual = person) In a data set tracking many people over many years, the variation in the number of children a person has in a given year.\n(Individual = city) Overall, Paris, France has more restaurants than Paris, Texas.\n(Individual = genre) The average pop music album sells more copies than the average jazz album\n(Individual = genre) Miles Davis’ Kind of Blue sold very well for a jazz album.\n(Individual = genre) Michael Jackson’s Thriller, a pop album, sold many more copies than Kind of Blue, a jazz album."
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#removing-between-variation",
    "href": "2023/weeks/week08/slides11.html#removing-between-variation",
    "title": "🗓️ Week 8  Within variation",
    "section": "Removing Between Variation",
    "text": "Removing Between Variation\n\nOkay so that’s the concept\nRemove all the between variation so that all that’s left is within variation\nAnd in the process control for any variables that are made up only of between variation\nHow can we actually do this? And what’s really going on?\nLet’s first talk about the regression model itself that this implies\nThen let’s actually do the thing. There are two main ways: de-meaning and binary variables (they give the same result, for balanced panels anyway)"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#estimation-vs.-design",
    "href": "2023/weeks/week08/slides11.html#estimation-vs.-design",
    "title": "🗓️ Week 8  Within variation",
    "section": "Estimation vs. Design",
    "text": "Estimation vs. Design\n\nTo be clear, this is exactly 0% different from what we’ve done before in terms of controlling for stuff\nAnd in fact we’re about to do the exact same thing we did before by just adding a categorical control variable for county or whatever\n(and in fact the “within” thing holds with other categorical controls - a categorical control for education isolates variation “within education levels”)\nThe difference is the reason we’re doing it. It’s fixed effects because a categorical control for individual controls for a lot of stuff, and we think closes a lot of back doors for us, not just one, and not just ones we can measure"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#the-model",
    "href": "2023/weeks/week08/slides11.html#the-model",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Model",
    "text": "The Model\nThe \\(it\\) subscript says this variable varies over individual \\(i\\) and time \\(t\\)\n\\[Y_{it} = \\beta_0 + \\beta_1 X_{it} + \\varepsilon_{it}\\]\n\n\\(X_{it}\\) is related to LocalStuff which is not in the model and thus in the error term!\nRegular ol’ omitted variable bias. If we don’t adjust for the individual effect, we get a biased \\(\\hat{\\beta}_1\\)\n(this bias is called “pooling bias” although it’s really just a form of omitted variable bias)\nWe really have this then: \\[Y_{it} = \\beta_0 + \\beta_1 X_{it} + (\\alpha_i + \\varepsilon_{it})\\]"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#de-meaning",
    "href": "2023/weeks/week08/slides11.html#de-meaning",
    "title": "🗓️ Week 8  Within variation",
    "section": "De-meaning",
    "text": "De-meaning\n\nLet’s do de-meaning first, since it’s most closely and obviously related to the “removing between variation” explanation we’ve been going for\nThe process here is simple!\n\n\nFor each variable \\(X_{it}\\), \\(Y_{it}\\), etc., get the mean value of that variable for each individual \\(\\bar{X}_i, \\bar{Y}_i\\)\nSubtract out that mean to get residuals \\((X_{it} - \\bar{X}_i), (Y_{it} - \\bar{Y}_i)\\)\nWork with those residuals\n\n\nThat’s it!"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#how-does-this-work",
    "href": "2023/weeks/week08/slides11.html#how-does-this-work",
    "title": "🗓️ Week 8  Within variation",
    "section": "How does this work?",
    "text": "How does this work?\n\nThat \\(\\alpha_i\\) term gets absorbed\nThe residuals are, by construction, no longer related to the \\(\\alpha_i\\), so it no longer goes in the residuals!\n\n\\[(Y_{it} - \\bar{Y}_i) = \\beta_0 + \\beta_1(X_{it} - \\bar{X}_i) + \\varepsilon_{it}\\]"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#lets-do-it",
    "href": "2023/weeks/week08/slides11.html#lets-do-it",
    "title": "🗓️ Week 8  Within variation",
    "section": "Let’s do it!",
    "text": "Let’s do it!\n\nWe can use group_by to get means-within-groups and subtract them out\n\n\ndata(crime4, package = 'wooldridge')\ncrime4 &lt;- crime4 %&gt;%\n  ## Filter to the data points from our graph\n  filter(county %in% c(1,3,7, 23),\n         prbarr &lt; .5) %&gt;%\n  group_by(county) %&gt;%\n  mutate(mean_crime = mean(crmrte),\n         mean_prob = mean(prbarr)) %&gt;%\n  mutate(demeaned_crime = crmrte - mean_crime,\n         demeaned_prob = prbarr - mean_prob)"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#and-regress",
    "href": "2023/weeks/week08/slides11.html#and-regress",
    "title": "🗓️ Week 8  Within variation",
    "section": "And Regress!",
    "text": "And Regress!\n\norig_data &lt;- feols(crmrte ~ prbarr, data = crime4)\nde_mean &lt;- feols(demeaned_crime ~ demeaned_prob, data = crime4)\netable(orig_data, de_mean)\n\n                        orig_data           de_mean\nDependent Var.:            crmrte    demeaned_crime\n                                                   \nConstant         0.0118* (0.0050) 1.41e-18 (0.0004)\nprbarr          0.0486** (0.0167)                  \ndemeaned_prob                     -0.0305* (0.0117)\n_______________ _________________ _________________\nS.E. type                     IID               IID\nObservations                   27                27\nR2                        0.25308           0.21445\nAdj. R2                   0.22321           0.18303\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#interpreting-a-within-relationship",
    "href": "2023/weeks/week08/slides11.html#interpreting-a-within-relationship",
    "title": "🗓️ Week 8  Within variation",
    "section": "Interpreting a Within Relationship",
    "text": "Interpreting a Within Relationship\n\nHow can we interpret that slope of -0.03?\nThis is all within variation so our interpretation must be within-county\nSo, “comparing a county in year A where its arrest probability is 1 (100 percentage points) higher than it is in year B, we expect the number of crimes per person to drop by .03”\nOr if we think we’ve causally identified it (and want to work on a more realistic scale), “raising the arrest probability by 1 percentage point in a county reduces the number of crimes per person in that county by .0003”.\nWe’re basically “controlling for county” (and will do that explicitly in a moment)\nSo your interpretation should think of it in that way - holding county constant i.e. comparing two observations with the same value of county i.e. comparing a county to itself at a different point in time"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#concept-checks-1",
    "href": "2023/weeks/week08/slides11.html#concept-checks-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy does subtracting the within-individual mean of each variable “control for individual”?\nIn a sentence, interpret the slope coefficient in the estimated model \\((Y_{it} - \\bar{Y}_i) = 2 + 3(X_{it} - \\bar{X}_i)\\) where \\(Y\\) is “blood pressure”, \\(X\\) is “stress at work”, and \\(i\\) is an individual person"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#the-least-squares-dummy-variable-approach",
    "href": "2023/weeks/week08/slides11.html#the-least-squares-dummy-variable-approach",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Least Squares Dummy Variable Approach",
    "text": "The Least Squares Dummy Variable Approach\n\nDe-meaning the data isn’t the only way to do it!\nYou can also use the least squares dummy variable (another word for “binary variable”) method\nWe just treat “individual” like the categorical variable it is and add it as a control! Again, the regression approach is exactly the same as with any categorical control, but the research design reason for doing it is different"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#lets-do-it-1",
    "href": "2023/weeks/week08/slides11.html#lets-do-it-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Let’s do it!",
    "text": "Let’s do it!\n\nlsdv &lt;- feols(crmrte ~ prbarr + factor(county), data = crime4)\netable(orig_data, de_mean, lsdv, keep = c('prbarr', 'demeaned_prob'))\n\n                        orig_data           de_mean              lsdv\nDependent Var.:            crmrte    demeaned_crime            crmrte\n                                                                     \nprbarr          0.0486** (0.0167)                   -0.0305* (0.0124)\ndemeaned_prob                     -0.0305* (0.0117)                  \n_______________ _________________ _________________ _________________\nS.E. type                     IID               IID               IID\nObservations                   27                27                27\nR2                        0.25308           0.21445           0.94114\nAdj. R2                   0.22321           0.18303           0.93044\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#the-same",
    "href": "2023/weeks/week08/slides11.html#the-same",
    "title": "🗓️ Week 8  Within variation",
    "section": "The same!",
    "text": "The same!\n\nThe result is the same, as it should be\nExcept for that \\(R^2\\) - What is that “within R2”?\nBecause de-meaning takes out the part explained by the fixed effects ( \\(\\alpha_i\\) ) before running the regression, while LSDV does it in the regression\nSo the .94 is the portion of crmrte explained by prbarr and county, whereas the .21 is the “within - \\(R^2\\)” - the portion of the within variation that’s explained by prbarr\nNeither is wrong (and the .94 isn’t “better”), they’re just measuring different things"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#why-lsdv",
    "href": "2023/weeks/week08/slides11.html#why-lsdv",
    "title": "🗓️ Week 8  Within variation",
    "section": "Why LSDV?",
    "text": "Why LSDV?\n\nA benefit of the LSDV approach is that it calculates the fixed effects \\(\\alpha_i\\) for you\nWe left those out of the table with the coefs argument of export_summs (we rarely want them) but here they are:\n\n\n\nOLS estimation, Dep. Var.: crmrte\nObservations: 27 \nStandard-errors: IID \n                  Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)       0.045631   0.004116  11.08640 1.7906e-10 ***\nprbarr           -0.030491   0.012442  -2.45068 2.2674e-02 *  \nfactor(county)3  -0.025308   0.002165 -11.68996 6.5614e-11 ***\nfactor(county)7  -0.009870   0.001418  -6.96313 5.4542e-07 ***\nfactor(county)23 -0.008587   0.001258  -6.82651 7.3887e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.001933   Adj. R2: 0.930441\n\n\n\nInterpretation is exactly the same as with a categorical variable - we have an omitted county, and these show the difference relative to that omitted county"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#why-lsdv-1",
    "href": "2023/weeks/week08/slides11.html#why-lsdv-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Why LSDV?",
    "text": "Why LSDV?\n\nThis also makes clear another element of what’s happening! Just like with a categorical var, the line is moving up and down to meet the counties\nGraphically, de-meaning moves all the points together in the middle to draw a line, while LSDV moves the line up and down to meet the points"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#why-not-lsdv",
    "href": "2023/weeks/week08/slides11.html#why-not-lsdv",
    "title": "🗓️ Week 8  Within variation",
    "section": "Why Not LSDV?",
    "text": "Why Not LSDV?\n\nLSDV is computationally expensive\nIf there are a lot of individuals, or big data, or if you have many sets of fixed effects (yes you can do more than just individual - we’ll get to that next time!), it can be very slow\nMost professionally made fixed-effects commands use de-meaning, but then adjust the standard errors properly\n(They also leave the fixed effects coefficients off the regression table by default)"
  },
  {
    "objectID": "2023/weeks/week08/slides11.html#concept-checks-2",
    "href": "2023/weeks/week08/slides11.html#concept-checks-2",
    "title": "🗓️ Week 8  Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy can’t we use individual-person fixed effects to study the impact of race on traffic stops?\nThe within \\(R^2\\) from is .3, and the overall \\(R^2\\) is .5. Interpret these two numbers in sentences\nIn a sentence, interpret the slope coefficient in the estimated model \\((Y_{it} - \\bar{Y}_i) = 1 + .5(X_{it} - \\bar{X}_i)\\) where \\(Y\\) is “school funding per child” and \\(X\\) is “population growth”, and \\(i\\) is city"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#a-pickle",
    "href": "2023/weeks/week08/slides.html#a-pickle",
    "title": "🗓️ Week 8  Within variation",
    "section": "A Pickle",
    "text": "A Pickle\n\nSo obviously this is a problem, and it’s not one we can reason or trick our way out of\nIf we don’t have the variable we need to control for, we don’t have it\n… or do we?"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#the-rest-of-the-term",
    "href": "2023/weeks/week08/slides.html#the-rest-of-the-term",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Rest of the Term",
    "text": "The Rest of the Term\n\nMuch of the rest of the term is going to be focused on finding ways to control for stuff that we can’t measure\nSeems impossible! But it is possible, at least in some circumstances\nToday, we will be talking about within variation and between variation, and the ability to control for all between variation using fixed effects"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#panel-data-1",
    "href": "2023/weeks/week08/slides.html#panel-data-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Panel Data",
    "text": "Panel Data\n\nHere’s what (a few rows from) a panel data set looks like - a variable for individual (county), a variable for time (year), and then the data\n\n\n\n\n\n\nCounty\nYear\nCrimeRate\nProbofArrest\n\n\n\n\n1\n81\n0.0398849\n0.289696\n\n\n1\n82\n0.0383449\n0.338111\n\n\n1\n83\n0.0303048\n0.330449\n\n\n1\n84\n0.0347259\n0.362525\n\n\n1\n85\n0.0365730\n0.325395\n\n\n1\n86\n0.0347524\n0.326062\n\n\n1\n87\n0.0356036\n0.298270\n\n\n3\n81\n0.0163921\n0.202899\n\n\n3\n82\n0.0190651\n0.162218\n\n\n\n 9 rows out of 630. \"Prob. of Arrest\" is estimated probability of being arrested when you commit a crime"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#between-and-within-1",
    "href": "2023/weeks/week08/slides.html#between-and-within-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nIf we look at the overall variation, just pretending this is all together, we get this"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#between-and-within-2",
    "href": "2023/weeks/week08/slides.html#between-and-within-2",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nBETWEEN variation is what we get if we look at the relationship between the means of each county"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#between-and-within-3",
    "href": "2023/weeks/week08/slides.html#between-and-within-3",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nAnd I mean it! Only look at those means! The individual year-to-year variation within county doesn’t matter."
  },
  {
    "objectID": "2023/weeks/week08/slides.html#between-and-within-4",
    "href": "2023/weeks/week08/slides.html#between-and-within-4",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nWithin variation goes the other way - it treats those orange crosses as their own individualized sets of axes and looks at variation within county from year-to-year only!\nWe basically slide the crosses over on top of each other and then analyze that data"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#between-and-within-5",
    "href": "2023/weeks/week08/slides.html#between-and-within-5",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nWe can clearly see that between counties there’s a strong positive relationship\nBut if you look within a given county, the relationship isn’t that strong, and actually seems to be negative\nWhich would make sense - if you think your chances of getting arrested are high, that should be a deterrent to crime\nBut what are we actually doing here? Let’s think about the causal diagram / data-generating process!\nWhat goes into the probability of arrest and the crime rate? Lots of stuff!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#the-crime-rate",
    "href": "2023/weeks/week08/slides.html#the-crime-rate",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Crime Rate",
    "text": "The Crime Rate\n\n“LocalStuff” is just all the things unique to that area\n“LawAndOrder” is how committed local politicians are to “Law and Order Politics”"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#between-and-within-6",
    "href": "2023/weeks/week08/slides.html#between-and-within-6",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nFor each of these variables we can ask if they vary between groups and/or within groups\nLocalStuff is all the stuff unique to that county - geography, landmarks, the quality of the schools, almost by definition this only varies between groups. It’s not like the things that make your county unique are different each year (or at least not very different)\nWhether the county has LawAndOrder and how many CivilRights you’re allowed might change a bit year to year, but in general, political climates like that change pretty slowly. At a bit of a stretch we can call that something that only varies between groups too\nPolice budgets (and thus number of police on the streets) and Poverty (which varies with the economy) vary both between counties, but also within counties from year to year\nVariables with between variation only (by our assumption): LocalStuff, LawAndOrder, CivilRights\nVariables with both between and within variation: Police, Poverty"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#between-and-within-7",
    "href": "2023/weeks/week08/slides.html#between-and-within-7",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nLet’s simplify our graph!\nSome of the variables only vary between counties\nSo, we can replace those variables on the graph with the variable County\nRight? That’s where all the variation is anyway"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#the-crime-rate-1",
    "href": "2023/weeks/week08/slides.html#the-crime-rate-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Crime Rate",
    "text": "The Crime Rate\n\n“LocalStuff” is just all the things unique to that area\n“LawAndOrder” is how committed local politicians are to “Law and Order Politics”"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#between-and-within-8",
    "href": "2023/weeks/week08/slides.html#between-and-within-8",
    "title": "🗓️ Week 8  Within variation",
    "section": "Between and Within",
    "text": "Between and Within\n\nNow the task of identifying ProbArrest \\(\\rightarrow\\) CrimeRate becomes much simpler!\nIf we control for County, that will close a lot of back doors for us\n(based on the diagram, all we need to control for is County and Poverty!)\nConveniently, we can control for County just like it was any other variable!\nAnd when we do, we automatically control for all variables that only have between variation, whatever they are, even if we can’t measure them directly or didn’t think about them\nAll that’s left is the within variation"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#removing-between-variation",
    "href": "2023/weeks/week08/slides.html#removing-between-variation",
    "title": "🗓️ Week 8  Within variation",
    "section": "Removing Between Variation",
    "text": "Removing Between Variation\n\nOkay so that’s the concept\nRemove all the between variation so that all that’s left is within variation\nAnd in the process control for any variables that are made up only of between variation\nHow can we actually do this? And what’s really going on?\nLet’s first talk about the regression model itself that this implies\nThen let’s actually do the thing. There are two main ways: de-meaning and binary variables (they give the same result, for balanced panels anyway)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#estimation-vs.-design",
    "href": "2023/weeks/week08/slides.html#estimation-vs.-design",
    "title": "🗓️ Week 8  Within variation",
    "section": "Estimation vs. Design",
    "text": "Estimation vs. Design\n\nTo be clear, this is exactly 0% different from what we’ve done before in terms of controlling for stuff\nAnd in fact we’re about to do the exact same thing we did before by just adding a categorical control variable for county or whatever\n(and in fact the “within” thing holds with other categorical controls - a categorical control for education isolates variation “within education levels”)\nThe difference is the reason we’re doing it. It’s fixed effects because a categorical control for individual controls for a lot of stuff, and we think closes a lot of back doors for us, not just one, and not just ones we can measure"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#the-model",
    "href": "2023/weeks/week08/slides.html#the-model",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Model",
    "text": "The Model\nThe \\(it\\) subscript says this variable varies over individual \\(i\\) and time \\(t\\)\n\\[Y_{it} = \\beta_0 + \\beta_1 X_{it} + \\varepsilon_{it}\\]\n\n\\(X_{it}\\) is related to LocalStuff which is not in the model and thus in the error term!\nRegular ol’ omitted variable bias. If we don’t adjust for the individual effect, we get a biased \\(\\hat{\\beta}_1\\)\n(this bias is called “pooling bias” although it’s really just a form of omitted variable bias)\nWe really have this then: \\[Y_{it} = \\beta_0 + \\beta_1 X_{it} + (\\alpha_i + \\varepsilon_{it})\\]"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#de-meaning",
    "href": "2023/weeks/week08/slides.html#de-meaning",
    "title": "🗓️ Week 8  Within variation",
    "section": "De-meaning",
    "text": "De-meaning\n\nLet’s do de-meaning first, since it’s most closely and obviously related to the “removing between variation” explanation we’ve been going for\nThe process here is simple!\n\n\nFor each variable \\(X_{it}\\), \\(Y_{it}\\), etc., get the mean value of that variable for each individual \\(\\bar{X}_i, \\bar{Y}_i\\)\nSubtract out that mean to get residuals \\((X_{it} - \\bar{X}_i), (Y_{it} - \\bar{Y}_i)\\)\nWork with those residuals\n\n\nThat’s it!"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#how-does-this-work",
    "href": "2023/weeks/week08/slides.html#how-does-this-work",
    "title": "🗓️ Week 8  Within variation",
    "section": "How does this work?",
    "text": "How does this work?\n\nThat \\(\\alpha_i\\) term gets absorbed\nThe residuals are, by construction, no longer related to the \\(\\alpha_i\\), so it no longer goes in the residuals!\n\n\\[(Y_{it} - \\bar{Y}_i) = \\beta_0 + \\beta_1(X_{it} - \\bar{X}_i) + \\varepsilon_{it}\\]"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#lets-do-it",
    "href": "2023/weeks/week08/slides.html#lets-do-it",
    "title": "🗓️ Week 8  Within variation",
    "section": "Let’s do it!",
    "text": "Let’s do it!\n\nWe can use group_by to get means-within-groups and subtract them out\n\n\ndata(crime4, package = 'wooldridge')\ncrime4 &lt;- crime4 %&gt;%\n  ## Filter to the data points from our graph\n  filter(county %in% c(1,3,7, 23),\n         prbarr &lt; .5) %&gt;%\n  group_by(county) %&gt;%\n  mutate(mean_crime = mean(crmrte),\n         mean_prob = mean(prbarr)) %&gt;%\n  mutate(demeaned_crime = crmrte - mean_crime,\n         demeaned_prob = prbarr - mean_prob)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#and-regress",
    "href": "2023/weeks/week08/slides.html#and-regress",
    "title": "🗓️ Week 8  Within variation",
    "section": "And Regress!",
    "text": "And Regress!\n\norig_data &lt;- feols(crmrte ~ prbarr, data = crime4)\nde_mean &lt;- feols(demeaned_crime ~ demeaned_prob, data = crime4)\netable(orig_data, de_mean)\n\n                        orig_data           de_mean\nDependent Var.:            crmrte    demeaned_crime\n                                                   \nConstant         0.0118* (0.0050) 1.41e-18 (0.0004)\nprbarr          0.0486** (0.0167)                  \ndemeaned_prob                     -0.0305* (0.0117)\n_______________ _________________ _________________\nS.E. type                     IID               IID\nObservations                   27                27\nR2                        0.25308           0.21445\nAdj. R2                   0.22321           0.18303\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#interpreting-a-within-relationship",
    "href": "2023/weeks/week08/slides.html#interpreting-a-within-relationship",
    "title": "🗓️ Week 8  Within variation",
    "section": "Interpreting a Within Relationship",
    "text": "Interpreting a Within Relationship\n\nHow can we interpret that slope of -0.03?\nThis is all within variation so our interpretation must be within-county\nSo, “comparing a county in year A where its arrest probability is 1 (100 percentage points) higher than it is in year B, we expect the number of crimes per person to drop by .03”\nOr if we think we’ve causally identified it (and want to work on a more realistic scale), “raising the arrest probability by 1 percentage point in a county reduces the number of crimes per person in that county by .0003”.\nWe’re basically “controlling for county” (and will do that explicitly in a moment)\nSo your interpretation should think of it in that way - holding county constant i.e. comparing two observations with the same value of county i.e. comparing a county to itself at a different point in time"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#the-least-squares-dummy-variable-approach",
    "href": "2023/weeks/week08/slides.html#the-least-squares-dummy-variable-approach",
    "title": "🗓️ Week 8  Within variation",
    "section": "The Least Squares Dummy Variable Approach",
    "text": "The Least Squares Dummy Variable Approach\n\nDe-meaning the data isn’t the only way to do it!\nYou can also use the least squares dummy variable (another word for “binary variable”) method\nWe just treat “individual” like the categorical variable it is and add it as a control! Again, the regression approach is exactly the same as with any categorical control, but the research design reason for doing it is different"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#lets-do-it-1",
    "href": "2023/weeks/week08/slides.html#lets-do-it-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Let’s do it!",
    "text": "Let’s do it!\n\nlsdv &lt;- feols(crmrte ~ prbarr + factor(county), data = crime4)\netable(orig_data, de_mean, lsdv, keep = c('prbarr', 'demeaned_prob'))\n\n                        orig_data           de_mean              lsdv\nDependent Var.:            crmrte    demeaned_crime            crmrte\n                                                                     \nprbarr          0.0486** (0.0167)                   -0.0305* (0.0124)\ndemeaned_prob                     -0.0305* (0.0117)                  \n_______________ _________________ _________________ _________________\nS.E. type                     IID               IID               IID\nObservations                   27                27                27\nR2                        0.25308           0.21445           0.94114\nAdj. R2                   0.22321           0.18303           0.93044\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#the-same",
    "href": "2023/weeks/week08/slides.html#the-same",
    "title": "🗓️ Week 8  Within variation",
    "section": "The same!",
    "text": "The same!\n\nThe result is the same, as it should be\nExcept for that \\(R^2\\) - What is that “within R2”?\nBecause de-meaning takes out the part explained by the fixed effects ( \\(\\alpha_i\\) ) before running the regression, while LSDV does it in the regression\nSo the .94 is the portion of crmrte explained by prbarr and county, whereas the .21 is the “within - \\(R^2\\)” - the portion of the within variation that’s explained by prbarr\nNeither is wrong (and the .94 isn’t “better”), they’re just measuring different things"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#why-lsdv",
    "href": "2023/weeks/week08/slides.html#why-lsdv",
    "title": "🗓️ Week 8  Within variation",
    "section": "Why LSDV?",
    "text": "Why LSDV?\n\nA benefit of the LSDV approach is that it calculates the fixed effects \\(\\alpha_i\\) for you\nWe left those out of the table with the coefs argument of export_summs (we rarely want them) but here they are:\n\n\n\nOLS estimation, Dep. Var.: crmrte\nObservations: 27 \nStandard-errors: IID \n                  Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)       0.045631   0.004116  11.08640 1.7906e-10 ***\nprbarr           -0.030491   0.012442  -2.45068 2.2674e-02 *  \nfactor(county)3  -0.025308   0.002165 -11.68996 6.5614e-11 ***\nfactor(county)7  -0.009870   0.001418  -6.96313 5.4542e-07 ***\nfactor(county)23 -0.008587   0.001258  -6.82651 7.3887e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.001933   Adj. R2: 0.930441\n\n\n\nInterpretation is exactly the same as with a categorical variable - we have an omitted county, and these show the difference relative to that omitted county"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#why-lsdv-1",
    "href": "2023/weeks/week08/slides.html#why-lsdv-1",
    "title": "🗓️ Week 8  Within variation",
    "section": "Why LSDV?",
    "text": "Why LSDV?\n\nThis also makes clear another element of what’s happening! Just like with a categorical var, the line is moving up and down to meet the counties\nGraphically, de-meaning moves all the points together in the middle to draw a line, while LSDV moves the line up and down to meet the points"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#why-not-lsdv",
    "href": "2023/weeks/week08/slides.html#why-not-lsdv",
    "title": "🗓️ Week 8  Within variation",
    "section": "Why Not LSDV?",
    "text": "Why Not LSDV?\n\nLSDV is computationally expensive\nIf there are a lot of individuals, or big data, or if you have many sets of fixed effects (yes you can do more than just individual - we’ll get to that next time!), it can be very slow\nMost professionally made fixed-effects commands use de-meaning, but then adjust the standard errors properly\n(They also leave the fixed effects coefficients off the regression table by default)"
  },
  {
    "objectID": "2023/weeks/week08/slides.html#concept-checks-2",
    "href": "2023/weeks/week08/slides.html#concept-checks-2",
    "title": "🗓️ Week 8  Within variation",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy can’t we use individual-person fixed effects to study the impact of race on traffic stops?\nThe within \\(R^2\\) from is .3, and the overall \\(R^2\\) is .5. Interpret these two numbers in sentences\nIn a sentence, interpret the slope coefficient in the estimated model \\((Y_{it} - \\bar{Y}_i) = 1 + .5(X_{it} - \\bar{X}_i)\\) where \\(Y\\) is “school funding per child” and \\(X\\) is “population growth”, and \\(i\\) is city\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#bias-and-the-error-term",
    "href": "2023/weeks/week07/slides.html#bias-and-the-error-term",
    "title": "Binary Variables and Functional Form",
    "section": "Bias and the Error Term",
    "text": "Bias and the Error Term\n\nAll of the nice stuff we’ve gotten so far makes some assumptions about our true model\n\n\\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\n\nIn particular, we’ve made some assumptions about the error term \\(\\varepsilon\\)\nSo what is that error term exactly, and what are we assuming about it?"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-error-term",
    "href": "2023/weeks/week07/slides.html#the-error-term",
    "title": "Binary Variables and Functional Form",
    "section": "The Error Term",
    "text": "The Error Term\n\nThe error term contains everything that isn’t in our model\nIf \\(Y\\) were a pure function of \\(X\\), for example if \\(Y\\) was “height in feet” and \\(X\\) was “height in inches”, we wouldn’t have an error term, because a straight line fully describes the relationship perfectly with no variation\nBut in most cases, the line is a simplification - we’re leaving other stuff out! That’s in the error term"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-error-term-1",
    "href": "2023/weeks/week07/slides.html#the-error-term-1",
    "title": "Binary Variables and Functional Form",
    "section": "The Error Term",
    "text": "The Error Term\n\nConsider this data generating process:\n\n\\[ ClassGrade = \\beta_0 + \\beta_1 StudyTime + \\varepsilon \\]\n\nSurely StudyTime isn’t the only thing that determines your ClassGrade\nEverything else is in the error term!\nProfessorLeniency, InterestInSubject, Intelligence, and so on and so on…"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-error-term-2",
    "href": "2023/weeks/week07/slides.html#the-error-term-2",
    "title": "Binary Variables and Functional Form",
    "section": "The Error Term",
    "text": "The Error Term\n\nIsn’t that really bad? We’re leaving out a bunch of important stuff!\nIf you want to predict \\(Y\\) as accurately as possible then we’re probably going to do a bad job of it\nBut if our real interest is *figuring out the relationship between \\(X\\) and \\(Y\\), then it’s fine to leave stuff out, as long as whatever’s left in the error term obeys a few important assumptions"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#error-term-assumptions",
    "href": "2023/weeks/week07/slides.html#error-term-assumptions",
    "title": "Binary Variables and Functional Form",
    "section": "Error Term Assumptions",
    "text": "Error Term Assumptions\n\nThe most important assumption about the error term is that it is unrelated to \\(X\\)\nIf \\(X\\) and \\(\\varepsilon\\) are correlated, \\(\\hat{\\beta}_1\\) will be biased - its distribution no longer has the true \\(\\beta_1\\) as its mean\nIn these cases we can say ” \\(X\\) is endogenous” or “we have omitted variable bias”\nNo amount of additional sample size will fix that problem!\n(what will fix the problem? We’ll get to that one later)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#omitted-variable-bias",
    "href": "2023/weeks/week07/slides.html#omitted-variable-bias",
    "title": "Binary Variables and Functional Form",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\n\nWe can intuitively think about whether omitted variable bias is likely to make our estimates too high or too low\nThe sign of the bias will be the sign of the relationship between the omitted variable and \\(X\\), times the sign of the relationship between the omitted variable bias and \\(Y\\)\nInterestInSubject is positively related to both StudyTime and ClassGrade, and \\(+\\times+ = +\\), so our estimates are positively biased / too high\nMore precisely we have that the mean of the \\(\\hat{\\beta}_1\\) sampling distribution is \\[\\beta_1 + corr(X,\\varepsilon)\\frac{\\sigma_\\varepsilon}{\\sigma_X}\\]"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#thinking-through-this-bias",
    "href": "2023/weeks/week07/slides.html#thinking-through-this-bias",
    "title": "Binary Variables and Functional Form",
    "section": "Thinking Through this Bias",
    "text": "Thinking Through this Bias\n\nIf \\(Z\\) hangs around \\(X\\), but \\(Y\\) doesn’t know about it, then the coefficient on \\(X\\) will get all the credit for \\(Z\\)\nThis is a good way to keep that “direction of bias” problem in mind\nAnd you do want to keep it in mind! This is important for understanding general correlations you see in the wild, too\nAnd helps keep in line some things - for example, if \\(Z\\) is unrelated to \\(X\\), it won’t bias you!!"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#thinking-through-this-bias-1",
    "href": "2023/weeks/week07/slides.html#thinking-through-this-bias-1",
    "title": "Binary Variables and Functional Form",
    "section": "Thinking Through this Bias",
    "text": "Thinking Through this Bias\n\nThat means that when you’re thinking about the controls you need, that only includes things related to \\(X\\)\nAdding controls for things related to \\(Y\\) not \\(X\\) can make the model predict better and reduce standard errors, but won’t remove omitted variable bias"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#less-serious-error-concerns",
    "href": "2023/weeks/week07/slides.html#less-serious-error-concerns",
    "title": "Binary Variables and Functional Form",
    "section": "Less Serious Error Concerns",
    "text": "Less Serious Error Concerns\n\nOmitted variable bias can, well, bias us, which is very bad\nThere are some other assumptions that can fail that may also pose a problem to us but less so\nWe’ve assumed so far not just that \\(\\varepsilon\\) is unrelated to \\(X\\), but also that the variance of \\(\\varepsilon\\) is unrelated to \\(X\\), and that the \\(\\varepsilon\\)s are unrelated to each other\nIf these assumptions fail, our standard errors will be wrong, but we won’t be biased, and also there are ways to fix the standard errors\nWe will cover these only briefly, they’ll come back later"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#heteroskedasticity",
    "href": "2023/weeks/week07/slides.html#heteroskedasticity",
    "title": "Binary Variables and Functional Form",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nIf the variance of the error term is different for different values of \\(X\\), then we have “heteroskedasticity”\nNotice in the below graph how the spread of the points around the line (the variance of the error term) is bigger on the right than the left"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#heteroskedasticity-1",
    "href": "2023/weeks/week07/slides.html#heteroskedasticity-1",
    "title": "Binary Variables and Functional Form",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nWe can correct for this using heteroskedasticity-robust standard errors which sort of “squash down” the big variances and then re-estimate the standard errors\nWe can do this in feols with vcov = 'hetero' in R or with “robust” in STATA"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#correlated-errors",
    "href": "2023/weeks/week07/slides.html#correlated-errors",
    "title": "Binary Variables and Functional Form",
    "section": "Correlated Errors",
    "text": "Correlated Errors\n\nIf the error terms are correlated with each other, then our standard errors will also be wrong\nHow could this happen? For example, maybe you’ve surveyed a bunch of people in different towns - the error terms within a town might be clustered\nOr maybe you have time series data. If a term in the error term is “sticky” or has “momentum” it will likely be a similar error term a few time periods in a row, beign correlated across time, or autocorrelated\nAgain, this doesn’t bias \\(\\hat{\\beta}_1\\) but it can affect standard errors!"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#the-right-hand-side-1",
    "href": "2023/weeks/week07/slides.html#the-right-hand-side-1",
    "title": "Binary Variables and Functional Form",
    "section": "The Right Hand Side",
    "text": "The Right Hand Side\nWe will look at three features of the right-hand side\n\nWhat if the variable is categorical or binary? (binary variables)\nWhat if the variable has a nonlinear effect on \\(Y\\) (polynomials and logarithms)\nWhat if the effect of one variable depends on the value of another variable? (interaction terms)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#binary-data-1",
    "href": "2023/weeks/week07/slides.html#binary-data-1",
    "title": "Binary Variables and Functional Form",
    "section": "Binary Data",
    "text": "Binary Data\n\nA variable is binary if it only has two values - 0 or 1 (or “No” or “Yes”, etc.)\nBinary variables are super common in econometrics!\nDid you get the treatment? Yes / No\nDo you live in the US? Yes / No\nIs a floating exchange rate in effect? Yes / No"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#comparison-of-means",
    "href": "2023/weeks/week07/slides.html#comparison-of-means",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nWhen a binary variable is an independent variable, what we are often interested in doing is comparing means\nIs mean income higher inside the US or outside?\nIs mean height higher for kids who got a nutrition supplement or those who didn’t?\nIs mean GDP growth higher with or without a floating exchange rate?"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#comparison-of-means-1",
    "href": "2023/weeks/week07/slides.html#comparison-of-means-1",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nLet’s compare log earnings in 1993 between married people 30 or older vs. never-married people 30 or older\nSeems to be a slight favor to the married men\n\n\ndata(PSID, package = 'Ecdat')\nPSID &lt;- PSID %&gt;%\n  filter(age &gt;= 30, married %in% c('married','never married'), earnings &gt; 0) %&gt;%\n  mutate(married  = married == 'married')\nPSID %&gt;%\n  group_by(married) %&gt;%\n  summarize(log_earnings = mean(log(earnings)))\n\n# A tibble: 2 × 2\n  married log_earnings\n  &lt;lgl&gt;          &lt;dbl&gt;\n1 FALSE           9.26\n2 TRUE            9.47"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#comparison-of-means-2",
    "href": "2023/weeks/week07/slides.html#comparison-of-means-2",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#comparison-of-means-3",
    "href": "2023/weeks/week07/slides.html#comparison-of-means-3",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\n\nThe difference between the means follows a t-distribution under the null that they’re identical\nSo of course we can do a hypothesis test of whether they’re different. But why bother trotting out a specific test when we can just do a regression?\n(In fact, a lot of specific tests can be replaced with basic regression, see this explainer)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#comparison-of-means-4",
    "href": "2023/weeks/week07/slides.html#comparison-of-means-4",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nNotice:\n\nThe intercept gives the mean for the non-married group\nThe coefficient on marriedTRUE gives the married minus non-married difference\ni.e. the coefficient on a binary variable in a regression gives the difference in means\nIf we’d defined it the other way, with “not married” as the independent variable, the intercept would be the mean for the married group (i.e. “not married = 0”), and the coefficient would be the exact same but times \\(-1\\) (same difference, just opposite direction!)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#comparison-of-means-5",
    "href": "2023/weeks/week07/slides.html#comparison-of-means-5",
    "title": "Binary Variables and Functional Form",
    "section": "Comparison of Means",
    "text": "Comparison of Means\nWhy does OLS give us a comparison of means when you give it a binary variable?\n\nThe only \\(X\\) values are 0 (FALSE) and 1 (TRUE)\nBecause of this, OLS no longer really fits a line, it’s more of two separate means\nAnd when you’re estimating to minimize the sum of squared errors separately for each group, can’t do any better than to predict the mean!\nSo you get the mean of each group as each group’s prediction"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#binary-with-controls",
    "href": "2023/weeks/week07/slides.html#binary-with-controls",
    "title": "Binary Variables and Functional Form",
    "section": "Binary with Controls",
    "text": "Binary with Controls\n\nObviously this is handy for including binary controls, but why do this for binary treatments? Because we can add controls!\n\n\n\n                feols(log(earning..\nDependent Var.:       log(earnings)\n                                   \nConstant          8.740*** (0.1478)\nmarriedTRUE      0.3404*** (0.0579)\nkids            -0.2259*** (0.0159)\nage              0.0223*** (0.0038)\n_______________ ___________________\nS.E. type                       IID\nObservations                  2,803\nR2                          0.07609\nAdj. R2                     0.07510\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#multicollinearity",
    "href": "2023/weeks/week07/slides.html#multicollinearity",
    "title": "Binary Variables and Functional Form",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nWhy is just one side of it on the regression? Why aren’t “married” and “not married” BOTH included?\nBecause regression couldn’t give an answer!"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#multicollinearity-1",
    "href": "2023/weeks/week07/slides.html#multicollinearity-1",
    "title": "Binary Variables and Functional Form",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMean of married is \\(9.47\\) and of non-married is \\(9.26\\). \\[ \\log(Earnings) = 0 + 9.47Married + 9.26NonMarried \\] \\[ \\log(Earnings) = 3 + 6.47Married + 6.26NonMarried \\]\nThese (and infinitely many other options) all give the exact same predictions! OLS can’t pick between them. There’s no single best way to minimize squared residuals\nSo we pick one with convenient properties, setting one of the categories to have a coefficient of 0 (dropping it) and making the coefficient on the other the difference relative to the one we left out"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#more-than-two-categories-1",
    "href": "2023/weeks/week07/slides.html#more-than-two-categories-1",
    "title": "Binary Variables and Functional Form",
    "section": "More than Two Categories",
    "text": "More than Two Categories\n\nThat interpretation - dropping one and making the other relative to that, conveniently extends to multi-category variables\nWhy stop at binary categorical variables? There are plenty of categorical variables with more than two values\nWhat is your education level? What is your religious denomination? What continent are you on?\nWe can put these in a regression by turning each value into its own binary variable\n(and then dropping one so the coefficients on the others give you the difference with the omitted one)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#interpreting-ols-1",
    "href": "2023/weeks/week07/slides.html#interpreting-ols-1",
    "title": "Binary Variables and Functional Form",
    "section": "Interpreting OLS",
    "text": "Interpreting OLS\n\nTo think more about the right-hand-side, let’s go back to our original interpretation of an OLS coefficient \\[ Y = \\beta_0 + \\beta_1X + \\varepsilon \\]\nA one-unit change in \\(X\\) is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\nThis logic still works with binary variables since “a one-unit change in \\(X\\)” means “changing \\(X\\) from No to Yes”\nNotice that this assumes that a one-unit change in \\(X\\) always has the same effect on \\(\\beta_1\\) no matter what else is going on\nWhat if that’s not true?"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#functional-form",
    "href": "2023/weeks/week07/slides.html#functional-form",
    "title": "Binary Variables and Functional Form",
    "section": "Functional Form",
    "text": "Functional Form\n\nWe talked before about times when a linear model like standard OLS might not be sufficient\nHowever, as long as those non-linearities are on the right hand side, we can fix the problem easily but just having \\(X\\) enter non-linearly! Run it through a transformation!\nThe most common transformations by far are polynomials and logarithms"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#functional-form-1",
    "href": "2023/weeks/week07/slides.html#functional-form-1",
    "title": "Binary Variables and Functional Form",
    "section": "Functional Form",
    "text": "Functional Form\n\nWhy do this? Because sometimes a straight line is clearly not going to do the trick!"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#polynomials-1",
    "href": "2023/weeks/week07/slides.html#polynomials-1",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\n\\(\\beta_1X\\) is a “first order polynomial” - there’s one term\n\\(\\beta_1X + \\beta_2X^2\\) is a “second order polynomial” or a “quadratic” - two terms (note both included, it’s not just \\(X^2\\))\n\\(\\beta_1X + \\beta_2X^2 + \\beta_3X^3\\) is a third-order or cubic, etc."
  },
  {
    "objectID": "2023/weeks/week07/slides.html#polynomials-2",
    "href": "2023/weeks/week07/slides.html#polynomials-2",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\nWhat do they do?\n\nThe more polynomial terms, the more flexible the line can be. With enough terms you can mimic any shape of relationship\nOf course, if you just add a whole buncha terms, it gets very noisy, and prediction out-of-sample gets very bad\nKeep it minimal - quadratics are almost always enough, unless you have reason to believe there’s a true more-complex relationship. You can try adding higher-order terms and see if they make a difference"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#polynomials-3",
    "href": "2023/weeks/week07/slides.html#polynomials-3",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nThe true relationship is quadratic"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#polynomials-4",
    "href": "2023/weeks/week07/slides.html#polynomials-4",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nHigher-order terms don’t do anything for us here (because a quadratic is sufficient!)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#polynomials-5",
    "href": "2023/weeks/week07/slides.html#polynomials-5",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nInterpret polynomials using the derivative\n\\(\\partial Y/\\partial X\\) will be different depending on the value of \\(X\\) (as it should! Notice in the graph that the slope changes for different values of \\(X\\))\n\n\\[ Y = \\beta_1X + \\beta_2X^2 \\] \\[ \\partial Y/\\partial X = \\beta_1 + 2\\beta_2X \\]\nSo at \\(X = 0\\), the effect of a one-unit change in \\(X\\) is \\(\\beta_1\\). At \\(X = 1\\), it’s \\(\\beta_1 + \\beta_2\\). At \\(X = 5\\) it’s \\(\\beta_1 + 5\\beta_2\\)."
  },
  {
    "objectID": "2023/weeks/week07/slides.html#polynomials-6",
    "href": "2023/weeks/week07/slides.html#polynomials-6",
    "title": "Binary Variables and Functional Form",
    "section": "Polynomials",
    "text": "Polynomials\n\nIMPORTANT: when you have a polynomial, the coefficients on each individual term mean very little on their own. You have to consider them alongisde the other coefficients from the polynomial! Never interpret \\(\\beta_1\\) here without thinking about \\(\\beta_2\\) alongside. Also, the significance of the individual terms doesn’t really matter - consider doing an F-test of all of them at once."
  },
  {
    "objectID": "2023/weeks/week07/slides.html#concept-check",
    "href": "2023/weeks/week07/slides.html#concept-check",
    "title": "Binary Variables and Functional Form",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat’s the effect of a one-unit change in \\(X\\) at \\(X = 0\\), \\(X = 1\\), and \\(X = 2\\) for each of these?\n\n\n\n                feols(Y ~ X, dat.. feols(Y ~ X + I(.. feols(Y ~ X + I(...1\nDependent Var.:                  Y                  Y                    Y\n                                                                          \nConstant         7.285*** (0.5660)   -0.1295 (0.3839)      0.0759 (0.5091)\nX               -8.934*** (0.1953)   0.9779* (0.3831)      0.4542 (0.9331)\nX square                           -2.003*** (0.0752)   -1.738*** (0.4368)\nX cube                                                    -0.0354 (0.0574)\n_______________ __________________ __________________   __________________\nS.E. type                      IID                IID                  IID\nObservations                   200                200                  200\nR2                         0.91357            0.98122              0.98126\nAdj. R2                    0.91313            0.98103              0.98097\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#logarithms-1",
    "href": "2023/weeks/week07/slides.html#logarithms-1",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\nAnother common transformation, both for dependent and independent variables, is to take the logarithm\nThis has the effect of pulling in extreme values from strongly right-skewed data and making linear relationships pop out\nIncome, for example, is almost always used with a logarithm\nIt also gives the coefficients a nice percentage-based interpretation"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#logarithms-2",
    "href": "2023/weeks/week07/slides.html#logarithms-2",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#or-if-you-prefer",
    "href": "2023/weeks/week07/slides.html#or-if-you-prefer",
    "title": "Binary Variables and Functional Form",
    "section": "Or if you prefer…",
    "text": "Or if you prefer…\n\nNotice the change in axes"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#logarithms-3",
    "href": "2023/weeks/week07/slides.html#logarithms-3",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\nHow can we interpret them?\nThe key is to remember that \\(\\log(X) + a \\approx \\log((1+a)X)\\), meaning that a \\(a\\)-unit change in \\(log(X)\\) is similar to a \\(a\\times100%\\) change in \\(X\\)\nSo, walk through our “one-unit change in the variable” logic from before, but whenever we hit a log, change that into a percentage!"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#logarithms-4",
    "href": "2023/weeks/week07/slides.html#logarithms-4",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\n\n\\(Y = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1X\\) a one-unit change in \\(X\\) is associated with a \\(\\beta_1\\times 100\\)% change in \\(Y\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1\\log(X)\\) A one-unit change in \\(\\log(X)\\), or a or a 100% change in \\(X\\), is associated with a \\(\\beta_1\\)-unit change in \\(\\log(Y)\\), or a \\(\\beta_1\\times100\\)% change in \\(Y\\).\n(Try also with changes smaller than one unit - that’s usually more reasonable)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#logarithms-5",
    "href": "2023/weeks/week07/slides.html#logarithms-5",
    "title": "Binary Variables and Functional Form",
    "section": "Logarithms",
    "text": "Logarithms\nDownsides:\n\nLogarithms require that all data be positive. No negatives or zeroes!\nFairly rare that a variable with negative values wants a log anyway\nBut zeroes are common! A common practice is to just do \\(log(X+1)\\) but this is pretty arbitrary"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#functional-form-2",
    "href": "2023/weeks/week07/slides.html#functional-form-2",
    "title": "Binary Variables and Functional Form",
    "section": "Functional Form",
    "text": "Functional Form\n\nIn general, you want the shape of your function to match the shape of the relationship in the data (or, even better, the true relationship)\nPolynomials and logs can usually get you there!\nWhich to use? Use logs for highly skewed data or variables with exponential relationships\nUse polynomials if it doesn’t look straight! Check that scatterplot and see how not-straight it is!"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#interactions",
    "href": "2023/weeks/week07/slides.html#interactions",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nFor both polynomials and logarithms, the effect of a one-unit change in \\(X\\) differs depending on its current value (for logarithms, a 1-unit change in \\(X\\) is different percentage changes in \\(X\\) depending on current value)\nBut why stop there? Maybe the effect of \\(X\\) differs depending on the current value of other variables!\nEnter interaction terms!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z + \\varepsilon \\] - Interaction terms are a little tough but also extremely important."
  },
  {
    "objectID": "2023/weeks/week07/slides.html#interactions-1",
    "href": "2023/weeks/week07/slides.html#interactions-1",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\nExpect to come back to these slides, as you’re almost certainly going to use interaction terms in both our assessment and the dissertation"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#interactions-2",
    "href": "2023/weeks/week07/slides.html#interactions-2",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nChange in the value of a control can shift a regression line up and down\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z\\), estimated as \\(Y = .01 + 1.2X + .95Z\\):"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#interactions-3",
    "href": "2023/weeks/week07/slides.html#interactions-3",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nBut an interaction can both shift the line up and down AND change its slope\nUsing the model \\(Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z\\), estimated as \\(Y = .035 + 1.14X + .94Z + 1.02X\\times Z\\):"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#interactions-4",
    "href": "2023/weeks/week07/slides.html#interactions-4",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nHow can we interpret an interaction?\nThe idea is that the interaction shows how the effect of one variable changes as the value of the other changes\nThe derivative helps!\n\n\\[ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X\\times Z \\] \\[ \\partial Y/\\partial X = \\beta_1 + \\beta_3 Z \\]\n\nThe effect of \\(X\\) is \\(\\beta_1\\) when \\(Z = 0\\), or \\(\\beta_1 + \\beta_3\\) when \\(Z = 1\\), or \\(\\beta_1 + 3\\beta_3\\) if \\(Z = 3\\)!"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#interactions-5",
    "href": "2023/weeks/week07/slides.html#interactions-5",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nOften we are doing interactions with binary variables to see how an effect differs across groups\nNow, instead of the intercept giving the baseline and the binary coefficient giving the difference, the coefficient on \\(X\\) is the baseline effect of \\(X\\) and the interaction is the difference in the effect of \\(X\\)\nThe interaction coefficient becomes “the difference in the effect of \\(X\\) between the \\(Z\\) =”No” and \\(Z\\) = “Yes” groups”\n(What if it’s continuous? Mathematically the same but the thinking changes - the interaction term is the difference in the effect of \\(X\\) you get when increasing \\(Z\\) by one unit)"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#interactions-6",
    "href": "2023/weeks/week07/slides.html#interactions-6",
    "title": "Binary Variables and Functional Form",
    "section": "Interactions",
    "text": "Interactions\n\nMarriage for those without a college degree raises earnings by 24%. A college degree reduces the marriage premium by 25%. Marriage for those with a college degree reduces earnings by .24 - .25 = -1%\n\n\n\n                          feols(log(earnin..\nDependent Var.:                log(earnings)\n                                            \nConstant                   9.087*** (0.0583)\nmarriedTRUE               0.2381*** (0.0638)\ncollegeTRUE               0.8543*** (0.1255)\nmarriedTRUE x collegeTRUE  -0.2541. (0.1363)\n_________________________ __________________\nS.E. type                                IID\nObservations                           2,803\nR2                                   0.06253\nAdj. R2                              0.06153\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#notes-on-interactions",
    "href": "2023/weeks/week07/slides.html#notes-on-interactions",
    "title": "Binary Variables and Functional Form",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nLike with polynomials, the coefficients on their own now have little meaning and must be evaluated alongside each other. \\(\\beta_1\\) by itself is just “the effect of \\(X\\) when \\(Z = 0\\)”, not “the effect of \\(X\\)”\nYes, you do almost always want to include both variables in un-interacted form and interacted form. Otherwise the interpretation gets very thorny"
  },
  {
    "objectID": "2023/weeks/week07/slides.html#notes-on-interactions-1",
    "href": "2023/weeks/week07/slides.html#notes-on-interactions-1",
    "title": "Binary Variables and Functional Form",
    "section": "Notes on Interactions",
    "text": "Notes on Interactions\n\nInteraction effects are poorly powered. You need a lot of data to be able to tell whether an effect is different in two groups. If \\(N\\) observations is adequate power to see if the effect itself is different from zero, you need a sample of roughly \\(16\\times N\\) to see if the difference in effects is nonzero. Sixteen times!!\nIt’s tempting to try interacting your effect with everything to see if it’s bigger/smaller/nonzero in some groups, but because it’s poorly powered, this is a bad idea! You’ll get a lot of false positives\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html",
    "href": "2023/weeks/week11/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#background-1",
    "href": "2023/weeks/week11/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#code-and-software",
    "href": "2023/weeks/week11/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week11/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week11/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week11/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week11/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#workflow-1",
    "href": "2023/weeks/week11/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week11/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#checklist",
    "href": "2023/weeks/week11/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week11/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week11/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week11/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week11/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week11/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week11/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week11/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week11/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week11/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week11/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week11/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week11/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week11/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week11/page1.html",
    "href": "2023/weeks/week11/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week11/page1.html#seminar-slides",
    "href": "2023/weeks/week11/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week11/page1.html#communication",
    "href": "2023/weeks/week11/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html",
    "href": "2023/weeks/week10/slides_sem.html",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)\n\n\n\n\n\n\nFor causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes\n\n\n\n\n\n\nOnce upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication\n\n\n\n\n\n\nThe original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper\n\n\n\n\n\nHappy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape\n\n\n\n\n\nMost mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#background-1",
    "href": "2023/weeks/week10/slides_sem.html#background-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Think about the following:\n\nfor most of the history of science, and really all the social sciences, there was practically no data – only theories about data\nToday we are drowning in data (remember the memes)\nOrdinary people have more data than they know what to do with\n\nSmart watches, sleep data, calories, screen time bla bla bla\n\n\nAs such, in causal inference (or any quantitative work) it’s hard to separate the question from the actual work (programming)"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#code-and-software",
    "href": "2023/weeks/week10/slides_sem.html#code-and-software",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "For causal Inference we need:\n\nData\nSoftware for data (Stata for example)\nUnderstanding of statistics\nProgramming skills\n\nWhich softare?\n\nR, Stata, Python, SAS, SPSS\n\nToday we are going to see Language agnostic programming principnes that are usually not covered in econ classes"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#making-mistakes",
    "href": "2023/weeks/week10/slides_sem.html#making-mistakes",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Once upon a time there were 2 young researchers that spent 8 months trying to disprove an important work in political psychology (i.e. liberals are less biased than conservatives)\nAfter about 10 conference presentations, half a dozen talks\nThey decided to send an email to the people they were trying to disprove and send the paper for publication"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#making-mistakes-1",
    "href": "2023/weeks/week10/slides_sem.html#making-mistakes-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "The original (super famous) academics came back suggesting that their analysis doesn’t produce the same results\nDigging in their directories, the youngsters found countless versions of codes and data - hundreds of files with randome names\nAnd once the code was running again, they found a critical coding error (one comma missing) that corrected (destroyed) their results\nThe youngsters were devastated. Took them another 2 years to pick up the courage, redo the work and actually disprove the original paper"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#anna-karenina-principle",
    "href": "2023/weeks/week10/slides_sem.html#anna-karenina-principle",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Happy families are all alike; every unhappy family is unhappy in its own way - Leo Tolstoy, Anna Karenina\nGood empirical work is all alike; every bad empirical work is bad in its own way - Scott Cunningham, Causal Mixtape"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#what-do-we-learn",
    "href": "2023/weeks/week10/slides_sem.html#what-do-we-learn",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "",
    "text": "Most mistakes are not due to insufficient knowledge of coding\nThe cose of most errors is due to poorly designed empirical workflow\nThis is why we ll spend a whole seminar on this"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#workflow-1",
    "href": "2023/weeks/week10/slides_sem.html#workflow-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Workflow",
    "text": "Workflow\nDefinition:\nA workflow consists of an orchestrated and repeatable pattern of activity, enabled by the systematic organization of resources into processes that transform materials, provide services, or process information"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#empirical-workflow",
    "href": "2023/weeks/week10/slides_sem.html#empirical-workflow",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical Workflow",
    "text": "Empirical Workflow\n\nWorkflow is like a checklist you bind yourself to when you get your hands on data\n\nIt’s like a morning routine:\n\nAlarm goes off\nYou wash up\nMake coffee\n\n\nEmpirical workflow is a core set of tasks which as you mature you build from experience\n\nFinding the outlier errors is difficult\nWorkflows are to catch the most common errors\nIt’s like an intermediate step between getting data and analysing data"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#checklist",
    "href": "2023/weeks/week10/slides_sem.html#checklist",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Checklist",
    "text": "Checklist\n\nEmpirical workflows are really just a checklist of actions you will take before analyzing ANY data\nIt is imperative that you don’t start analysing the data until the list is checked off\nRESIST IT\nIt should be a few, simple, yet non-negotiable, programming commands and exercises to check for errors\nWe ll go over mine - feel free to add your own if you want"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "href": "2023/weeks/week10/slides_sem.html#empirical-workflows-require-scarce-time-inputs",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Empirical workflows require scarce time inputs",
    "text": "Empirical workflows require scarce time inputs\n\nAll of us are living at the edge of our resource constraints, and our most scarce resources is time\nTo do anything, we must sacrifice something else because all activities use time – including cleaning and arranging our data\nData work is like a marathon:\n\nInvolves far more time training than running - involves far more time doing tedious and repetitive tasks\n\nSince tedious task are repeated, the have the most potential for error\nEspecially for dissertations - However long you thing cleaning and organising data will take, multiply by 10"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#step-1---read-the-codebook",
    "href": "2023/weeks/week10/slides_sem.html#step-1---read-the-codebook",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 1 - Read the codebook",
    "text": "Step 1 - Read the codebook\n\nDatasets often come with very large documetns describing in detail the production of data\nYou mush become as much of an expert on the codebook as you are on your own research topic\nAllows to interpret the data you acquired\nSet aside time to study it and keep it handy\nSame goes for readme files that go with some datasets"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#do-not-touch-the-original-data",
    "href": "2023/weeks/week10/slides_sem.html#do-not-touch-the-original-data",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nAnd I repeat Do Not Touch the Original Data"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#do-not-touch-the-original-data-1",
    "href": "2023/weeks/week10/slides_sem.html#do-not-touch-the-original-data-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Do Not Touch the Original Data",
    "text": "Do Not Touch the Original Data\n\nEmpirical workflows require data manipulation\nIt is imperative taht you always only work with copies\nNever save over the original dataset - be careful of softwares like excel that may do it automatically\nAvoid this by storing the raw data separate from copies\nIf you alter the raw data, you might never have the chance to get them back"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#step-2---folder-structure-and-directories",
    "href": "2023/weeks/week10/slides_sem.html#step-2---folder-structure-and-directories",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Folder Structure and Directories",
    "text": "Step 2 - Folder Structure and Directories\n\nAfter our error, I did an extensive research on how problems like these tend to happen\nCame back wit hthe following important steps\n\nHierarchical folder structure\nAutomation\nNaming convention\nVersion Control"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#step-2---helping-your-future-self",
    "href": "2023/weeks/week10/slides_sem.html#step-2---helping-your-future-self",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Helping your future self",
    "text": "Step 2 - Helping your future self\n\nRemember your future self is also operating at the edge of her production possibilities\nThe typical quantitative project may have hundreds of files of various types and will take years from first thought to completion\nSo imagine if all files are not stored together, without some sort of archivign\nBest way to resolve this is hierarchical folder structures"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#step-2---subdirectory-organisation",
    "href": "2023/weeks/week10/slides_sem.html#step-2---subdirectory-organisation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Subdirectory organisation",
    "text": "Step 2 - Subdirectory organisation"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#step-2---automation",
    "href": "2023/weeks/week10/slides_sem.html#step-2---automation",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Automation",
    "text": "Step 2 - Automation\n\nAvoid as much as possible copy pasting information\nYour future self doesn’t remember 4 weeks/months from now how you made these tables and figures\nUse a code notebook (do file in Stata) where all commands you run are kept together in order\nEvery final output should be able to be reproduced by just pressing one run command"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#step-2---beautiful-code",
    "href": "2023/weeks/week10/slides_sem.html#step-2---beautiful-code",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 2 - Beautiful Code",
    "text": "Step 2 - Beautiful Code\n\nYour ideal goal is to make beautiful code\nAt minimum, don’t make it ugly - unreadable for your future self\nConsider a new text editor like Visual Studio Code which allows for\n\nColored syntax\nIndentation\nColumn editing and more\n\nStata and R come with their own editors but Visual Studio allows you to have one for all"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#step-3---eyeballing",
    "href": "2023/weeks/week10/slides_sem.html#step-3---eyeballing",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#step-3---eyeballing-1",
    "href": "2023/weeks/week10/slides_sem.html#step-3---eyeballing-1",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 3 - Eyeballing",
    "text": "Step 3 - Eyeballing\n\nHuman eyes have evolved to spot patterns\nWe can therefore use it to find things that belong and those that don’t\nFirst just scan the data in its spreadsheet form - get comfortable with what you are going to be using\nBrowse for Stata for example\nUse this to see if anything jumps out"
  },
  {
    "objectID": "2023/weeks/week10/slides_sem.html#step-4---missing-observations",
    "href": "2023/weeks/week10/slides_sem.html#step-4---missing-observations",
    "title": "💻 Seminar 1  Hidden Curriculum",
    "section": "Step 4 - Missing observations",
    "text": "Step 4 - Missing observations\n\nCheck the size of your dataset in Stata using count\nCheck the number of observations per variable in stata using summarize or summ\nUse tabulate also to check if, very oftern, missing observations are recorded with a -9 or -1 or 9999 (some illogical - probably- negative value)"
  },
  {
    "objectID": "2023/weeks/week10/page1.html",
    "href": "2023/weeks/week10/page1.html",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "",
    "text": "This session will introduce you to the hidden curriculum of quantitative applications. What is a workflow, Basic principles of coding and a dive in STATA."
  },
  {
    "objectID": "2023/weeks/week10/page1.html#seminar-slides",
    "href": "2023/weeks/week10/page1.html#seminar-slides",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "Seminar Slides",
    "text": "Seminar Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week10/page1.html#communication",
    "href": "2023/weeks/week10/page1.html#communication",
    "title": "💻 Seminar 1 - Hidden Curriculum",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nHave questions? Feel free to pop them up on #Week 1 channel on Discord"
  },
  {
    "objectID": "2023/weeks/week10/page.html",
    "href": "2023/weeks/week10/page.html",
    "title": "🗓️ Week 10 Instrumental Variables",
    "section": "",
    "text": "In this week, we will focus on estimating causal parameteres through Instrumental Variables. First, we will approach the concept of instruments through Direct Acyclical Graphs (DAGs) and then through the LATE effect (Local Average Treatment Effect)."
  },
  {
    "objectID": "2023/weeks/week10/page.html#lecture-slides",
    "href": "2023/weeks/week10/page.html#lecture-slides",
    "title": "🗓️ Week 10 Instrumental Variables",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week10/page.html#recommended-reading",
    "href": "2023/weeks/week10/page.html#recommended-reading",
    "title": "🗓️ Week 10 Instrumental Variables",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week10/page.html#communication",
    "href": "2023/weeks/week10/page.html#communication",
    "title": "🗓️ Week 10 Instrumental Variables",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week10/slides.html",
    "href": "2023/weeks/week10/slides.html",
    "title": "🗓️ Week 10 Instrumental Variables",
    "section": "",
    "text": "Check-in\n\nWe’ve been talking about within variation and RDD, which are ways of controlling for stuff (and so solving endogeneity problems) without having to control for everything on all the back door paths\nThey do this by finding a point in time at which only the treatment changes (DID) or finding a situation where nothing important changes over time (fixed effects)\nWhat if, instead of closing back doors by controlling for stuff (between variation), we instead isolated just the exogenous part of the treatment?\n\n\n\n\nIsolating Front Doors\nThe idea is this:\n\nThe treatment varies for all sorts of reasons\nMany of those reasons are endogenous. If you get treatment because you’re really rich, that wealth is likely going to be related to whatever outcome\nSome reasons are exogenous. If you get treatment because it was accidentally given to you at random, that’s unrelated to outcome\nThose exogenous reasons for treatment we can call instruments and we can perform instrumental variables analysis (IV)\nIf we use just the part of treatment driven by the instruments, then that part of treatment is exogenous and we can ignore all the back doors! We’re identified\n\n\n\n\nExperiments\n\nIf this sounds strange or implausible to you, we have an existing example ready to go\nRandomized experiments are conceptually very similar to the use of instruments\nMany endogenous reasons why people might get treatment. But the experiment’s randomization is an exogenous reason\n\n\n\n\n\n\n\n\n\nExperiments\n\nThe only difference between a randomized experiment and an instrument is that in randomized experiments, we control and impose the randomization\nIn the case of instruments we must find that exogenous variation in treatment in the world\n\n\n\n\n\n\n\n\n\nNatural Experiments\n\nThat’s why these are often referred to as “natural experiments” (although this is a broader term - DID and regression discontinuity are types of natural experiments too)\nThat’s our goal though!\n\n\nFind a source of exogenous variation in treatment\nIsolate just the part of treatment driven by that exogenous variation\nLook at the relationship between that part of treatment and the outcome\nYou’ve identified the effect!\n\n\n\n\nIntuitively\n\nThis is sort of like the opposite of controlling for \\(Z\\) \nWe look for what part of \\(X1\\) is explained by \\(Z\\)\nBut instead of removing that variation by controlling for \\(Z\\)…\nWe keep that variation, and remove all other variation in \\(X1\\) \n\n\n\n\nGraphically\n\n\n\n\n\n\n\n\nConcept Checks\n\nWhy is it important to partition the sources of variation in treatment into exogenous and endogenous? Why not do that with the outcome instead?\nWhat does it mean to say that we treat an instrument like the opposite of a control variable, and how do we do that?\nIn the animation on the previous slide, why does it draw a straight line between the two points and measure its slope? What does that give us?\n\n\n\n\nRelevance and Validity\nFor this to work, we need two things to hold:\n\nRelevance: the instrument must be a strong predictor of the treatment. It can’t be trivial or unimportant (or else what variation are you really isolating? You’ve got nothing to go on!)\nValidity: the instrument must actually be exogenous! (Or at least exogenous after adding controls). The endogeniety problem doesn’t go away with IV, it’s just shifted from the treatment variable to the instrument\n\nNow, let’s keep these assumptions in mind as we move, and think about how we can actually carry this out\n\n\n\nTwo Stage Least Squares\nSo our goal is to: (1) use the instrument to predict treatment, and then (2) use that predicted treatment to predict the outcome!\nWe need a separate equation for each of those steps\n“First stage”: predict treatment \\(X_1\\) with the instrument \\(Z\\), perhaps also a control \\(X_2\\).\n\\[X_1 = \\gamma_0 + \\gamma_1Z + \\gamma_2X_2 + \\nu\\]\nThen, use that equation to predict \\(X_1\\), getting \\(\\hat{X_1}\\). Then, use those predictions to predict \\(Y\\) in the “second stage”\n\\[Y = \\beta_0 + \\beta_1\\hat{X_1} + \\beta_2X_2 + \\varepsilon\\]\n\n\n\nTwo Stage Least Squares\n\nIn general we don’t actually carry out this process ourselves because it will get the standard errors wrong (they need to be adjusted for the fact that \\(\\hat{X}_1\\) is estimated)\nBut this shows what we’re doing - we’re isolating just the variation in treatment that is explained by the instrument\n\\(\\hat{X}_1\\) contains only variation driven by \\(Z\\)\nSo if \\(Z\\) is exogenous after controlling for \\(X_2\\), then so is \\(\\hat{X}_1\\)\nAnd \\(\\hat{\\beta}_1\\) will be identified\n\n\n\n\nConcept Checks\n\nWhy do you think that doing 2SLS by hand would make the standard errors be wrong?\nIntuitively, why would using Z-predicted values of X in predicting Y give us the causal effect if Z is a valid instrument?\nWhy do we include the control variables in both the first and second stages?\n\n\n\n\nThe Good and the Bad\nWhat is good and bad about this whole process?\nThe Good:\n\nCausal identification!\nFairly easy to do\nIntuitive - just like a randomized experiment, but in the wild\n\nThe Bad (we’ll go into these in detail)\n\nWe get a local average treatment effect (LATE)\nMonotonicity\nSmall-sample bias\nInstruments can’t be weak\nGood and believable instruments are really hard to find\n\n\n\n\nThe Local Average Treatment Effect\n\nIV only allows variation in the treatment that is driven by the instrument - that’s the whole point\nThis also means that we can only see the effect among people for whom the instrument drives their treatment\nIf a treatment improves your outcome by 2, but my outcome by only 1, and the instrument has a big effect on whether you get treatment, but only a little effect on me, then our IV estimate will be a lot closer to 2 than to 1\nThis is a “local average treatment effect” - our estimate is local to people who are affected by the instrument (and even more local to those affected more heavily than others)\n\n\n\n\nThe Local Average Treatment Effect\n\nSo?\nThis does mean that the IV estimate won’t be representative of everyone’s effect\nOr even of the people who actually were treated\nIt might be less informative about what would happen if we treated more people than if we did an actual experiment\nBut we might have to live with that to be able to use the cleaner identification\n\n\n\n\nMonotonicity\n\nAlso, think about that - we weight people by how strongly they’re affected by the instrument\nWhat if someone is affected by the instrument in an opposite direction to everyone else? This would be a violation of monotonicity\nThen, they would get a negative weight and our estimate doesn’t make much sense any more!\nSo we need to assume that everyone is either unaffected by the instrument, or affected in the exact same direction as everyone else\nWhen might this not be true? For example, say we’re using rainfall as an instrument for agricultural productivity\nRain might help in dry areas, but make things worse in already-too-wet areas. The instrument would help productivity some places and hurt it in others\n\n\n\n\nSmall-sample Bias\n\nIV is actually a biased estimator!\nThe mean of its sampling distribution is not the population parameter!\nOr rather, it would be the population parameter at infinite sample size, but we don’t have that\nIn small samples, the bias of IV is\n\n\\[\\frac{corr(Z,\\varepsilon)}{corr(Z,X_1)}\\frac{\\sigma_\\varepsilon}{\\sigma_{X_1}}\\]\n\nIf \\(Z\\) is valid, then in infinite samples \\(corr(Z,\\varepsilon)=0\\) and this goes away. But in a non-infinite sample, it will be nonzero by chance, inducing some bias. The smaller the sample, the more likely we are to get a large value by random chance\nThe bias is smaller the stronger the relationship between \\(Z\\) and \\(X_1\\), the smaller the sum of squared errors, and the bigger the variation in \\(X_1\\)\n\n\n\n\nWeak Instruments\n\nThis means we probably shouldn’t be using IV in small samples\nThis also means that it’s really important that \\(corr(Z,X_1)\\) isn’t small!\nIf \\(Z\\) has only a trivial effect on \\(X_1\\), then it’s not relevant - even if it’s truly exogenous, it doesn’t matter because there’s no variation in \\(X_1\\) we can isolate with it\nAnd our small-sample bias will be big! (imagine the term in the previous slide if \\(corr(Z,X_1) = .00001\\) !)\n\n\n\n\nWeak Instruments\n\nThere are some rules of thumb for how strong an instrument must be to be counted as “not weak”\nA t-statistic above 3, or an F statistic from a joint test of the instruments that is 10 or above\nThese rules of thumb aren’t great - selecting a model on the basis of significance naturally biases your results. But people do use them a lot so you should be aware\nWhat you really want is to know the population effect of \\(Z\\) on \\(X1\\) - you want the F-statistic from that to be 10+. Of course we don’t actually know that.\n\n\n\n\nWeak Instruments\n\nWhatever we feel about the rules-of-thumb, they’re quite common\nSo much so that you get it by default when looking at feols() IV output\n\n\niv &lt;- feols(Y ~ X2 | X1 ~ Z, data = df, se = 'hetero')\niv\n\nTSLS estimation, Dep. Var.: Y, Endo.: X1, Instr.: Z\nSecond stage: Dep. Var.: Y\nObservations: 200 \nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value  Pr(&gt;|t|)    \n(Intercept)  3.697702   0.166064 22.266665 &lt; 2.2e-16 ***\nfit_X1       0.553733   0.392039  1.412446   0.15940    \nX2          -0.022595   0.152040 -0.148612   0.88201    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.94425   Adj. R2: -0.019456\nF-test (1st stage), X1: stat = 346.4    , p &lt; 2.2e-16 , on 1 and 197 DoF.\n            Wu-Hausman: stat =   3.61357, p = 0.058777, on 1 and 196 DoF.\n\n\n\n346.43 is way above 10! We’re probably fine in this particular regression\n\n\n\n\nInstruments and Caution\n\nGood IVs are really hard to find\nClaiming the IV is exogenous (even after adding controls) is the same difficult problem when we’re doing it for the IV as when we’re doing it for the treatment\nWe’ve just shifted the claim to a variable it’s more likely to be true for\nAnd this is social science, where everything is related to everything else. So why isn’t your instrument part of that?\n\n\n\n\nInstruments and Caution\n\nLet’s take rainfall as an example\nFor a long time, developmental economists would use rainfall as an instrument for agricultural productivity\nControlling for location, variation the exact amount of rainfall from year to year is basically random, right?\nMaking it a good instrument so we can see the effect of agricultural productivity on other stuff\nExcept that rainfall also affects all other sorts of stuff (like what kind of transportation people take)\nAlso the monotonicity thing we talked about\nAnd my rainfall is correlated with my neighbor’s rainfall\nAlso, wait a minute… other people use rainfall as an instrument for warfare! If it’s relevant for warfare, doesn’t that make it invalid for agriculture (and vice versa)?\nAnd other things\n\n\n\n\nInstruments and Caution\n\nSo rainfall isn’t seen as a great instrument any more\nIn fact, lots of clever instruments that used to be thought of as good aren’t acceptable now due to similar problems - parental education as an instrument for your own, distance-you-live-from-a-college as an instrument on going-to-college, quarter of birth on education…\nNo wonder we’re so skeptical of cool-looking instruments now!\n\n\n\n\nInstruments and Caution\n\nAcceptable instruments these days fall into one of a few categories:\n\n\nActual literal randomization (like in a randomized experiment with imperfect compliance - see the Experiments module. Or similarly, fuzzy regression discontinuity - see the Regression Discontinuity module)\nVariables that are truly from outside the system and unrelated to anything social-sciency, like mistakes or computer glitches (for example, job interview scores for teachers were added incorrectly, as an instrument for progressing through the interview - Goldhaber, Grout, & Huntington-Klein 2017)\nVariables you’d be really surprised to find out are relevant but just happen to be - and when you look into it there’s a good reason it’s relevant\n\n\nIf you want to do IV, learn a lot a lot a lot of context so you can know really well how the IV fits into the data-generating process. You should be an expert on the topic you’re using IV in. Otherwise, how can you know it’s valid?\n\n\n\n\nConcept Checks\n\nGive an example where a local average treatment effect might give a very different answer from the average treatment effect\nWhy will a violation of monotonicity make our estimate meaningless? Hint: think about the local average treatment effect\nWhy would it be a problem for validity if the same instrument can be used for multiple different treatments?\nWhy would we be suspicious of any instrument that “makes sense” as a determinant of treatment?\n\n\n\n\nOther Things about IV\n\nOk, enough of the scare tactics! Just be very aware of these problems\nWhat other neat things can we do with IV?\n\nWell,\n\nWe can use multiple instruments, not just one\n(We could have multiple endogenous variables too, but we won’t cover that right now)\nWe can use IV to solve measurement issues rather than endogeneity issues\nWe can use IV to break simultaneous causality\n\n\n\n\nMultiple Instruments\n\nHow do we use more than one instrument for a single endogenous variable?\nEasy! Just add it to our prediction equation\n\n\\[X_1 = \\gamma_0 + \\gamma_1Z_1 + \\gamma_2Z_2 + \\gamma_3X_2 + \\nu\\]\n\\[Y = \\beta_0 + \\beta_1\\hat{X_1} + \\beta_2X_2 + \\varepsilon\\]\nor in R, feols(Y ~ X2 | X1 ~ Z1 + Z2 + X2, data = df)\n(we can add as many instruments as we want as long as we have valid ones! Some modern methods use hundreds of instruments and use machine learning to pick between them)\n\n\n\nMultiple Instruments\nWhy would we do this?\n\nIt can improve the fit and prediction of \\(\\hat{X}_1\\) which can reduce our weak-instrument problems\nIt changes what is estimated slightly - we still get a local average treatment effect, but now we get a weighted average of the local average treatment effects for each instrument, perhaps bringing in the effects for more people or weighting people more evenly; maybe that’s a thing we want\nWe can perform an overidentification test to get a better sense of validity\n\n\n\n\nOveridentification Tests\n\n“Overidentification” just means we have more identifying conditions (validity assumptions) than we actually need. We only need one instrument, but we have two! (or more)\nSo we can compare what we get using each instrument individually\nIf we assume that at least one of them is valid, and they both produce similar results, then that’s evidence that both are valid\nLike using one clock to set the time on another clock\nIf they’re dissimilar, then one of them is likely invalid, but we don’t know which one - we just know they’re different\n(also maybe they’re just producing different local average treatment effects, but let’s not use that copout!)\n\n\n\n\nOveridentification Tests\n\nWe can do this using fitstat() in fixest\n\n\n# Create data where Z1 is valid and Z2 is invalid\ndf &lt;- tibble(Z1 = rnorm(1000), Z2 = rnorm(1000)) %&gt;%\n  mutate(X = Z1 + Z2 + rnorm(1000)) %&gt;%\n  # True effect is 1\n  mutate(Y = X + Z2 + rnorm(1000))\n\niv &lt;- feols(Y~ 1 | X ~ Z1 + Z2, data = df, se = 'hetero')\nfitstat(iv, 'sargan')\n\nSargan: stat = 267.8, p &lt; 2.2e-16, on 1 DoF.\n\n\n\nThat’s a small p-value! We can reject that the results are similar for each IV, telling us that one is endogenous (although without seeing the actual data generating process we couldn’t guess if it were \\(Z1\\) or \\(Z2\\) )\n\n\n\n\nOveridentification Tests\n\nAnd how different are they? What did the test see that it was comparing? (Notice the first model gives an accurate coefficient of 1)\n\n\niv1 &lt;- feols(Y~ 1 | X ~ Z1, data = df)\niv2 &lt;- feols(Y~ 1 | X ~ Z2, data = df)\n\nexport_summs(iv1, iv2, statistics = c(N = 'nobs'))\n\n\n\n\n\nModel 1\nModel 2\n\n\n(Intercept)\n-0.01    \n0.00    \n\n\n\n(0.04)   \n(0.05)   \n\n\nfit_X\n1.08 ***\n1.92 ***\n\n\n\n(0.04)   \n(0.05)   \n\n\nN\n1000       \n1000       \n\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n\n\n\n\nThat’s it!\nBe sure to cover\n\nThe Swirl\nThe homework\nThe assigned paper"
  },
  {
    "objectID": "2023/weeks/week11/page.html",
    "href": "2023/weeks/week11/page.html",
    "title": "🗓️ Week 11 - Difference in Differences",
    "section": "",
    "text": "In this week, we will focus on causal inference through Difference in Difference estimators. First, we will decompose the mechanics and conditions of Diff-in-Diff estimators and then we will briefly discuss current methodological debates and advances."
  },
  {
    "objectID": "2023/weeks/week11/page.html#lecture-slides",
    "href": "2023/weeks/week11/page.html#lecture-slides",
    "title": "🗓️ Week 11 - Difference in Differences",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week11/page.html#recommended-reading",
    "href": "2023/weeks/week11/page.html#recommended-reading",
    "title": "🗓️ Week 11 - Difference in Differences",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture."
  },
  {
    "objectID": "2023/weeks/week11/page.html#communication",
    "href": "2023/weeks/week11/page.html#communication",
    "title": "🗓️ Week 11 - Difference in Differences",
    "section": "📟 Communication",
    "text": "📟 Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Discord"
  },
  {
    "objectID": "2023/weeks/week11/slides.html",
    "href": "2023/weeks/week11/slides.html",
    "title": "🗓️ Week 11 Difference in Differences",
    "section": "",
    "text": "Check-in\n\nWe’re thinking through ways that we can identify the effect of interest without having to control for everything\nOne way is by focusing on within variation - if all the endogeneity can be controlled for or only varies between-individuals, we can just focus on within variation to identify it\nPro: control for a bunch of stuff\nCon: washes out a lot of variation! Result can be noisier if there’s not much within-variation to work with\nAlso, this requires no endogenous variation over time\nThat might be a tricky assumption! Often there are plenty of back doors that shift over time\n\n\n\n\nDifference-in-Differences\n\nToday we will talk about difference-in-differences (DID), which is a way of using within variation in a more deliberate way in order to identify the effect we want\nAll we need is a treatment that goes into effect at a particular time, and we need a group that is treated and a group that is not\nThen, we compare the within-variation for the treated group vs. the within-variation for the untreated group\nVoila, we have an effect!\n\n\n\n\nDifference-in-Differences\n\nBecause the requirements to use it are so low, DID is used a lot\nAny time a policy is enacted but isn’t enacted everywhere at once? DID!\nPlus, the logic is pretty straightforward\nNotice it doesn’t even get a full chapter in the textbook? That’s not because it’s unimportant - it’s because it’s easy!\nHere we’ll cover the concept, next time some example studies\n\n\n\n\nDifference-in-Differences\n\nThe question DID tries to answer is “what was the effect of (some policy) on the people who were affected by it?”\nWe have some data on the people who were affected both before the policy went into effect and after\nHowever, we can’t just compare before and after, because we have a back door path - Time! Things change over Time for reasons unrelated to Treatment\n\n\n\n\n\n\n\n\n\nDifference-in-Differences\n\nWhy not just control for Time? We can certainly measure that and we’ve controlled for it before!\nBut we can’t! It would wash out all the variation!\nYou’re either Before and Untreated, or After and Treated. So if you control for Time, you’re comparing people with the same values of Time - who must also have the same values of Treatment! So you can’t compare Treated and Untreated to get the effect\n\n\n\n\nDifference-in-Differences\n\nThe solution is to bring in a control group who is Untreated in both Before and After periods\nNow the diagram looks like this\nThere’s more to control for (we have to control for Group differences too - isolate within variation), but this ALLOWS us to control for Time\n\n\n\n\n\n\n\n\n\nDifference-in-Difference\n\nWe control for Group by isolating within variation (comparing After to Before)\nThen we control for Time by comparing those two sources of Within variation\nWe ask how much more increase was there in Treatment than control?\nThe Control change is probably the increase you could have expected regardless of Treatment\nSo anything on top of that is the effect of Treatment\nNow we’ve controlled for Group and Time, identifying the effect!\n\n\n\n\nExample\n\nLet’s say we have a pill that’s supposed to make you taller\nGive it to a kid Adelaide who is 48 inches tall\nNext year they’re 54 inches tall - a six inch increase! But they probably would have grown some anyway without the pill. Surely the pill doesn’t make you six inches taller.\nSO we compare them to their twin Bella, who started at 47 inches but we DON’T give a pill to\nNext year that twin is 51 inches tall - a four inch increase. So Adelaide probably would have grown about 4 inches without the pill. So the pill boosted her by \\((54 - 48) - (51 - 47) = 6 - 4 = 2\\) additional inches\n“Adelaide, who was Treated, grew by two more inches than Bella, who was Untreated, did over the same period of time, so the pill made Adelaide grow by two inches”\nThat’s DID!\n\n\n\n\nExample\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\n\nDifference-in-Differences\nWhat changes are included in each value?\n\nUntreated Before: Untreated Group Mean\nUntreated After: Untreated Group Mean + Time Effect\nTreated Before: Treated Group Mean\nTreated After: Treated Group Mean + Time Effect + Treatment Effect\nUntreated After - Before = Time Effect\nTreated After - Before = Time Effect + Treatment Effect\nDID = (Treated After - Before) - (Untreated After - Before) = Treatment EFfect\n\n\n\n\nConcept Checks\n\nWhy do we need a control group? What does this let us do?\nWhat do we need to assume is true about our control group?\nIn 2015, a new, higher minimum wage went into effect in Seattle, but this increase did not occur in some of the areas surrounding Seattle. How might you use DID to estimate the effect of this minimum wage change on employment levels?\n\n\n\n\nDifference-in-Difference\n\nOf course, this only uses four data points\nUsually these four points would be four means from lots of observations, not just two people in two time periods\nHow can we do this and get things like standard errors, and perhaps include controls?\nWhy, use OLS of course!\n\n\n\n\nDifference-in-Differences\n\nWe can use what we know about binary variables and interaction terms to get our DID\n\n\\[Y_{it} = \\beta_0 + \\beta_1After_t + \\beta_2Treated_i + \\beta_3After_tTreated_i + \\varepsilon_{it}\\] where \\(After_t\\) is a binary variable for being in the post-treatment period, and \\(Treated_t\\) is a binary variable for being in the treated group\n\n\n    Person   Time Height After Treated\n1 Adelaide Before     48 FALSE    TRUE\n2 Adelaide  After     54  TRUE    TRUE\n3    Bella Before     47 FALSE   FALSE\n4    Bella  After     51  TRUE   FALSE\n\n\n\n\n\nDifference-in-Differences\n\nHow can we interpret this using what we know?\n\n\\[Y_{it} = \\beta_0 + \\beta_1After_t + \\beta_2Treated_i + \\beta_3After_tTreated_i + \\varepsilon_{it}\\]\n\n\\(\\beta_0\\) is the prediction when \\(Treated_i = 0\\) and \\(After_t = 0\\) \\(\\rightarrow\\) the Untreated Before mean!\n\\(\\beta_1\\) is the difference between Before and After for \\(Treated_i = 0\\) \\(\\rightarrow\\) Untreated (After - Before)\n\\(\\beta_2\\) is the difference between Treated and Untreated for \\(After_t = 0\\) \\(\\rightarrow\\) Before (Treated - Untreated)\n\\(\\beta_3\\) is how much bigger the Before-After difference is for \\(Treated_i = 1\\) than for \\(Treated_i = 0\\) \\(\\rightarrow\\) (Treated After - Before) - (Untreated After - Before) = DID!\n\n\n\n\nGraphically\n\n\n\n\n\n\n\n\nDesign vs. Regression\n\nLike with fixed effects, there is a distinction between regression model and research design\nWe have a model with an interaction term\nNot all models with interaction terms are DID!\nIt’s DID because it’s an interaction between treated/control and before/after\nIf you don’t have a before/after, or you don’t have a control group, that same setup may tell you something interesting but it won’t be DID!\n\n\n\n\nExample\n\nThe Earned Income Tax Credit was increased in 1993. This may increase chances single mothers (treated) return to work, but likely not affect single non-moms (control)\nDoes this program help moms get back to work?\n\n\ndf &lt;- read.csv('http://nickchk.com/eitc.csv') %&gt;%\n  mutate(after = year &gt;= 1994,\n         treated = children &gt; 0)\ndf %&gt;% \n  group_by(after, treated) %&gt;%\n  summarize(proportion_working = mean(work))\n\n# A tibble: 4 × 3\n# Groups:   after [2]\n  after treated proportion_working\n  &lt;lgl&gt; &lt;lgl&gt;                &lt;dbl&gt;\n1 FALSE FALSE                0.575\n2 FALSE TRUE                 0.446\n3 TRUE  FALSE                0.573\n4 TRUE  TRUE                 0.491\n\n\n\n\n\nExample\n\nWe can do it by just comparing the points, like we did with Adelaide and Bella\nThis will give us the DID estimate: The EITC increase increases the probability of working by 4.7 percentage points\nBut not standard errors, or the ability to include controls easily\n\n\nmeans &lt;- df %&gt;% \n  group_by(after, treated) %&gt;%\n  summarize(proportion_working = mean(work)) %&gt;%\n  pull(proportion_working)\n(means[4] - means[2]) - (means[3] - means[1])\n\n[1] 0.04687313\n\n\n\n\n\nLet’s try OLS!\n\ndid_reg &lt;- feols(work ~ after*treated, data = df)\netable(did_reg, digits = 3)\n\n                                  did_reg\nDependent Var.:                      work\n                                         \nConstant                 0.575*** (0.009)\nafterTRUE                  -0.002 (0.013)\ntreatedTRUE             -0.129*** (0.012)\nafterTRUE x treatedTRUE   0.047** (0.017)\n_______________________ _________________\nS.E. type                             IID\nObservations                       13,746\nR2                                0.01260\nAdj. R2                           0.01238\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nConcept Checks\n\nThere are four coefficients on the previous slide. Interpret them carefully in a sentence each.\nWhy can the \\(After \\times TreatedGroup\\) interaction stand in for \\(Treated\\) ?\n\n\n\n\nMore!!\n\nYou can recognize \\(After_t\\) and \\(Treated_i\\) as fixed effects\n\\(Treated_i\\) is a fixed effect for group - we only need one coefficient for it since there are only two groups\nAnd \\(After_t\\) is a fixed effect for time - one coefficient for two time periods\nYou can have more than one set of fixed effects like this! Our interpretation is now within-group and within-time\n(i.e. comparing the within-group variation across groups)\n\n\n\n\nMore!!\n\nWe can extend this to having more than two groups, some of which get treated and some of which don’t\nAnd more than two time periods! Multiple before and/or multiple after\nWe don’t have a full set of interaction terms, we still only need the one, which we can now call \\(CurrentlyTreated_{it}\\)\nIf you have more than two groups and/or more than two time periods, then this is what you should be doing\n\n\n\n\nMore!!\n\nLet’s make some quick example data to show this off, with the first treated period being period 7 and the treated groups being 1 and 9, and a true effect of 3\n\n\ndid_data &lt;- tibble(group = sort(rep(1:10, 10)),\n                   time = rep(1:10, 10)) %&gt;%\n  mutate(CurrentlyTreated  = group %in% c(1,9) & time &gt;= 7) %&gt;%\n  mutate(Outcome = group + time + 3*CurrentlyTreated + rnorm(100))\ndid_data\n\n# A tibble: 100 × 4\n   group  time CurrentlyTreated Outcome\n   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;              &lt;dbl&gt;\n 1     1     1 FALSE               1.50\n 2     1     2 FALSE               2.43\n 3     1     3 FALSE               3.70\n 4     1     4 FALSE               5.75\n 5     1     5 FALSE               4.57\n 6     1     6 FALSE               6.47\n 7     1     7 TRUE               11.1 \n 8     1     8 TRUE               12.6 \n 9     1     9 TRUE               12.6 \n10     1    10 TRUE               13.6 \n# ℹ 90 more rows\n\n\n\n\n\nMore!!\n\n# Put group first so the clustering is on group\nmany_periods_did &lt;- feols(Outcome ~ CurrentlyTreated | group + time, data = did_data)\netable(many_periods_did)\n\n                      many_periods_did\nDependent Var.:                Outcome\n                                      \nCurrentlyTreatedTRUE 3.095*** (0.4991)\nFixed-Effects:       -----------------\ngroup                              Yes\ntime                               Yes\n____________________ _________________\nS.E.: Clustered              by: group\nObservations                       100\nR2                             0.96131\nWithin R2                      0.34238\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nDowners and Assumptions\n\nSo… does this all work?\nThat example got pretty close to the truth of 3 but who knows in other cases!\nWhat needs to be true for this to work?\n\n\n\n\nDoes it Work?\n\nThis gives us a causal effect as long as the only reason the gap changed was the treatment\nIf Adelaide would have grown six inches anyway, then the gap would have grown by 2, pill or not. The pill did nothing in that case! But we don’t know that and mistakenly say it helped her grow by 2\nIn fixed effects, we need to assume that there’s no uncontrolled endogenous variation across time\nIn DID, we need to assume that there’s no uncontrolled endogenous variation across this particular before/after time change\nAn easier assumption to justify but still an assumption!\n\n\n\n\nParallel Trends\n\nThis assumption - that nothing else changes at the same time, is the poorly-named “parallel trends”\nAgain, this assumes that, if the Treatment hadn’t happened to anyone, the gap between the two would have stayed the same\nSometimes people check whether this assumption is plausible by seeing if prior trends are the same for Treated and Untreated - if we have multiple pre-treatment periods, was the gap changing a lot during that period?\nSometimes people also “adjust for prior trends” to fix parallel trends violations, or use related methods like Synthetic Control (these are outside the scope of this class)\n\n\n\n\nPrior Trends\n\nLet’s see how that EITC example looks in the leadup to 1994\nThey look like the gap between them is pretty constant before 1994! They move up and down but the gap stays the same. That’s good.\n\n\n\n\n\n\n\n\n\nPrior Trends\n\nFormally, prior trends being the same tells us nothing about parallel trends\nBut it can be suggestive. Going back to the height pill example, what if instead of comparing Adelaide and Bella, child twins, we compared Adelaide to me?\nSeeing the gap closing anyway in previous years would be a pretty good clue that it’s not just the pill\n\n\n\n\n\n\n\n\n\nParallel Trends\n\nJust because prior trends are equal doesn’t mean that parallel trends holds.\nParallel trends is about what the before-after change would have been - we can’t see that!\nFor example, let’s say we want to see the effect of online teaching on student test scores, using COVID school shutdowns to get a Before/After\nAs of March/April 2020, some schools had gone online (Treated) and others hadn’t (Untreated)\nTest score trends were probably pretty similar in the Before periods (Jan/Feb 2020), so prior trends are likely the same\nBut LOTS of stuff changed between Jan/Feb and Mar/Apr, like, uh, Coronavirus, lockdowns, etc. not just online teaching! SO parallel trends likely wouldn’t hold\n\n\n\n\nConcept Checks\n\nGo back to the Seattle minimum wage effect example from the first Concept Check slide. Clearly state what the parallel trends assumption means in this context.\nIt’s possible (although perhaps unlikely) that parallel trends can hold even if it looks like the treatment and control groups were trending apart before treatment went into effect. How is this possible?\n\n\n\n\nAnd a Warning!\n\nOne other note about DID is that its popularity is relatively recent, as things go, so we’re still learning a lot about it!\nMost relevant has to do with staggered rollout DID\nYou may have noticed that the OLS version of DID doesn’t necessarily need to have treatment applied at one particular time\nSay Washington gets treated in 2017, then California in 2018 - we can use these both as Treated groups and just use the same two-way fixed effects, right?\nWell… that’s what we thought for a long time. And you’ll see a LOT of published studies doing this. But it turns out to actually bias results by quite a lot\nThere are more complex modern estimators for staggered rollout DID (say, in the did package), but they’re a bit much to go into for this class\nFor now, stick to DID designs where all the Treated groups are treated in the same period"
  },
  {
    "objectID": "2023/weeks/week12/page.html",
    "href": "2023/weeks/week12/page.html",
    "title": "🗓️ Guest Lecture - Difference in Differences",
    "section": "",
    "text": "In this extra lecture, we will host Dr. Georgios Kavetsos (Associate Professor in Behavioural Science Queen Mary University) to explore applications of Diff-in-Diff in happiness and policy evaluations"
  },
  {
    "objectID": "2023/weeks/week12/page.html#lecture-slides",
    "href": "2023/weeks/week12/page.html#lecture-slides",
    "title": "🗓️ Guest Lecture - Difference in Differences",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nSlides will be uploaded closer to the event\n🎥 Looking for lecture recordings? You can only find those on Moodle."
  },
  {
    "objectID": "2023/weeks/week12/page.html#papers-to-review",
    "href": "2023/weeks/week12/page.html#papers-to-review",
    "title": "🗓️ Guest Lecture - Difference in Differences",
    "section": "📚 Papers to review",
    "text": "📚 Papers to review\n\nThe effect of the Brexit Referendum Result on Subjective Well-being here\nQuantifying the intangible impact of the Olympics using subjective well-being data here"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#check-in",
    "href": "2023/weeks/week09/slides.html#check-in",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Check-in",
    "text": "Check-in\n\nWe’re thinking through ways that we can identify the effect of interest without having to control for everything\nOne way is by focusing on within variation - if all the endogeneity can be controlled for or only varies between-individuals, we can just focus on within variation to identify it\nPro: control for a bunch of stuff\nCon: washes out a lot of variation! Result can be noisier if there’s not much within-variation to work with\nAlso, this requires no endogenous variation over time\nThat might be a tricky assumption! Often there are plenty of back doors that shift over time"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#regression-discontinuity-1",
    "href": "2023/weeks/week09/slides.html#regression-discontinuity-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\nThe basic idea is this:\n\nWe look for a treatment that is assigned on the basis of being above/below a cutoff value of a continuous variable\nFor example, if you get above a certain test score they let you into a “gifted and talented” program\n\nOr if you are just on one side of a time zone line, your day starts one hour earlier/later\nOr if a candidate gets 50.1% of the vote they’re in, 40.9% and they’re out\n\n\nWe call these continuous variables “Running variables” because we run along them until we hit the cutoff"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#regression-discontinuity-2",
    "href": "2023/weeks/week09/slides.html#regression-discontinuity-2",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nBut wait, hold on, if treatment is driven by running variables, won’t we have a back door going through those very same running variables?? Yes!\nAnd we can’t just control for RunningVar because that’s where all the variation in treatment comes from. Uh oh!"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#regression-discontinuity-3",
    "href": "2023/weeks/week09/slides.html#regression-discontinuity-3",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nThe key here is realizing that the running variable affects treatment only when you go across the cutoff\nSo really the diagram looks like this!"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#regression-discontinuity-4",
    "href": "2023/weeks/week09/slides.html#regression-discontinuity-4",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nSo what does this mean?\nIf we can control for the running variable everywhere except the cutoff, then…\nWe will be controlling for the running variable, closing that back door\nBut leaving variation at the cutoff open, allowing for variation in treatment\nWe focus on just the variation around the treatment, narrowing the range of the running variable we use so sharply that it’s basically controlled for. Then the effect of cutoff on treatment is like an experiment!"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#regression-discontinuity-5",
    "href": "2023/weeks/week09/slides.html#regression-discontinuity-5",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nBasically, the idea is that right around the cutoff, treatment is randomly assigned\nIf you have a test score of 89.9 (not high enough for gifted-and-talented), you’re basically the same as someone who has a test score of 90.0 (just barely high enough)\nBut we get variation in treatment!\nThis specifically gives us the effect of treatment for people who are right around the cutoff a.k.a. a “local average treatment effect” (we still won’t know the effect of being put in gifted-and-talented for someone who gets a 30)"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#regression-discontinuity-6",
    "href": "2023/weeks/week09/slides.html#regression-discontinuity-6",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nA very basic idea of this, before we even get to regression, is to create a binned chart\nAnd see how the bin values jump at the cutoff\nA binned chart chops the Y-axis up into bins\nThen takes the average Y value within that bin. That’s it!\nThen, we look at how those X bins relate to the Y binned values.\nIf it looks like a pretty normal, continuous relationship… then JUMPS UP at the cutoff X-axis value, that tells us that the treatment itself must be doing something!"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#regression-discontinuity-7",
    "href": "2023/weeks/week09/slides.html#regression-discontinuity-7",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#concept-checks",
    "href": "2023/weeks/week09/slides.html#concept-checks",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWhy is it important that we look as norrowly as possible around the cutoff? What does this get us over comparing the entire treated and untreated groups?\nCan you think of an example of a treatment that is assigned at least partially on a cutoff?\nWhy can’t we just control for the running variable as we normally would to solve the endogeneity problem?"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#fitting-lines-in-rdd",
    "href": "2023/weeks/week09/slides.html#fitting-lines-in-rdd",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Fitting Lines in RDD",
    "text": "Fitting Lines in RDD\n\nLooking purely just at the cutoff and making no use of the space away from the cutoff throws out a lot of useful information\nWe know that the running variable is related to outcome, so we can probably improve our prediction of what the value on either side of the cutoff should be if we use data away from the cutoff to help with prediction than if we just use data near the cutoff, which is what that animation does\nWe can do this with good ol’ OLS.\nThe bin plot we did can help us pick a functional form for the slope"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#fitting-lines-in-rdd-1",
    "href": "2023/weeks/week09/slides.html#fitting-lines-in-rdd-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Fitting Lines in RDD",
    "text": "Fitting Lines in RDD\n\nTo be clear, producing the line(s) below is our goal. How can we do it?\nThe true model I’ve made is an RDD effect of .7, with a slope of 1 to the left of the cutoff and a slope of 1.5 to the right"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#regression-in-rdd",
    "href": "2023/weeks/week09/slides.html#regression-in-rdd",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression in RDD",
    "text": "Regression in RDD\n\nFirst, we need to transform our data\nWe need a “Treated” variable that’s TRUE when treatment is applied - above or below the cutoff\nThen, we are going to want a bunch of things to change at the cutoff. This will be easier if the running variable is centered around the cutoff. So we’ll turn our running variable \\(X\\) into \\(X - cutoff\\) and call that \\(XCentered\\)"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#varying-slope",
    "href": "2023/weeks/week09/slides.html#varying-slope",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Varying Slope",
    "text": "Varying Slope\n\nTypically, you will want to let the slope vary to either side\nIn effect, we are fitting an entirely different regression line on each side of the cutoff\nWe can do this by interacting both slope and intercept with \\(treated\\)!\nCoefficient on Treated is how the intercept jumps - that’s our RDD effect. Coefficient on the interaction is how the slope changes\n\n\\[Y = \\beta_0 + \\beta_1Treated + \\beta_2XCentered + \\beta_3Treated\\times XCentered + \\varepsilon\\]"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#varying-slope-1",
    "href": "2023/weeks/week09/slides.html#varying-slope-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Varying Slope",
    "text": "Varying Slope\n(as an aside, sometimes the effect of interest is the interaction term - the change in slope! This answers the question “does the effect of \\(X\\) on \\(Y\\) change at the cutoff? This is called a”regression kink” design. We won’t go more into it here, but it is out there!)"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#polynomial-terms",
    "href": "2023/weeks/week09/slides.html#polynomial-terms",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Polynomial Terms",
    "text": "Polynomial Terms\n\nWe don’t need to stop at linear slopes!\nJust like we brought in our knowledge of binary and interaction terms to understand the linear slope change, we can bring in polynomials too. Add a square maybe!\nDon’t get too wild with cubes, quartics, etc. - polynomials tend to be at their “weirdest” near the edges, and we don’t want super-weird predictions right at the cutoff. It could give us a mistaken result!\nA square term should be enough"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#polynomial-terms-1",
    "href": "2023/weeks/week09/slides.html#polynomial-terms-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Polynomial Terms",
    "text": "Polynomial Terms\n\nHow do we do this? Interactions again. Take any regression equation… \\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\\]\nAnd just center the \\(X\\) (let’s call it \\(XC\\), add on a set of the same terms multiplied by \\(Treated\\) (don’t forget \\(Treated\\) by itself - that’s \\(Treated\\) times the interaction!)\n\n\\[Y = \\beta_0 + \\beta_1XC + \\beta_2XC^2 + \\beta_3Treated + \\beta_4Treated\\times XC + \\beta_5Treated\\times XC^2 + \\varepsilon\\]\n\nThe coefficient on \\(Treated\\) remains our “jump at the cutoff” - our RDD estimate!\n\n\n\n                              feols(Y ~ X_cent..\nDependent Var.:                                Y\n                                                \nConstant                        -0.0340 (0.0385)\nX_centered                      0.6990. (0.3641)\ntreatedTRUE                   0.7677*** (0.0577)\nX_centered square               -0.5722 (0.7117)\nX_centered x treatedTRUE         0.7509 (0.5359)\ntreatedTRUE x I(X_centered^2)     0.5319 (1.034)\n_____________________________ __________________\nS.E. type                                    IID\nObservations                               1,000\nR2                                       0.84779\nAdj. R2                                  0.84702\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#concept-checks-1",
    "href": "2023/weeks/week09/slides.html#concept-checks-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Concept Checks",
    "text": "Concept Checks\n\nWould the coefficient on \\(Treated\\) still be the regression discontinuity effect estimate if we hadn’t centered \\(X\\)? Why or why not?\nWhy might we want to use a polynomial term in our RDD model?\nWhat relationship are we assuming between the outcome variable and the running variable if we choose not to include \\(XCentered\\) in our model at all (i.e. a “zero-order polynomial”)"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#assumptions",
    "href": "2023/weeks/week09/slides.html#assumptions",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Assumptions",
    "text": "Assumptions\n\nWe knew there must be some assumptions lurking around here\nWhat are we assuming about the error term and endogeneity here?\nSpecifically, we are assuming that the only thing jumping at the cutoff is treatment\nSort of like parallel trends (see week 11), but maybe more believable since we’ve narrowed in so far\nFor example, if having an income below 150% of the poverty line gets you access to food stamps AND to job training, then we can’t really use that cutoff to get the effect of just food stamps\nThe only thing different about just above/just below should be treatment"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#graphically",
    "href": "2023/weeks/week09/slides.html#graphically",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Graphically",
    "text": "Graphically"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#windows-1",
    "href": "2023/weeks/week09/slides.html#windows-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Windows",
    "text": "Windows\n\nPay attention to the sample sizes, accuracy (true value .7) and standard errors!\n\n\nm1 &lt;- feols(Y~treated*X_centered, data = df)\nm2 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .25))\nm3 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .1))\nm4 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .05))\nm5 &lt;- feols(Y~treated*X_centered, data = df %&gt;% filter(abs(X_centered) &lt; .01))\netable(m1,m2,m3,m4,m5, keep = 'treatedTRUE')\n\n                                         m1                 m2\nDependent Var.:                           Y                  Y\n                                                              \ntreatedTRUE              0.7467*** (0.0376) 0.7723*** (0.0566)\ntreatedTRUE x X_centered 0.4470*** (0.1296)   0.6671. (0.4022)\n________________________ __________________ __________________\nS.E. type                               IID                IID\nObservations                          1,000                492\nR2                                  0.84769            0.74687\nAdj. R2                             0.84723            0.74531\n\n                                         m3                 m4              m5\nDependent Var.:                           Y                  Y               Y\n                                                                              \ntreatedTRUE              0.7086*** (0.0900) 0.6104*** (0.1467) 0.5585 (0.4269)\ntreatedTRUE x X_centered     -1.307 (1.482)      6.280 (4.789)   41.21 (72.21)\n________________________ __________________ __________________ _______________\nS.E. type                               IID                IID             IID\nObservations                            206                 93              15\nR2                                  0.69322            0.59825         0.48853\nAdj. R2                             0.68867            0.58470         0.34904\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#granular-running-variable-1",
    "href": "2023/weeks/week09/slides.html#granular-running-variable-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Granular Running Variable",
    "text": "Granular Running Variable\n\nNot a whole lot we can do about this\nThere are some fancy RDD estimators that allow for granular running variables\nBut in general, if this is what you’re facing, you might be in trouble\nBefore doing an RDD, think “is it plausible that someone with the highest value just below the cutoff, and someone with the lowest value just above the cutoff are only at different values because of random chance?”"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#looking-for-lumping-1",
    "href": "2023/weeks/week09/slides.html#looking-for-lumping-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nIf there’s manipulation of the running variable around the cutoff, we can often see it in the presence of lumping\nI.e. if there’s a big cluster of observations to one side of the cutoff and a seeming gap missing on the other side"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#looking-for-lumping-2",
    "href": "2023/weeks/week09/slides.html#looking-for-lumping-2",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nHere’s an example from the real world in medical research - statistically, p-values should be uniformly distributed\nBut it’s hard to get insignificant results published in some journals. So people might “p-hack” until they find some form of analysis that’s significant, and also we have heavy selection into publication based on \\(p &lt; .05\\). Can’t use that cutoff for an RDD!\n\n\np-value graph from Perneger & Combescure, 2017"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#looking-for-lumping-3",
    "href": "2023/weeks/week09/slides.html#looking-for-lumping-3",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nHow can we look for this stuff?\nWe can look graphically by just checking for a jump at the cutoff in number of observations after binning\n\n\ndf_bin_count &lt;- df %&gt;%\n  # Select breaks so that one of hte breakpoints is the cutoff\n  mutate(X_bins = cut(X, breaks = 0:10/10)) %&gt;%\n  group_by(X_bins) %&gt;%\n  count()"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#looking-for-lumping-4",
    "href": "2023/weeks/week09/slides.html#looking-for-lumping-4",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nThe first one looks pretty good. We have one that looks not-so-good on the right"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#looking-for-lumping-5",
    "href": "2023/weeks/week09/slides.html#looking-for-lumping-5",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Looking for Lumping",
    "text": "Looking for Lumping\n\nAnother thing we can do is do a “placebo test”\nCheck if variables other than treatment or outcome vary at the cutoff\nWe can do this by re-running our RDD but just swapping out some other variable for our outcome\nIf we get a significant jump, that’s bad! That tells us that other things are changing at the cutoff which implies some sort of manipulation (or just super lousy luck)"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#regression-discontinuity-in-r",
    "href": "2023/weeks/week09/slides.html#regression-discontinuity-in-r",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "Regression Discontinuity in R",
    "text": "Regression Discontinuity in R\n\nWe can specify an RDD model by just telling it the dependent variable \\(Y\\), the running variable \\(X\\), and the cutoff \\(c\\).\nWe can also specify how many polynomials to us with p\n(it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)\nIt will also pick a window for us with h\nPlenty of other options\nIncluding a fuzzy option to specify actual treatment outside of the running variable/cutoff combo"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#rdrobust",
    "href": "2023/weeks/week09/slides.html#rdrobust",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nWe’ve gone through all kinds of procedures for doing RDD in R already using regression\nBut often, professional researchers won’t do it that way!\nWe’ll use packages and formulas that do things like “picking a bandwidth (window)” for us in a smart way, or not relying so strongly on linearity\nThe rdrobust package does just that!"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#rdrobust-1",
    "href": "2023/weeks/week09/slides.html#rdrobust-1",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5))\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  501          499\nEff. Number of Obs.             185          170\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.174        0.174\nBW bias (b)                   0.293        0.293\nrho (h/b)                     0.594        0.594\nUnique Obs.                     501          499\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.707     0.085     8.311     0.000     [0.540 , 0.874]     \n        Robust         -         -     6.762     0.000     [0.484 , 0.878]     \n============================================================================="
  },
  {
    "objectID": "2023/weeks/week09/slides.html#rdrobust-2",
    "href": "2023/weeks/week09/slides.html#rdrobust-2",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nsummary(rdrobust(df$Y, df$X, c = .5, fuzzy = df$treatment))\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  501          499\nEff. Number of Obs.             185          170\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.174        0.174\nBW bias (b)                   0.293        0.293\nrho (h/b)                     0.594        0.594\nUnique Obs.                     501          499\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.707     0.085     8.311     0.000     [0.540 , 0.874]     \n        Robust         -         -     6.762     0.000     [0.484 , 0.878]     \n============================================================================="
  },
  {
    "objectID": "2023/weeks/week09/slides.html#rdrobust-3",
    "href": "2023/weeks/week09/slides.html#rdrobust-3",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "rdrobust",
    "text": "rdrobust\n\nWe can even have it automatically make plots of our RDD! Same syntax\n\n\nrdplot(df$Y, df$X, c = .5)"
  },
  {
    "objectID": "2023/weeks/week09/slides.html#thats-it",
    "href": "2023/weeks/week09/slides.html#thats-it",
    "title": "🗓️ Week 9 Regression Discontinuity Designs",
    "section": "That’s it!",
    "text": "That’s it!\n\nThat’s what we have for RDD\nGo explore the regression discontinuity Seminar\nAnd the paper to read!\n\n\n\n\nPB4A7- Quantitative Applications for Behavioural Science"
  },
  {
    "objectID": "2023/weeks/week00/slides.html#causality-and-prediction",
    "href": "2023/weeks/week00/slides.html#causality-and-prediction",
    "title": "🗓️ Week 0 Presessionals",
    "section": "Causality and Prediction",
    "text": "Causality and Prediction\n\nGreat! Still, why do we care about this class?\nIn econometrics, we are working with data\nStatisticians also work with data\nSo do data scientists\nThe goals for these groups differ"
  }
]