---
subtitle: "PB4A7- Quantitative Applications for Behavioural Science"
title: "<font style='font-size:1em;'>üóìÔ∏è Week 0<br/> Presessionals</font>"
author: Dr. [George Melios](#)
institute: '[London School of Economics and Political Science](#)'
date: 18 September 2023
date-meta: 18 September 2023
date-format: "DD MMM YYYY"
toc: true
toc-depth: 1
toc-title: "What we will cover today:"
center-title-slide: false
from: markdown+emoji
format:
  revealjs: 
    fig-responsive: true
    theme: simple
    slide-number: true
    mouse-wheel: false
    preview-links: auto
    logo: /figures/logos/MY_INSTITUTION.png
    css: /css/styles_slides.css
    footer: 'PB4A7- Quantitative Applications for Behavioural Science'
---

# Who are we

## Your lecturer {.smaller}

::: columns

::: {.column style="display:inline-block;width:40%;height:60%;border-radius:1em;margin:1%;padding:1.5%;background-color:#f5f5f5"}
<figure>
    <img src="/figures/people/George.jpeg" alt="Photo of George" role="presentation" style="object-fit: cover;width:5em;height:5em;border-radius: 50%;font-size:1em;" class="img-responsive atto_image_button_text-bottom">
    <figcaption>
        <strong>Dr.&nbsp;<span><a href="#" target="_blank" class="external">Melios</a></span></strong> 
        <br/>Research Fellow
    </figcaption>
</figure>
:::

::: {.column style="width:50%;font-size:0.85em;margin-left:3%;"}
- PhD in Economics
- **Background**: Economics, Political Science, Behavioural Science

<span class="tag" style="background-color:var(--quarto-hl-st-color)">Political Economy</span>
<br/>
<span class="tag" style="background-color:var(--quarto-hl-fu-color)">Beliefs</span>
<br/>
<span class="tag" style="background-color:var(--quarto-hl-dv-color)">Wellbeing</span>

:::

:::

# Welcome to Econometrics

## What is this class

- This is econometrics
- Econometrics is a field that covers how economists think about statistical analysis
- Many other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be

---

- So what is econometrics?
- Econometrics focuses on the study of *observational data*
- Observational data are measurements of things that *the researcher does not control*
- Given that we are working with observational data, we still want to understand the *causes of things*
- The world is what it is, we are only here to study it

---

# Welcome to Econometrics

- When is this useful?
- We can't randomly assign or experiment with things like how much education you get, or what our tax rates are, or what a stock's recent returns have been, or where you live - that would generally be impossible or cruel
- But we still want to learn about the *underlying patterns* related to those data - how does X cause Y? What is the correct model to use to understand X and Y?
- That would be simple if we could control the situation, turn things on and off, randomize stuff
- But we can't! So what now? Enter econometrics.

---

# Welcome to Econometrics

- This is a great course (if I do say so)
- When I took it, what it left me feeling was *powerful*
- It gave me the ability to answer questions I was interested in on my own
- And to understand the degree of confidence I could have in my own results and in others
- "A new study says..." you don't need to take the paper's word for that any more!

---

# Admin

- Review the syllabus (and other materials on Canvas). Reading assignments there
- Our textbook is [The Effect](https://theeffectbook.net), by me, which can be read for free.
- Also these slides
- Programming in R: free, you will want to have access to it. We will get to this next time
- Assignments: Weekly homework combining econometrics and R tasks
- There is a midterm and a final
- A data exploration project
- A group data analysis project
- And completion of Swirl modules and online discussion

---

# Causality and Prediction

- Okay so what are we doing here exactly?
- In econometrics, we are working with data
- Statisticians also work with data
- So do data scientists
- The *goals* for these groups differ considerably

---

# Causality and Prediction

- Data scientists are generally concerned with *prediction*
- They want to use the data at hand to *predict* what comes next
- They generally don't care *why* they're making the prediction they are
- This can be really handy for certain tasks - "is this picture a cat or a dog?" "what's the probability that a customer with qualities X, Y, and Z will end up purchasing our good?" "do you have lymphoma?"

---

# Causality and Prediction

- Econometricians, on the other hand, care almost exclusively about *why*
- Data scientists want to minimize *prediction error*
- Econometricians want to minimize *inference* and *identification error*
- We want to correctly understand *the underlying data generating process*

---

# Inference Error and Randomness

- One big problem we face when trying to figure out how variables relate to each other is *randomness*
- This is simply the fact that, when we gather data, we can only possibly get a subsample of *all* the data
- So, just by random chance, the relationship we get in our data might not be quite the same as the true relationship

---

# Inference Error and Randomness

- So if we look in a data set and see that $X$ and $Y$ appear to be positively related to each other...
- Are they actually positively related, or is that just random chance?
- If they are positively related, maybe we're understating or overstating *how* positively related

---

# Inference Error and Randomness

- If the true relationship is 0, then in the data we'll see a positive relationship half the time, and a negative relationship half the time
- Even though the truth is 0!
- How can we properly make an *inference* about whether the relationship is 0 or not (or positive, or negative, or *how* positive or negative), taking into account this randomness?
- That's being careful about inference. The statisticians teach us all about this!


---

# Identification Error

- What is identification error?
- *Identification* is how you link the *result you see* with the *conclusion you draw from it*
- For example, say you observe that kids who play video games are more aggressive in everyday life (result), and you conclude from that result that video games make kids more aggressive (conclusion)
- If *seeing that result is actually evidence for that conclusion*, then we are properly *identified*

---

# Identification Error

- But if there's another reason why we might see that result, i.e. if the same result could give us a different conclusion, like *kids who are aggressive play more video games* or *people notice aggression more when kids play video games*, then we have made an *identification error* - our result was not identified!
- Identification error is when your result in the data doesn't actually have a clear theoretical ("why" or "because") interpretation
- For example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts, you have committed an identification error
- One day in and all we can do is complain, eesh

---

# Data Generating Process

- To avoid identification error, economists think closely about the *data generating process*
- What is a data generating process?
- The data generating process is the *true set of laws* that determine where our data comes from
- For example, if you hold a rock and drop it, it falls to the floor
- What is the data we observe? (Hold the rock & Rock is up) and (Let go & Rock is down)
- What is the data generating process? Gravity makes the rock fall down when you drop it

---

# Data Generating Process

- Another example is a model of supply and demand
- We observe prices and quantities in a competitive market
- What led to those being the prices and quantities we see?
- The supply and demand model and its equilibrium, we theorize!

---

---
title: "Instrumental Variables"
subtitle: "The (random) ripple effect"
date: "Updated `r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: TRUE
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    # Run xaringan::summon_remark() for this
    #chakra: libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE) 
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, fig.height = 6)
library(tidyverse)
library(gganimate)
library(estimatr)
library(magick)
library(dagitty)
library(ggthemes)
library(directlabels)
library(ggdag)
library(fixest)
library(jtools)
library(scales)
library(Cairo)
theme_metro <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16),
        axis.title.x = element_text(hjust = 1),
        axis.title.y = element_text(hjust = 1, angle = 0))
}
theme_void_metro <- function(x) {
  theme_void() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
theme_metro_regtitle <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
```

# Check-in

- We've been talking about within variation and RDD, which are ways of controlling for stuff (and so solving endogeneity problems) without having to control for *everything* on all the back door paths
- They do this by finding a point in time at which only the treatment changes (DID) or finding a situation where nothing important changes over time (fixed effects)
- What if, instead of closing back doors by controlling for stuff (between variation), we instead *isolated just the exogenous part of the treatment*?

---

# Isolating Front Doors

The idea is this:

- The treatment varies for all sorts of reasons
- Many of those reasons are endogenous. If you get treatment because you're really rich, that wealth is likely going to be related to whatever outcome
- *Some reasons are exogenous*. If you get treatment because it was accidentally given to you at random, that's unrelated to outcome
- Those *exogenous reasons for treatment* we can call *instruments* and we can perform *instrumental variables* analysis (IV)
- If we use *just the part of treatment driven by the instruments*, then *that part of treatment is exogenous* and we can ignore all the back doors! We're identified

---

# Experiments

- If this sounds strange or implausible to you, we have an existing example ready to go
- Randomized experiments are conceptually *very* similar to the use of instruments
- Many endogenous reasons why people might get treatment. But the experiment's randomization is an exogenous reason

```{r, dev = 'CairoPNG'}
dag <- dagify(Outcome ~ Treatment + AnnoyingEndogeneity,
              Treatment ~ AnnoyingEndogeneity + Randomization,
              coords=list(
                x=c(Randomization = 0,Treatment = 3, AnnoyingEndogeneity = 4, Outcome = 5),
                y=c(Randomization = 1, Treatment = 1, AnnoyingEndogeneity = 2, Outcome = 1)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=10) + 
  theme_void_metro() + 
  expand_limits(x=c(-0.5,2.5))

```

---

# Experiments

- The *only* difference between a randomized experiment and an instrument is that in randomized experiments, we control and impose the randomization
- In the case of instruments we must find that exogenous variation in treatment in the world

```{r, dev = 'CairoPNG'}
dag <- dagify(Outcome ~ Treatment + AnnoyingEndogeneity,
              Treatment ~ AnnoyingEndogeneity + Instrument,
              coords=list(
                x=c(Instrument = 0,Treatment = 3, AnnoyingEndogeneity = 4, Outcome = 5),
                y=c(Instrument = 1, Treatment = 1, AnnoyingEndogeneity = 2, Outcome = 1)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=10) + 
  theme_void_metro() + 
  expand_limits(x=c(-.5,2.5))

```

---

# Natural Experiments

- That's why these are often referred to as "natural experiments" (although this is a broader term - DID and regression discontinuity are types of natural experiments too)
- That's our goal though!

1. Find a source of exogenous variation in treatment
1. Isolate just the part of treatment driven by *that exogenous variation*
1. Look at the relationship between *that part of treatment* and the outcome
1. You've identified the effect!

---

# Intuitively

- This is sort of like the *opposite of controlling for $Z$ *
- We look for what part of $X1$ is explained by $Z$
- But instead of *removing* that variation by controlling for $Z$...
- We *keep* that variation, and *remove all other variation in $X1$ *

---

# Graphically

```{r, dev='CairoPNG', echo=FALSE, fig.width=8,fig.height=7}
df <- data.frame(Z = as.integer(1:200>100),
                 W = rnorm(200)) %>%
  mutate(X = .5+2*W +2*Z+ rnorm(200)) %>%
  mutate(Y = -X + 4*W + 1 + rnorm(200),time="1") %>%
  group_by(Z) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y),YL=NA,XL=NA) %>%
  ungroup()

#Calculate correlations
before_cor <- paste("1. Raw data. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')
afterlab <- '6. The slope between points is the effect of X on Y.'

dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),
  #Step 2: Add x-lines
  df %>% mutate(mean_Y=NA,time='2. What differences in X are explained by Z?'),
  #Step 3: X de-meaned 
  df %>% mutate(X = mean_X,mean_Y=NA,time="3. Remove everything in X not explained by Z"),
  #Step 4: Remove X lines, add Y
  df %>% mutate(X = mean_X,mean_X=NA,time="4. What differences in Y are explained by Z?"),
  #Step 5: Y de-meaned
  df %>% mutate(X = mean_X,Y = mean_Y,mean_X=NA,time="5. Remove everything in Y not explained by Z"),
  #Step 6: Raw demeaned data only
  df %>% mutate(X =  mean_X,Y =mean_Y,mean_X=NA,mean_Y=NA,YL=mean_Y,XL=mean_X,time=afterlab))

#Get line segments
endpts <- df %>%
  group_by(Z) %>%
  summarize(mean_X=mean(mean_X),mean_Y=mean(mean_Y))

p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(Z)))+geom_point()+
  geom_vline(aes(xintercept=mean_X,color=as.factor(Z)))+
  geom_hline(aes(yintercept=mean_Y,color=as.factor(Z)))+
  guides(color=guide_legend(title="Z"))+
  geom_segment(aes(x=ifelse(time==afterlab,endpts$mean_X[1],NA),
                   y=endpts$mean_Y[1],xend=endpts$mean_X[2],
                   yend=endpts$mean_Y[2]),size=1,color='blue')+
  scale_color_colorblind()+
  labs(title = 'X -> Y, With Binary Z as an Instrumental Variable \n{next_state}')+
  theme_metro_regtitle() +
  transition_states(time,transition_length=c(6,16,6,16,6,6),state_length=c(50,22,12,22,12,50),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()

animate(p,nframes=175)
```

---

# Concept Checks

- Why is it important to partition the sources of variation in treatment into exogenous and endogenous? Why not do that with the outcome instead?
- What does it mean to say that we treat an instrument like the opposite of a control variable, and how do we do that?
- In the animation on the previous slide, why does it draw a straight line between the two points and measure its slope? What does that give us?

---

# Relevance and Validity

For this to work, we need two things to hold:

- **Relevance**: the instrument must be a *strong predictor* of the treatment. It can't be trivial or unimportant (or else what variation are you really isolating? You've got nothing to go on!)
- **Validity**: the instrument must actually be *exogenous*! (Or at least exogenous after adding controls). The endogeniety problem doesn't *go away* with IV, it's just shifted from the treatment variable to the instrument

Now, let's keep these assumptions in mind as we move, and think about how we can actually carry this out

---

# Two Stage Least Squares

So our goal is to: (1) use the instrument to predict treatment, and then (2) use that predicted treatment to predict the outcome!

We need a separate equation for each of those steps

"First stage": predict treatment $X_1$ with the instrument $Z$, perhaps also a control $X_2$.

$$X_1 = \gamma_0 + \gamma_1Z + \gamma_2X_2 + \nu$$

Then, use that equation to predict $X_1$, getting $\hat{X_1}$. Then, use those predictions to predict $Y$ in the "second stage"

$$Y = \beta_0 + \beta_1\hat{X_1} + \beta_2X_2 + \varepsilon$$

---

# Two Stage Least Squares

- In general we don't actually carry out this process ourselves because it will get the standard errors wrong (they need to be adjusted for the fact that $\hat{X}_1$ is estimated)
- But this shows what we're doing - we're isolating just the variation in treatment that is explained by the instrument
- $\hat{X}_1$ contains *only* variation driven by $Z$
- So if $Z$ is exogenous after controlling for $X_2$, then so is $\hat{X}_1$
- And $\hat{\beta}_1$ will be identified

---

# R

We can do this in R by hand (although as mentioned, the SEs will be wrong)

```{r, eval = FALSE, echo = TRUE}
first_stage <- feols(X1 ~ Z + X2, data = df)
df <- df %>%
  mutate(predict_X1 = predict(first_stage))

second_stage <- feols(Y ~ predict_X1 + X2, data = df)
```

---

# R

There are a bunch of ways to do it properly in R. The classic is `ivreg` in the **AER** package, but other functions are more fully-featured, including robust SEs, clustering, and fixed effects if you want them. `felm` in **lfe**, `tsls` in **sem**, a number of things in **ivpack**

We'll be using good ol' `feols` from **fixest**

```{r, eval = FALSE, echo = TRUE}
iv_regression <- feols(Y ~ X2 | X1 ~ Z, data = df, se = 'hetero')
```

---

# R

If we actually want to *look at* that first stage, we can extract that

```{r, eval = FALSE, echo = TRUE}
iv_regression$iv_first_stage
```

---

# Concept Checks

- Why do you think that doing 2SLS by hand would make the standard errors be wrong?
- Intuitively, why would using Z-predicted values of X in predicting Y give us the causal effect if Z is a valid instrument?
- Why do we include the control variables in both the first and second stages?

---

# The Good and the Bad

What is good and bad about this whole process?

The Good:

- Causal identification!
- Fairly easy to do
- Intuitive - just like a randomized experiment, but in the wild

The Bad (we'll go into these in detail)

- We get a local average treatment effect (LATE)
- Monotonicity
- Small-sample bias
- Instruments can't be weak
- Good and believable instruments are *really hard to find*


---

# The Local Average Treatment Effect

- IV only allows variation in the treatment *that is driven by the instrument* - that's the whole point
- This also means that we can only see the effect *among people for whom the instrument drives their treatment*
- If a treatment improves *your* outcome by 2, but *my* outcome by only 1, and the instrument has a *big effect* on whether you get treatment, but only a *little effect* on me, then our IV estimate will be a lot closer to 2 than to 1
- This is a "local average treatment effect" - our estimate is *local* to people who are affected by the instrument (and even *more* local to those affected more heavily than others)

---

# The Local Average Treatment Effect

- So?
- This does mean that the IV estimate won't be representative of *everyone's* effect
- Or even of *the people who actually were treated*
- It might be less informative about *what would happen if we treated more people* than if we did an actual experiment
- But we might have to live with that to be able to use the cleaner identification

---

# Monotonicity

- Also, think about that - we weight people by how strongly they're affected by the instrument
- What if someone is affected by the instrument *in an opposite direction* to everyone else? This would be a violation of *monotonicity*
- Then, they would get a *negative weight* and our estimate doesn't make much sense any more!
- So we need to assume that everyone is either *unaffected* by the instrument, or *affected in the exact same direction as everyone else*
- When might this not be true? For example, say we're using rainfall as an instrument for agricultural productivity
- Rain might help in dry areas, but make things worse in already-too-wet areas. The instrument would help productivity some places and hurt it in others

---

# Small-sample Bias

- IV is actually a *biased* estimator!
- The mean of its sampling distribution is *not* the population parameter!
- Or rather, it *would be* the population parameter at infinite sample size, but we don't have that
- In small samples, the bias of IV is

$$\frac{corr(Z,\varepsilon)}{corr(Z,X_1)}\frac{\sigma_\varepsilon}{\sigma_{X_1}}$$

- If $Z$ is valid, then in infinite samples $corr(Z,\varepsilon)=0$ and this goes away. But in a non-infinite sample, it will be nonzero by chance, inducing some bias. The smaller the sample, the more likely we are to get a large value by random chance
- The bias is smaller the stronger the relationship between $Z$ and $X_1$, the smaller the sum of squared errors, and the bigger the variation in $X_1$

---

# Weak Instruments

- This means we probably shouldn't be using IV in small samples
- This also means that it's really important that $corr(Z,X_1)$ isn't small!
- If $Z$ has only a trivial effect on $X_1$, then it's not *relevant* - even if it's truly exogenous, it doesn't matter because there's no variation in $X_1$ we can isolate with it
- And our small-sample bias will be big! (imagine the term in the previous slide if $corr(Z,X_1) = .00001$ !)

---

# Weak Instruments

- There are some rules of thumb for how strong an instrument must be to be counted as "not weak"
- A t-statistic above 3, or an F statistic from a joint test of the instruments that is 10 or above
- These rules of thumb aren't great - selecting a model on the basis of significance naturally biases your results. But people do use them a lot so you should be aware
- What you really want is to know the *population* effect of $Z$ on $X1$ - you want the F-statistic from *that* to be 10+. Of course we don't actually know that.

---

# Weak Instruments

- Whatever we feel about the rules-of-thumb, they're quite common
- So much so that you get it by default when looking at `feols()` IV output

```{r, echo = FALSE}
set.seed(200)
library(fabricatr)
df <- fabricate(
  N = 200,
  Y = rpois(N, lambda = 4),
  Z = rbinom(N, 1, prob = 0.4),
  X1  = Z * rbinom(N, 1, prob = 0.8),
  X2 = rnorm(N),
  G = sample(letters[1:4], N, replace = TRUE)
)

iv <- feols(Y ~ X2 | X1 ~ Z, data = df, se = 'hetero')
thef <- fitstat(iv, 'ivf', verbose = FALSE)$`ivf1::X1`$stat
```

```{r, echo = TRUE}
iv <- feols(Y ~ X2 | X1 ~ Z, data = df, se = 'hetero')
iv
```

- `r scales::number(thef, accuracy = .01)` is way above 10! We're probably fine in this particular regression

---

# Instruments and Caution

- Good IVs are *really hard to find*
- Claiming the IV is exogenous (even after adding controls) is the *same difficult problem* when we're doing it for the IV as when we're doing it for the treatment
- We've just shifted the claim to a variable it's more likely to be true for
- And this is social science, where *everything is related to everything else*. So why isn't your instrument part of that?

---

# Instruments and Caution

- Let's take rainfall as an example
- For a long time, developmental economists would use rainfall as an instrument for agricultural productivity
- Controlling for location, variation the exact amount of rainfall from year to year is basically random, right?
- Making it a good instrument so we can see the effect of agricultural productivity on other stuff
- Except that rainfall also affects all other sorts of stuff (like what kind of transportation people take)
- Also the monotonicity thing we talked about
- And *my* rainfall is correlated with my *neighbor's* rainfall
- Also, wait a minute... other people use rainfall as an instrument for warfare! If it's relevant for warfare, doesn't that make it invalid for agriculture (and vice versa)?
- And other things

---

# Instruments and Caution

- So rainfall isn't seen as a great instrument any more
- In fact, lots of clever instruments that used to be thought of as good aren't acceptable now due to similar problems - parental education as an instrument for your own, distance-you-live-from-a-college as an instrument on going-to-college, quarter of birth on education...
- No wonder we're so skeptical of cool-looking instruments now!

---

# Instruments and Caution

- Acceptable instruments these days fall into one of a few categories:

1. Actual literal randomization (like in a randomized experiment with imperfect compliance - see the Experiments module. Or similarly, fuzzy regression discontinuity - see the Regression Discontinuity module)
1. Variables that are truly from outside the system and unrelated to anything social-sciency, like mistakes or computer glitches (for example, job interview scores for teachers were added incorrectly, as an instrument for progressing through the interview - Goldhaber, Grout, & Huntington-Klein 2017)
1. Variables you'd be really surprised to find out are relevant but just happen to be - and when you look into it there's a good reason it's relevant

- If you want to do IV, **learn a lot a lot a lot of context** so you can know **really well** how the IV fits into the data-generating process. You should be an expert on the topic you're using IV in. Otherwise, how can you know it's valid?

---

# Concept Checks

- Give an example where a local average treatment effect might give a very different answer from the average treatment effect
- Why will a violation of monotonicity make our estimate meaningless? Hint: think about the local average treatment effect
- Why would it be a problem for validity if the same instrument can be used for multiple different treatments?
- Why would we be suspicious of any instrument that "makes sense" as a determinant of treatment?

---

# Other Things about IV

- Ok, enough of the scare tactics! Just be very aware of these problems
- What other neat things can we do with IV?

Well,

- We can use *multiple* instruments, not just one
- (We could have multiple endogenous variables too, but we won't cover that right now)
- We can use IV to solve *measurement issues* rather than endogeneity issues
- We can use IV to break *simultaneous causality*

---

# Multiple Instruments

- How do we use more than one instrument for a single endogenous variable?
- Easy! Just add it to our prediction equation

$$X_1 = \gamma_0 + \gamma_1Z_1 + \gamma_2Z_2 + \gamma_3X_2 + \nu$$

$$Y = \beta_0 + \beta_1\hat{X_1} + \beta_2X_2 + \varepsilon$$

or in R, `feols(Y ~ X2 | X1 ~ Z1 + Z2 + X2, data = df)`

(we can add as many instruments as we want as long as we have valid ones! Some modern methods use *hundreds* of instruments and use machine learning to pick between them)

---

# Multiple Instruments

Why would we do this?

- It can improve the fit and prediction of $\hat{X}_1$ which can reduce our weak-instrument problems
- It changes what is estimated slightly - we still get a local average treatment effect, but now we get a weighted average of the local average treatment effects for each instrument, perhaps bringing in the effects for more people or weighting people more evenly; maybe that's a thing we want
- We can perform an overidentification test to get a better sense of validity

---

# Overidentification Tests

- "Overidentification" just means we have more identifying conditions (validity assumptions) than we actually need. We only need one instrument, but we have two! (or more)
- So we can compare what we get using each instrument individually
- If we assume that *at least one of them is valid*, and they both produce similar results, then that's evidence that *both* are valid
- Like using one clock to set the time on another clock
- If they're dissimilar, then *one* of them is likely invalid, but we don't know which one - we just know they're different
- (also maybe they're just producing different local average treatment effects, but let's not use that copout!)

---

# Overidentification Tests

- We can do this using `fitstat()` in **fixest**

```{r, echo = FALSE}
set.seed(1000)
```

```{r, echo = TRUE}
# Create data where Z1 is valid and Z2 is invalid
df <- tibble(Z1 = rnorm(1000), Z2 = rnorm(1000)) %>%
  mutate(X = Z1 + Z2 + rnorm(1000)) %>%
  # True effect is 1
  mutate(Y = X + Z2 + rnorm(1000))

iv <- feols(Y~ 1 | X ~ Z1 + Z2, data = df, se = 'hetero')
fitstat(iv, 'sargan')
```
 - That's a small p-value! We can reject that the results are similar for each IV, telling us that one is endogenous (although without seeing the actual data generating process we couldn't guess if it were $Z1$ or $Z2$ )

---

# Overidentification Tests

- And how different are they? What did the test see that it was comparing? (Notice the first model gives an accurate coefficient of 1)

```{r, echo = TRUE}
iv1 <- feols(Y~ 1 | X ~ Z1, data = df)
iv2 <- feols(Y~ 1 | X ~ Z2, data = df)

export_summs(iv1, iv2, statistics = c(N = 'nobs'))
```

---

# That's it!

Be sure to cover

- The Swirl
- The homework
- The assigned paper
