---
subtitle: "PB4A7- Quantitative Applications for Behavioural Science"
title: "<font style='font-size:1em;'>üóìÔ∏è Week 0<br/> Presessionals</font>"
author: Dr. [George Melios](#)
institute: '[London School of Economics and Political Science](#)'
date: 18 September 2023
date-meta: 18 September 2023
date-format: "DD MMM YYYY"
toc: true
toc-depth: 1
toc-title: "What we will cover today:"
center-title-slide: false
from: markdown+emoji
format:
  revealjs: 
    fig-responsive: true
    theme: simple
    slide-number: true
    mouse-wheel: false
    preview-links: auto
    logo: /figures/logos/MY_INSTITUTION.png
    css: /css/styles_slides.css
    footer: 'PB4A7- Quantitative Applications for Behavioural Science'
---

# Who are we

## Your lecturer {.smaller}

::: columns

::: {.column style="display:inline-block;width:40%;height:60%;border-radius:1em;margin:1%;padding:1.5%;background-color:#f5f5f5"}
<figure>
    <img src="/figures/people/George.jpeg" alt="Photo of George" role="presentation" style="object-fit: cover;width:5em;height:5em;border-radius: 50%;font-size:1em;" class="img-responsive atto_image_button_text-bottom">
    <figcaption>
        <strong>Dr.&nbsp;<span><a href="#" target="_blank" class="external">Melios</a></span></strong> 
        <br/>Research Fellow
    </figcaption>
</figure>
:::

::: {.column style="width:50%;font-size:0.85em;margin-left:3%;"}
- PhD in Economics
- **Background**: Economics, Political Science, Behavioural Science

<span class="tag" style="background-color:var(--quarto-hl-st-color)">Political Economy</span>
<br/>
<span class="tag" style="background-color:var(--quarto-hl-fu-color)">Beliefs</span>
<br/>
<span class="tag" style="background-color:var(--quarto-hl-dv-color)">Wellbeing</span>

:::

:::

# Welcome to Econometrics

## What is this class

- This is econometrics
- Econometrics is a field that covers how economists think about statistical analysis
- Many other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be

---

- So what is econometrics?
- Econometrics focuses on the study of *observational data*
- Observational data are measurements of things that *the researcher does not control*
- Given that we are working with observational data, we still want to understand the *causes of things*
- The world is what it is, we are only here to study it

---

# Welcome to Econometrics

- When is this useful?
- We can't randomly assign or experiment with things like how much education you get, or what our tax rates are, or what a stock's recent returns have been, or where you live - that would generally be impossible or cruel
- But we still want to learn about the *underlying patterns* related to those data - how does X cause Y? What is the correct model to use to understand X and Y?
- That would be simple if we could control the situation, turn things on and off, randomize stuff
- But we can't! So what now? Enter econometrics.

---

# Welcome to Econometrics

- This is a great course (if I do say so)
- When I took it, what it left me feeling was *powerful*
- It gave me the ability to answer questions I was interested in on my own
- And to understand the degree of confidence I could have in my own results and in others
- "A new study says..." you don't need to take the paper's word for that any more!

---

# Admin

- Review the syllabus (and other materials on Canvas). Reading assignments there
- Our textbook is [The Effect](https://theeffectbook.net), by me, which can be read for free.
- Also these slides
- Programming in R: free, you will want to have access to it. We will get to this next time
- Assignments: Weekly homework combining econometrics and R tasks
- There is a midterm and a final
- A data exploration project
- A group data analysis project
- And completion of Swirl modules and online discussion

---

# Causality and Prediction

- Okay so what are we doing here exactly?
- In econometrics, we are working with data
- Statisticians also work with data
- So do data scientists
- The *goals* for these groups differ considerably

---

# Causality and Prediction

- Data scientists are generally concerned with *prediction*
- They want to use the data at hand to *predict* what comes next
- They generally don't care *why* they're making the prediction they are
- This can be really handy for certain tasks - "is this picture a cat or a dog?" "what's the probability that a customer with qualities X, Y, and Z will end up purchasing our good?" "do you have lymphoma?"

---

# Causality and Prediction

- Econometricians, on the other hand, care almost exclusively about *why*
- Data scientists want to minimize *prediction error*
- Econometricians want to minimize *inference* and *identification error*
- We want to correctly understand *the underlying data generating process*

---

# Inference Error and Randomness

- One big problem we face when trying to figure out how variables relate to each other is *randomness*
- This is simply the fact that, when we gather data, we can only possibly get a subsample of *all* the data
- So, just by random chance, the relationship we get in our data might not be quite the same as the true relationship

---

# Inference Error and Randomness

- So if we look in a data set and see that $X$ and $Y$ appear to be positively related to each other...
- Are they actually positively related, or is that just random chance?
- If they are positively related, maybe we're understating or overstating *how* positively related

---

# Inference Error and Randomness

- If the true relationship is 0, then in the data we'll see a positive relationship half the time, and a negative relationship half the time
- Even though the truth is 0!
- How can we properly make an *inference* about whether the relationship is 0 or not (or positive, or negative, or *how* positive or negative), taking into account this randomness?
- That's being careful about inference. The statisticians teach us all about this!


---

# Identification Error

- What is identification error?
- *Identification* is how you link the *result you see* with the *conclusion you draw from it*
- For example, say you observe that kids who play video games are more aggressive in everyday life (result), and you conclude from that result that video games make kids more aggressive (conclusion)
- If *seeing that result is actually evidence for that conclusion*, then we are properly *identified*

---

# Identification Error

- But if there's another reason why we might see that result, i.e. if the same result could give us a different conclusion, like *kids who are aggressive play more video games* or *people notice aggression more when kids play video games*, then we have made an *identification error* - our result was not identified!
- Identification error is when your result in the data doesn't actually have a clear theoretical ("why" or "because") interpretation
- For example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts, you have committed an identification error
- One day in and all we can do is complain, eesh

---

# Data Generating Process

- To avoid identification error, economists think closely about the *data generating process*
- What is a data generating process?
- The data generating process is the *true set of laws* that determine where our data comes from
- For example, if you hold a rock and drop it, it falls to the floor
- What is the data we observe? (Hold the rock & Rock is up) and (Let go & Rock is down)
- What is the data generating process? Gravity makes the rock fall down when you drop it

---

# Data Generating Process

- Another example is a model of supply and demand
- We observe prices and quantities in a competitive market
- What led to those being the prices and quantities we see?
- The supply and demand model and its equilibrium, we theorize!

---
---
title: "Regression Discontinuity"
subtitle: "The Burpees of Causal Inference" 
date: "Updated `r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: TRUE
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    # Run xaringan::summon_remark() for this
    #chakra: libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE) 
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, fig.height = 6)
library(tidyverse)
library(gganimate)
library(estimatr)
library(magick)
library(dagitty)
library(directlabels)
library(ggdag)
library(ggthemes)
library(fixest)
library(jtools)
library(rdrobust)
library(scales)
library(Cairo)
theme_metro <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16),
        axis.title.x = element_text(hjust = 1),
        axis.title.y = element_text(hjust = 1, angle = 0))
}
theme_void_metro <- function(x) {
  theme_void() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
theme_metro_regtitle <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
```

```{css, echo=FALSE}
pre {
  max-height: 350px;
  overflow-y: auto;
}

pre[class] {
  max-height: 100px;
}
```


# Check-in

- We're thinking through ways that we can identify the effect of interest without having to control for everything
- One way is by focusing on *within variation* - if all the endogeneity can be controlled for or only varies between-individuals, we can just focus on within variation to identify it
- Pro: control for a bunch of stuff
- Con: washes out a lot of variation! Result can be noisier if there's not much within-variation to work with
- Also, this requires no endogenous variation over time
- That might be a tricky assumption! Often there are plenty of back doors that shift over time

---

# Regression Discontinuity

- Today we are going to talk about Regression discontinuity design (RDD)
- RDD is currently the darling of the econometric world for estimating causal effects without running an experiment
- It doesn't apply everywhere, but when it does, it's very easy to buy the identification assumptions
- Not that it doesn't have its own issues, of course, but it's pretty good!

---

# Regression Discontinuity

The basic idea is this:

- We look for a treatment that is assigned on the basis of being above/below a *cutoff value* of a continuous variable
- For example, if you get above a certain test score they let you into a "gifted and talented" program
- Or if you are just on one side of a time zone line, your day starts one hour earlier/later
- Or if a candidate gets 50.1% of the vote they're in, 40.9% and they're out
- Or if you're 65 years old you get Medicaid, if you're 64.99 years old you don't

We call these continuous variables "Running variables" because we *run along them* until we hit the cutoff

---

# Regression Discontinuity

- But wait, hold on, if treatment is driven by running variables, won't we have a back door going through those very same running variables?? Yes! 
- And we can't just control for RunningVar because that's where all the variation in treatment comes from. Uh oh!

```{r, dev = 'CairoPNG'}
dag <- dagify(Treatment ~ RunningVar,
              Outcome ~ Treatment + RunningVar,
              coords=list(
                x=c(Treatment = 1, RunningVar = 2, Outcome = 3),
                y=c(Treatment = 1, RunningVar = 2, Outcome = 1)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=10) + 
  theme_void_metro() + 
  expand_limits(x=c(.5,3.5))
```

---

# Regression Discontinuity

- The key here is realizing that the running variable affects treatment *only when you go across the cutoff*
- So really the diagram looks like this!

```{r, dev = 'CairoPNG'}
dag <- dagify(Treatment ~ Cutoff,
              Cutoff ~ RunningVar,
              Outcome ~ Treatment + RunningVar,
              coords=list(
                x=c(Treatment = 1, Cutoff = 1, RunningVar = 2, Outcome = 3),
                y=c(Treatment = 1, Cutoff = 2, RunningVar = 2, Outcome = 1)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=10) + 
  theme_void_metro() + 
  expand_limits(x=c(.5,3.5))
```

---

# Regression Discontinuity

- So what does this mean?
- If we can control for the running variable *everywhere except the cutoff*, then...
- We will be controlling for the running variable, closing that back door
- But leaving variation at the cutoff open, allowing for variation in treatment
- We focus on just the variation around the treatment, narrowing the range of the running variable we use so sharply that it's basically controlled for. Then the effect of cutoff on treatment is like an experiment!

---

# Regression Discontinuity

- Basically, the idea is that *right around the cutoff*, treatment is randomly assigned
- If you have a test score of 89.9 (not high enough for gifted-and-talented), you're basically the same as someone who has a test score of 90.0 (just barely high enough)
- So if we just focus around the cutoff, we close any back doors because it's basically random which side of the line you're on
- But we get variation in treatment!
- This specifically gives us the effect of treatment *for people who are right around the cutoff* a.k.a. a "local average treatment effect" (we still won't know the effect of being put in gifted-and-talented for someone who gets a 30)

---

# Regression Discontinuity

- A very basic idea of this, before we even get to regression, is to create a *binned chart* 
- And see how the bin values jump at the cutoff
- A binned chart chops the Y-axis up into bins
- Then takes the average Y value within that bin. That's it!
- Then, we look at how those X bins relate to the Y binned values. 
- If it looks like a pretty normal, continuous relationship... then JUMPS UP at the cutoff X-axis value, that tells us that the treatment itself must be doing something!

---

# Regression Discontinuity

```{r, echo=FALSE, fig.width=5, fig.height=4.5}
df <- data.frame(xaxisTime=runif(300)*20) %>%
  mutate(Y = .2*xaxisTime+3*(xaxisTime>10)-.1*xaxisTime*(xaxisTime>10)+rnorm(300),
         state="1",
         groupX=floor(xaxisTime)+.5,
         groupLine=floor(xaxisTime),
         cutLine=rep(c(9,11),150)) %>%
  group_by(groupX) %>%
  mutate(mean_Y=mean(Y)) %>%
  ungroup() %>%
  arrange(groupX)


dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(groupLine=NA,cutLine=NA,mean_Y=NA,state='1. Start with raw data.'),
  #Step 2: Add Y-lines
  df %>% mutate(cutLine=NA,state='2. Figure out how Y is explained by Running Variable.'),
  #Step 3: Collapse to means
  df %>% mutate(Y = mean_Y,state="3. Keep only what's explained by the Running Variable."),
  #Step 4: Zoom in on just the cutoff
  df %>% mutate(mean_Y = ifelse(xaxisTime > 9 & xaxisTime < 11,mean_Y,NA),Y=ifelse(xaxisTime > 9 & xaxisTime < 11,mean_Y,NA),groupLine=NA,state="4. Focus just on what happens around the cutoff."),
  #Step 5: Show the effect
  df %>% mutate(mean_Y = ifelse(xaxisTime > 9 & xaxisTime < 11,mean_Y,NA),Y=ifelse(xaxisTime > 9 & xaxisTime < 11,mean_Y,NA),groupLine=NA,state="5. The jump at the cutoff is the effect of treatment."))


p <- ggplot(dffull,aes(y=Y,x=xaxisTime))+geom_point()+
  geom_vline(aes(xintercept=10),linetype='dashed')+
  geom_point(aes(y=mean_Y,x=groupX),color="red",size=2)+
  geom_vline(aes(xintercept=groupLine))+
  geom_vline(aes(xintercept=cutLine))+
  geom_segment(aes(x=10,xend=10,
                   y=ifelse(state=='5. The jump at the cutoff is the effect of treatment.',
                            filter(df,groupLine==9)$mean_Y[1],NA),
                   yend=filter(df,groupLine==10)$mean_Y[1]),size=1.5,color='blue')+
  scale_color_colorblind()+
  theme_metro_regtitle() +
  scale_x_continuous(
    breaks = c(5, 15),
    label = c("Untreated", "Treated")
  )+xlab("Running Variable")+
  labs(title = 'The Effect of Treatment on Y using Regression Discontinuity \n{next_state}')+
  transition_states(state,transition_length=c(6,16,6,16,6),state_length=c(50,22,12,22,50),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()

animate(p,nframes=175)
```

---

# Concept Checks

- Why is it important that we look as norrowly as possible around the cutoff? What does this get us over comparing the entire treated and untreated groups?
- Can you think of an example of a treatment that is assigned at least partially on a cutoff?
- Why can't we just control for the running variable as we normally would to solve the endogeneity problem?

---

# Fitting Lines in RDD

- Looking purely just at the cutoff and making no use of the space *away* from the cutoff throws out a lot of useful information
- We know that the running variable is related to outcome, so we can probably improve our *prediction* of what the value on either side of the cutoff should be if we *use data away from the cutoff to help with prediction* than if we *just use data near the cutoff*, which is what that animation does
- We can do this with good ol' OLS.
- The bin plot we did can help us pick a functional form for the slope

---

# Fitting Lines in RDD

- To be clear, producing the line(s) below is our goal. How can we do it?
- The true model I've made is an RDD effect of .7, with a slope of 1 to the left of the cutoff and a slope of 1.5 to the right

```{r, echo = FALSE}
set.seed(2000)
df <- tibble(X = runif(1000)) %>%
  mutate(treated = X > .5) %>%
  mutate(X_centered = X - .5) %>%
  mutate(Y = X_centered + .7*treated + .5*X_centered*treated + rnorm(1000,0,.3))
  

ggplot(df, aes(x = X, y = Y, group = treated)) + 
  geom_point() + 
  geom_smooth(method = 'lm', color = 'red', se = FALSE, size = 1.5) + 
  geom_vline(aes(xintercept = .5), linetype = 'dashed') + 
  theme_metro() + 
  geom_segment(aes(x = .5, xend = .5, y = 0, yend = .73), color = 'blue', size = 2) + 
  annotate(geom = 'label', x = .5, y = .73, label = 'RDD Effect',color = 'blue', size = 16/.pt, hjust = 1.05)

```

---

# Regression in RDD

- First, we need to *transform our data*
- We need a "Treated" variable that's `TRUE` when treatment is applied - above or below the cutoff
- Then, we are going to want a bunch of things to change at the cutoff. This will be easier if the running variable is *centered around the cutoff*. So we'll turn our running variable $X$ into $X - cutoff$ and call that $XCentered$

```{r, eval = FALSE}
cutoff = .5

df <- df %>%
  mutate(treated = X >= .5,
         X_centered = X - .5)
```

---

# Varying Slope

- Typically, you will want to let the slope vary to either side
- In effect, we are fitting an entirely different regression line on each side of the cutoff
- We can do this by interacting both slope and intercept with $treated$!
- Coefficient on Treated is how the intercept jumps - that's our RDD effect. Coefficient on the interaction is how the slope changes

$$Y = \beta_0 + \beta_1Treated + \beta_2XCentered + \beta_3Treated\times XCentered + \varepsilon$$

```{r}
feols(Y ~ treated*X_centered, data = df)
```

---

# Varying Slope

(as an aside, sometimes the effect of interest is the interaction term - the change in slope! This answers the question "does the effect of $X$ on $Y$ change at the cutoff? This is called a "regression kink" design. We won't go more into it here, but it is out there!)

---

# Polynomial Terms

- We don't need to stop at linear slopes!
- Just like we brought in our knowledge of binary and interaction terms to understand the linear slope change, we can bring in polynomials too. Add a square maybe!
- Don't get too wild with cubes, quartics, etc. - polynomials tend to be at their "weirdest" near the edges, and we don't want super-weird predictions right at the cutoff. It could give us a mistaken result!
- A square term should be enough

---

# Polynomial Terms

- How do we do this? Interactions again. Take *any* regression equation...

$$Y = \beta_0 + \beta_1X + \beta_2X^2 + \varepsilon$$

- And just center the $X$ (let's call it $XC$, add on a set of the same terms multiplied by $Treated$ (don't forget $Treated$ by itself - that's $Treated$ times the interaction!)

$$Y = \beta_0 + \beta_1XC + \beta_2XC^2 + \beta_3Treated + \beta_4Treated\times XC + \beta_5Treated\times XC^2 + \varepsilon$$

- The coefficient on $Treated$ remains our "jump at the cutoff" - our RDD estimate!

```{r}
etable(feols(Y ~ X_centered*treated + I(X_centered^2)*treated, data = df))
```

---

# Concept Checks

- Would the coefficient on $Treated$ still be the regression discontinuity effect estimate if we hadn't centered $X$? Why or why not?
- Why might we want to use a polynomial term in our RDD model?
- What relationship are we assuming between the outcome variable and the running variable if we choose not to include $XCentered$ in our model at all (i.e. a "zero-order polynomial")

---

# Assumptions

- We knew there must be some assumptions lurking around here
- Some are more obvious (we should be using the correct functional form)
- Others are trickier. What are we assuming about the error term and endogeneity here?
- Specifically, we are assuming that *the only thing jumping at the cutoff is treatment*
- Sort of like parallel trends, but maybe more believable since we've narrowed in so far
- For example, if having an income below 150% of the poverty line gets you access to food stamps AND to job training, then we can't really use that cutoff to get the effect of just food stamps
- Or if the proportion of people who are self-employed jumps up just below 150% (based on *reported* income), that's a back door too!
- The only thing different about just above/just below should be treatment

---

# Graphically

```{r,  echo=FALSE, fig.width=5, fig.height=4.5}
df <- data.frame(X = runif(1000)+1,Treatment=as.factor("Untreated"),time="1") %>%
  mutate(Y = X + rnorm(1000)/6) 

cutoff <- 1.75

#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Untreated only
  df %>% mutate(time="1. If NOBODY got treatment, looks smooth."),
  #Step 2: Treated only only
  df %>% mutate(Y= Y + .25,Treatment="Treated",time="2. If EVERYBODY got treatment, looks smooth."),
  #Step 3: And the jump
  df %>% mutate(Y=ifelse(X > cutoff,Y+.25,Y),Treatment = ifelse(X>cutoff,"Treated","Untreated"),time='3. Observed! Jump only BECAUSE of treatment')
)

p <- ggplot(dffull,aes(y=Y,x=X,color=Treatment))+geom_point()+
  geom_smooth(aes(x=X,y=Y,group=Treatment),method='lm',col='red',se=FALSE)+
  geom_vline(aes(xintercept=cutoff),col='black',linetype='dashed',size=1.5)+
  scale_color_colorblind()+
  theme_metro_regtitle() +
  labs(title = 'Checking for Smoothness in Potential Outcomes at the Cutoff \n{next_state}',
       x="Test Score (X)",
       y="Outcome (Y)")+
  transition_states(time,transition_length=c(12, 12, 12),state_length=c(100,100,100),wrap=FALSE)+
  ease_aes('linear')+
  exit_fade()+enter_fade()

animate(p,nframes=160)
```

---

# Other Difficulties

More assumptions, limitations, and diagnostics!

- Windows
- Granular running variables
- Manipulated running variables
- Fuzzy regression discontinuity

---

# Windows

- The basic idea of RDD is that we're interested in *the cutoff*
- The points away from the cutoff are only useful in helping us predict values at the cutoff
- Do we really want that full range? Is someone's test score of 30 really going to help us much in predicting $Y$ at a test score of 89?
- So we might limit our analysis within just a narrow window around the cutoff, just like that initial animation we saw!
- This makes the exogenous-at-the-jump assumption more plausible, and lets us worry less about functional form (over a narrow range, not too much difference between a linear term and a square), but on the flip side reduces our sample size considerably

---

# Windows

- Pay attention to the sample sizes, accuracy (true value .7) and standard errors!

```{r, echo=FALSE}
set.seed(2000)
df <- tibble(X = runif(1000)) %>%
  mutate(treated = X > .5) %>%
  mutate(X_centered = X - .5) %>%
  mutate(Y = X_centered + .7*treated + .5*X_centered*treated + rnorm(1000,0,.3))
```

```{r, echo = TRUE}
m1 <- feols(Y~treated*X_centered, data = df)
m2 <- feols(Y~treated*X_centered, data = df %>% filter(abs(X_centered) < .25))
m3 <- feols(Y~treated*X_centered, data = df %>% filter(abs(X_centered) < .1))
m4 <- feols(Y~treated*X_centered, data = df %>% filter(abs(X_centered) < .05))
m5 <- feols(Y~treated*X_centered, data = df %>% filter(abs(X_centered) < .01))
etable(m1,m2,m3,m4,m5, keep = 'treatedTRUE')
```

---

# Granular Running Variable

- One assumption we're making is that the running variable varies more or less *continuously*
- That makes it possible to have, say, a test score of 89 compared to a test score of 90 it's almost certainly the same as except for random chance
- But what if our data only had test score in big chunks? I don't know you're 89 or 90, I just know you're "80-89" or "90-100"
- A lot less believable that the only difference between these groups is random chance and we've closed the back doors by focusing on the cutoff
- Plenty of other things change between 80 and 100! That's not "smooth at the cutoff"

---

# Granular Running Variable

- Not a whole lot we can do about this
- There are some fancy RDD estimators that allow for granular running variables
- But in general, if this is what you're facing, you might be in trouble
- Before doing an RDD, think "is it plausible that someone with the highest value just below the cutoff, and someone with the lowest value just above the cutoff are only at different values because of random chance?"

---

# Looking for Lumping

- Ok, now let's go back to our continuous running variables
- What if the running variable is *manipulated*?
- Imagine you're a teacher grading the gifted-and-talented exam. You see someone with an 89 and think "aww, they're so close! I'll just give them an extra point..."
- Or, if you live just barely on one side of a time zone line, but decide to move to the other side because you prefer waking up later
- Suddenly, that treatment is a lot less randomly assigned around the cutoff!

---

# Looking for Lumping

- If there's manipulation of the running variable around the cutoff, we can often see it in the presence of *lumping*
- I.e. if there's a big cluster of observations to one side of the cutoff and a seeming gap missing on the other side

---

# Looking for Lumping

- Here's an example from the real world in medical research - statistically, p-values *should* be uniformly distributed
- But it's hard to get insignificant results published in some journals. So people might "p-hack" until they find some form of analysis that's significant, and also we have heavy selection into publication based on $p < .05$. Can't use that cutoff for an RDD!


![p-value graph from Perneger & Combescure, 2017](p_value_distribution.png)

---

# Looking for Lumping

- How can we look for this stuff?
- We can look graphically by just checking for a jump at the cutoff in *number of observations* after binning

```{r, echo = TRUE}
df_bin_count <- df %>%
  # Select breaks so that one of hte breakpoints is the cutoff
  mutate(X_bins = cut(X, breaks = 0:10/10)) %>%
  group_by(X_bins) %>%
  count()
```

---

# Looking for Lumping

- The first one looks pretty good. We have one that looks not-so-good on the right

```{r, echo = FALSE}
bad_bins <- df_bin_count 
bad_bins$n <- sample(df_bin_count$n, 10)
bad_bins$n[5] <- 20
bad_bins$n[6] <- 160
bad_bins$Type <- 'Bad'

df_bin_count %>%
  mutate(Type = 'Good') %>%
  bind_rows(bad_bins) %>%
  mutate(Type = factor(Type, levels = c('Good','Bad'))) %>%
  group_by(Type) %>%
  mutate(n = n/sum(n)) %>%
  ggplot(aes(x = X_bins, y = n, fill = Type)) + 
  guides(fill = FALSE) + 
  geom_col() + 
  theme_metro() +
  theme(axis.text.x = element_text(angle = 90)) + 
  labs(y = 'Percent', x = "X") + 
  geom_vline(aes(xintercept = 5.5), linetype = 'dashed') +
  scale_y_continuous(labels = scales::percent, limits = c(0,.2)) +
  facet_wrap('Type')
```


---

# Looking for Lumping

- Another thing we can do is do a "placebo test"
- Check if variables *other than treatment or outcome* vary at the cutoff
- We can do this by re-running our RDD but just swapping out some other variable for our outcome
- If we get a significant jump, that's bad! That tells us that *other things are changing at the cutoff* which implies some sort of manipulation (or just super lousy luck)

---

# Fuzzy Regression Discontinuity

- So far, we've assumed that you're either on one side of the cutoff and untreated, or the other and treated
- What if it isn't so simple? What if the cutoff just *increases* your chances of treatment?
- For example, maybe about 10% of kids with too-low test scores get into gifted-and-talented, and 80% of kids with high-enough scores do
- For whatever reason!
- This is a "fuzzy regression discontinuity"
- Now, our RDD will understate the true effect, since it's being calculated on the assumption that we added treatment to 100% of people at the cutoff, when really it's 70%. So we'll get roughly only about 70% of the effect

---

# Fuzzy Regression Discontinuity

- We can account for this with a model designed to take this into account
- Specifically, we can use something called two-stage least squares (instrumental variables) to handle these sorts of situations
- (you can go see the instrumental variables module if you like for more detail)
- Basically, two-stage least squares estimates how much the chances of treatment go up at the cutoff, and scales the estimate by that change
- So it would take whatever result we got on the previous slide and divide it by .7 to get the true effect

---

# Fuzzy Regression Discontinuity

- Notice that the y-axis here isn't the outcome, it's "percentage treated"

```{r, echo = FALSE}
set.seed(1000)
df <- tibble(X = runif(1000)) %>%
  mutate(treatassign = .05 + .3*(X > .5)) %>%
  mutate(rand = runif(1000)) %>%
  mutate(treatment = treatassign > rand) %>%
  mutate(Y = .2 + .4*X + .5*treatment + rnorm(1000)) %>%
  mutate(X_center = X - .5) %>%
  mutate(above_cut = X > .5)
df %>%
  mutate(X_bins = cut(X, breaks = 0:10/10)) %>%
  group_by(X_bins) %>%
  summarize(n = mean(treatment)) %>%
  ggplot(aes(x = X_bins, y = n)) + 
  geom_col() + 
  labs(x = "X", y = "Proportion Treated") + 
  theme_metro_regtitle() + 
  theme(axis.text.x = element_text(angle = 90)) + 
  geom_vline(aes(xintercept = 5.5), linetype = 'dashed')
```

---

# Fuzzy Regression Discontinuity

- We can perform this using the instrumental-variables features of `feols`, giving it two treatment-response functions
- The first is an RDD specification where we use "treatment" - i.e. whether you were actually treated
- The second uses the same RDD specification, but replaces "treatment" with "above the cutoff"
- `feols(outcome ~ controls  | XC*treated ~ XC*assigned_to_treatment)`

---

# Fuzzy Regression Discontinuity

- (the true effect of treatment is .4 - okay, it's not perfect)

```{r, echo = TRUE}
predict_treatment <- feols(treatment ~ X_center*above_cut, data = df)
without_fuzzy <-feols(Y ~ X_center*treatment, data = df)
fuzzy_rdd <- feols(Y ~ 1 | X_center*treatment ~ X_center*above_cut, data = df)
etable(predict_treatment, without_fuzzy, fuzzy_rdd)
```
---

# Concept Checks

- Why does using a narrow window make the effect estimate noisier?
- How do we know that, if our treatment variable is fuzzily assigned, we will *underestimate* the effect if we just run a regular RDD, rather than overestimate it?
- Inuitively, why would we be skeptical that a regression discontinuity run on a very granular running variable is valid?

---

# Regression Discontinuity in R

- We've gone through all kinds of procedures for doing RDD in R already using regression
- But often, professional researchers won't do it that way!
- We'll use packages and formulas that do things like "picking a bandwidth (window)" for us in a smart way, or not relying so strongly on linearity
- The **rdrobust** package does just that!
- Let's look at `help(rdrobust, packge = 'rdrobust')`

---

# Regression Discontinuity in R

- We can specify an RDD model by just telling it the dependent variable $Y$, the running variable $X$, and the cutoff $c$.
- We can also specify how many polynomials to us with `p`
- (it applies the polynomials more locally than our linear OLS models do - a bit more flexible without weird corner preditions)
- It will also pick a window for us with `h`
- Plenty of other options
- Including a `fuzzy` option to specify actual treatment outside of the running variable/cutoff combo

---

# rdrobust

```{r, echo = TRUE}
summary(rdrobust(df$Y, df$X, c = .5))
```

---

# rdrobust

```{r, echo = TRUE}
summary(rdrobust(df$Y, df$X, c = .5, fuzzy = df$treatment))
```

---

# rdrobust

- We can even have it automatically make plots of our RDD! Same syntax

```{r, echo = TRUE}
rdplot(df$Y, df$X, c = .5)
```

---

# That's it!

- That's what we have for RDD
- Go explore the regression discontinuity Swirl
- And the homework
- And the paper to read!
