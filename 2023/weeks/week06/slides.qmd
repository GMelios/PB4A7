---
subtitle: "PB4A7- Quantitative Applications for Behavioural Science"
title: "<font style='font-size:1em;'>üóìÔ∏è Week 0<br/> Presessionals</font>"
author: Dr. [George Melios](#)
institute: '[London School of Economics and Political Science](#)'
date: 18 September 2023
date-meta: 18 September 2023
date-format: "DD MMM YYYY"
toc: true
toc-depth: 1
toc-title: "What we will cover today:"
center-title-slide: false
from: markdown+emoji
format:
  revealjs: 
    fig-responsive: true
    theme: simple
    slide-number: true
    mouse-wheel: false
    preview-links: auto
    logo: /figures/logos/MY_INSTITUTION.png
    css: /css/styles_slides.css
    footer: 'PB4A7- Quantitative Applications for Behavioural Science'
---

# Who are we

## Your lecturer {.smaller}

::: columns

::: {.column style="display:inline-block;width:40%;height:60%;border-radius:1em;margin:1%;padding:1.5%;background-color:#f5f5f5"}
<figure>
    <img src="/figures/people/George.jpeg" alt="Photo of George" role="presentation" style="object-fit: cover;width:5em;height:5em;border-radius: 50%;font-size:1em;" class="img-responsive atto_image_button_text-bottom">
    <figcaption>
        <strong>Dr.&nbsp;<span><a href="#" target="_blank" class="external">Melios</a></span></strong> 
        <br/>Research Fellow
    </figcaption>
</figure>
:::

::: {.column style="width:50%;font-size:0.85em;margin-left:3%;"}
- PhD in Economics
- **Background**: Economics, Political Science, Behavioural Science

<span class="tag" style="background-color:var(--quarto-hl-st-color)">Political Economy</span>
<br/>
<span class="tag" style="background-color:var(--quarto-hl-fu-color)">Beliefs</span>
<br/>
<span class="tag" style="background-color:var(--quarto-hl-dv-color)">Wellbeing</span>

:::

:::

# Welcome to Econometrics

## What is this class

- This is econometrics
- Econometrics is a field that covers how economists think about statistical analysis
- Many other social science fields (and fields like epidemiology) have picked up econometric tools as well becuase of how useful they tend to be

---

- So what is econometrics?
- Econometrics focuses on the study of *observational data*
- Observational data are measurements of things that *the researcher does not control*
- Given that we are working with observational data, we still want to understand the *causes of things*
- The world is what it is, we are only here to study it

---

# Welcome to Econometrics

- When is this useful?
- We can't randomly assign or experiment with things like how much education you get, or what our tax rates are, or what a stock's recent returns have been, or where you live - that would generally be impossible or cruel
- But we still want to learn about the *underlying patterns* related to those data - how does X cause Y? What is the correct model to use to understand X and Y?
- That would be simple if we could control the situation, turn things on and off, randomize stuff
- But we can't! So what now? Enter econometrics.

---

# Welcome to Econometrics

- This is a great course (if I do say so)
- When I took it, what it left me feeling was *powerful*
- It gave me the ability to answer questions I was interested in on my own
- And to understand the degree of confidence I could have in my own results and in others
- "A new study says..." you don't need to take the paper's word for that any more!

---

# Admin

- Review the syllabus (and other materials on Canvas). Reading assignments there
- Our textbook is [The Effect](https://theeffectbook.net), by me, which can be read for free.
- Also these slides
- Programming in R: free, you will want to have access to it. We will get to this next time
- Assignments: Weekly homework combining econometrics and R tasks
- There is a midterm and a final
- A data exploration project
- A group data analysis project
- And completion of Swirl modules and online discussion

---

# Causality and Prediction

- Okay so what are we doing here exactly?
- In econometrics, we are working with data
- Statisticians also work with data
- So do data scientists
- The *goals* for these groups differ considerably

---

# Causality and Prediction

- Data scientists are generally concerned with *prediction*
- They want to use the data at hand to *predict* what comes next
- They generally don't care *why* they're making the prediction they are
- This can be really handy for certain tasks - "is this picture a cat or a dog?" "what's the probability that a customer with qualities X, Y, and Z will end up purchasing our good?" "do you have lymphoma?"

---

# Causality and Prediction

- Econometricians, on the other hand, care almost exclusively about *why*
- Data scientists want to minimize *prediction error*
- Econometricians want to minimize *inference* and *identification error*
- We want to correctly understand *the underlying data generating process*

---

# Inference Error and Randomness

- One big problem we face when trying to figure out how variables relate to each other is *randomness*
- This is simply the fact that, when we gather data, we can only possibly get a subsample of *all* the data
- So, just by random chance, the relationship we get in our data might not be quite the same as the true relationship

---

# Inference Error and Randomness

- So if we look in a data set and see that $X$ and $Y$ appear to be positively related to each other...
- Are they actually positively related, or is that just random chance?
- If they are positively related, maybe we're understating or overstating *how* positively related

---

# Inference Error and Randomness

- If the true relationship is 0, then in the data we'll see a positive relationship half the time, and a negative relationship half the time
- Even though the truth is 0!
- How can we properly make an *inference* about whether the relationship is 0 or not (or positive, or negative, or *how* positive or negative), taking into account this randomness?
- That's being careful about inference. The statisticians teach us all about this!


---

# Identification Error

- What is identification error?
- *Identification* is how you link the *result you see* with the *conclusion you draw from it*
- For example, say you observe that kids who play video games are more aggressive in everyday life (result), and you conclude from that result that video games make kids more aggressive (conclusion)
- If *seeing that result is actually evidence for that conclusion*, then we are properly *identified*

---

# Identification Error

- But if there's another reason why we might see that result, i.e. if the same result could give us a different conclusion, like *kids who are aggressive play more video games* or *people notice aggression more when kids play video games*, then we have made an *identification error* - our result was not identified!
- Identification error is when your result in the data doesn't actually have a clear theoretical ("why" or "because") interpretation
- For example, if you observe that people tend to wear more shorts on days they eat more ice cream, and you conclude that eating ice cream makes you put on shorts, you have committed an identification error
- One day in and all we can do is complain, eesh

---

# Data Generating Process

- To avoid identification error, economists think closely about the *data generating process*
- What is a data generating process?
- The data generating process is the *true set of laws* that determine where our data comes from
- For example, if you hold a rock and drop it, it falls to the floor
- What is the data we observe? (Hold the rock & Rock is up) and (Let go & Rock is down)
- What is the data generating process? Gravity makes the rock fall down when you drop it

---

# Data Generating Process

- Another example is a model of supply and demand
- We observe prices and quantities in a competitive market
- What led to those being the prices and quantities we see?
- The supply and demand model and its equilibrium, we theorize!

---

---
title: "Within Variation and Fixed Effects"
subtitle: "i.e. one thing to do when measurement eludes you"
date: "Updated `r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: TRUE
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    # Run xaringan::summon_remark() for this
    #chakra: libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE) 
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, fig.height = 6)
library(tidyverse)
library(gganimate)
library(fixest)
library(magick)
library(wooldridge)
library(dagitty)
library(ggdag)
library(scales)
library(Cairo)
theme_metro <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16),
        axis.title.x = element_text(hjust = 1),
        axis.title.y = element_text(hjust = 1, angle = 0))
}
theme_void_metro <- function(x) {
  theme_void() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
theme_metro_regtitle <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
```

# Check-in

- So far we've been learning about how to set up, run, and interpret an ordinary least squares regression
- This is a key skill for anyone doing anything with data - even if you never run a regular ol' linear regression again, pretty much everything else in applied stats builds off of it in some way
- Another thing we've been doing is thinking about how to design and add controls to that regression to *identify* our effect of interest by closing back doors

---

# The Measurement Problem...

- And this has led us to some issues that have already popped up!
- For this approach to work, we need to not only *figure out* what we need to control for, using our diagram, but we need to *actually control for it*
- A lot of the time we don't have that data!
- And thus all the skeptical comments we had about the designs we came up with

---

# A Pickle

- So obviously this is a problem, and it's not one we can reason or trick our way out of
- If we don't have the variable we need to control for, we don't have it
- ... or do we?

---

# The Rest of the Term

- Much of the rest of the term is going to be focused on *finding ways to control for stuff that we can't measure*
- Seems impossible! But it is possible, at least in some circumstances
- Today, we will be talking about *within variation* and *between variation*, and the ability to control for all *between variation* using *fixed effects*

---

# Panel Data

- We are working now in the domain of *panel data*
- Panel data is when you observe the same individual over multiple time periods
- "Individual" could be a person, or a company, or a state, or a country, etc. There are $N$ individuals in the panel data
- "Time period" could be a year, a month, a day, etc.. There are $T$ time periods in the data
- For now we'll assume we observe each individual the same number of times, i.e. a *balanced* panel (so we have $N\times T$ observations)
- You can use this stuff with unbalanced panels too, it just gets a little more complex

---

# Panel Data

- Here's what (a few rows from) a panel data set looks like - a variable for individual (county), a variable for time (year), and then the data

```{r, dev = 'CairoPNG'}
data(crime4)
crime4 %>%
  select(county, year, crmrte, prbarr) %>%
  rename(County = county,
         Year = year,
         CrimeRate = crmrte,
         ProbofArrest = prbarr) %>%
  slice(1:9) %>%
  knitr::kable(note = '...') %>%
  kableExtra::add_footnote('9 rows out of 630. "Prob. of Arrest" is estimated probability of being arrested when you commit a crime', notation = 'none')
```

---

# Between and Within

- Let's pick a few counties and graph this out

```{r, dev = 'CairoPNG'}
crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  )) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt) + 
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate',
       caption = 'One outlier eliminated in County 7.') + 
  #scale_x_continuous(limits = c(.15, 2.5)) + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple'))
```

---

# Between and Within

- If we look at the overall variation, just pretending this is all together, we get this

```{r, dev = 'CairoPNG'}
crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  )) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt) + 
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate',
       caption = 'One outlier eliminated in County 7.') + 
  #scale_x_continuous(limits = c(.15, 2.5)) + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple')) + 
  geom_smooth(method = 'lm', aes(color = NULL, label = NULL), se = FALSE)
```


---


# Between and Within

- BETWEEN variation is what we get if we look at the relationship between the *means of each county*

```{r, dev = 'CairoPNG'}
crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  ),
  mcrm = mean(crmrte),
  mpr = mean(prbarr)) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt) + 
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate',
       caption = 'One outlier eliminated in County 7.') + 
  #scale_x_continuous(limits = c(.15, 2.5)) + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple')) + 
  geom_point(aes(x = mpr, y = mcrm), size = 20, shape = 3, color = 'darkorange') + 
  annotate(geom = 'text', x = .3, y = .02, label = 'Means Within Each County', color = 'darkorange', size = 14/.pt)
```

---

# Between and Within

- And I mean it! Only look at those means! The individual year-to-year variation within county doesn't matter.

```{r, dev = 'CairoPNG'}
crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  ),
  mcrm = mean(crmrte),
  mpr = mean(prbarr)) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  #geom_point() + 
  #geom_text(hjust = -.1, size = 14/.pt) + 
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate',
       caption = 'One outlier eliminated in County 7.') + 
  #scale_x_continuous(limits = c(.15, 2.5)) + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple')) + 
  geom_point(aes(x = mpr, y = mcrm), size = 20, shape = 3, color = 'darkorange') + 
  geom_smooth(aes(color = NULL), method = 'lm', se = FALSE)+
  annotate(geom = 'text', x = .3, y = .02, label = 'OLS Fit on These Four Points', color = 'blue', size = 14/.pt)
```

---

# Between and Within

- Within variation goes the other way - it treats those orange crosses as their own individualized sets of axes and looks at variation *within* county from year-to-year only!
- We basically slide the crosses over on top of each other and then analyze *that* data

```{r, echo=FALSE, fig.width=5, fig.height=4.5}
cranim <- crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  mutate(allcrm = mean(crmrte),
         allmpr = mean(prbarr)) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  ),
  mcrm = mean(crmrte),
  mpr = mean(prbarr),
  stage = '1. Raw Data')
cranim <- cranim %>%
  bind_rows(cranim %>% 
              mutate(crmrte = crmrte - mcrm + allcrm,
                     prbarr = prbarr - mpr + allmpr,
                     mcrm = allcrm,
                     mpr = allmpr,
                     stage = '2. Remove all between variation'))

p <- ggplot(cranim, aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt)  + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate',
       caption = 'One outlier eliminated in County 7.') + 
  #scale_x_continuous(limits = c(.15, 2.5)) + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple')) + 
  geom_smooth(aes(color = NULL), method = 'lm', se = FALSE)+
  geom_point(aes(x = mpr, y = mcrm), size = 20, shape = 3, color = 'darkorange') + 
  transition_states(stage) + 
  theme_metro_regtitle()

animate(p, nframes = 80)

```

---

# Between and Within

- We can clearly see that *between counties* there's a strong positive relationship
- But if you look *within* a given county, the relationship isn't that strong, and actually seems to be negative
- Which would make sense - if you think your chances of getting arrested are high, that should be a deterrent to crime
- But what are we actually doing here? Let's think about the causal diagram / data-generating process!
- What goes into the probability of arrest and the crime rate? Lots of stuff!

---

# The Crime Rate

- "LocalStuff" is just all the things unique to that area
- "LawAndOrder" is how committed local politicians are to "Law and Order Politics"

```{r, dev = 'CairoPNG'}
dag <- dagify(CrimeRate ~ ProbArrest + Poverty + CivilRights + LocalStuff,
              ProbArrest ~ Police + LawAndOrder + CivilRights + Poverty + LocalStuff,
              Police ~ LawAndOrder,
              CivilRights ~ LawAndOrder,
              Poverty ~ LocalStuff,
              coords=list(
                x=c(ProbArrest = 1, CrimeRate = 4, Poverty = 2.5, Police = 1.5, LawAndOrder = 1, CivilRights = 2.5, LocalStuff = 4),
                y=c(ProbArrest = 1, CrimeRate = 1, Poverty = 2, Police = 1.5, LawAndOrder = 2, CivilRights = 1.25, LocalStuff = 2)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=10) + 
  theme_void_metro() + 
  expand_limits(x=c(.5,4.5))

```


---

# Between and Within

- For each of these variables we can ask if they vary *between groups* and/or *within groups*
- LocalStuff is all the stuff unique to that county - geography, landmarks, the quality of the schools, almost by definition this only varies *between groups*. It's not like the things that make your county unique are different each year (or at least not very different)
- Whether the county has LawAndOrder and how many CivilRights you're allowed might change a bit year to year, but in general, political climates like that change pretty slowly. At a bit of a stretch we can call that something that only varies between groups too
- Police budgets (and thus number of police on the streets) and Poverty (which varies with the economy) vary both between counties, but also *within* counties from year to year
- Variables with between variation only (by our assumption): LocalStuff, LawAndOrder, CivilRights
- Variables with both between and within variation: Police, Poverty

---

# Between and Within

- Let's simplify our graph!
- Some of the variables only vary *between counties*
- So, we can replace those variables on the graph with the variable County
- Right? That's where all the variation is anyway

---

# The Crime Rate

- "LocalStuff" is just all the things unique to that area
- "LawAndOrder" is how committed local politicians are to "Law and Order Politics"

```{r, dev = 'CairoPNG'}
dag <- dagify(CrimeRate ~ ProbArrest + Poverty + County,
              ProbArrest ~ Police + County + Poverty,
              Police ~ County,
              Poverty ~ County,
              coords=list(
                x=c(ProbArrest = 1, CrimeRate = 4, Poverty = 1, Police = 1.5, County = 3),
                y=c(ProbArrest = 1, CrimeRate = 1, Poverty = 2, Police = 1.5, County = 2)
              )) %>% tidy_dagitty()
ggdag_classic(dag,node_size=10) + 
  theme_void_metro() + 
  expand_limits(x=c(.5,4.5))

```

---

# Between and Within

- Now the task of identifying ProbArrest $\rightarrow$ CrimeRate becomes much simpler!
- If we control for County, that will close a lot of back doors for us
- (based on the diagram, all we need to control for is County and Poverty!)
- Conveniently, we can control for County just like it was any other variable!
- And when we do, we automatically *control for all variables that only have between variation*, whatever they are, even if we can't measure them directly or didn't think about them
- *All that's left is the within variation*


---

# Concept Checks

- For each of these variables, would we expect them to have within variation, between variation, or both?
- (Individual = person) How a child's height changes as they age.
- (Individual = person) In a data set tracking many people over many years, the variation in the number of children a person has in a given year.
- (Individual = city) Overall, Paris, France has more restaurants than Paris, Texas.
- (Individual = genre) The average pop music album sells more copies than the average jazz album
- (Individual = genre) Miles Davis' *Kind of Blue* sold very well *for a jazz album*.
- (Individual = genre) Michael Jackson's *Thriller*, a pop album, sold many more copies than *Kind of Blue*, a jazz album.

---

# Removing Between Variation

- Okay so that's the concept
- Remove all the between variation so that all that's left is within variation
- And in the process control for any variables that are made up only of between variation
- How can we actually do this? And what's really going on?
- Let's first talk about the regression model itself that this implies
- Then let's actually do the thing. There are two main ways: *de-meaning* and *binary variables* (they give the same result, for balanced panels anyway)

---

# Estimation vs. Design

- To be clear, this is *exactly 0% different* from what we've done before in terms of controlling for stuff
- And in fact we're about to do the *exact same thing we did before* by just adding a categorical control variable for `county` or whatever
- (and in fact the "within" thing holds with other categorical controls - a categorical control for education isolates variation "within education levels")
- The difference is the *reason we're doing it*. It's fixed effects because *a categorical control for individual controls for a lot of stuff*, and we think closes a *lot* of back doors for us, not just one, and not just ones we can measure


---


# The Model


The $it$ subscript says this variable varies over individual $i$ and time $t$

$$Y_{it} = \beta_0 + \beta_1 X_{it} + \varepsilon_{it}$$

- What if there are individual-level components in the error term causing omitted variable bias? 
- $X_{it}$ is related to LocalStuff which is not in the model and thus in the error term!
- Regular ol' omitted variable bias. If we don't adjust for the individual effect, we get a biased $\hat{\beta}_1$ 
- (this bias is called "pooling bias" although it's really just a form of omitted variable bias)
- We really have this then:

$$Y_{it} = \beta_0 + \beta_1 X_{it} + (\alpha_i + \varepsilon_{it})$$

---

# De-meaning

- Let's do de-meaning first, since it's most closely and obviously related to the "removing between variation" explanation we've been going for
- The process here is simple!

1. For each variable $X_{it}$, $Y_{it}$, etc., get the mean value of that variable for each individual $\bar{X}_i, \bar{Y}_i$
2. Subtract out that mean to get residuals $(X_{it} - \bar{X}_i), (Y_{it} - \bar{Y}_i)$
3. Work with those residuals

- That's it!

---

# How does this work?

- That $\alpha_i$ term gets absorbed
- The residuals are, by construction, no longer related to the $\alpha_i$, so it no longer goes in the residuals!

$$(Y_{it} - \bar{Y}_i) = \beta_0 + \beta_1(X_{it} - \bar{X}_i) + \varepsilon_{it}$$

---

# Let's do it!

- We can use `group_by` to get means-within-groups and subtract them out

```{r, echo = TRUE}
data(crime4, package = 'wooldridge')
crime4 <- crime4 %>%
  # Filter to the data points from our graph
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(mean_crime = mean(crmrte),
         mean_prob = mean(prbarr)) %>%
  mutate(demeaned_crime = crmrte - mean_crime,
         demeaned_prob = prbarr - mean_prob)
```

---

# And Regress!

```{r, echo = TRUE}
orig_data <- feols(crmrte ~ prbarr, data = crime4)
de_mean <- feols(demeaned_crime ~ demeaned_prob, data = crime4)
etable(orig_data, de_mean)
```

---

# Interpreting a Within Relationship

- How can we interpret that slope of `-0.03`?
- This is all *within variation* so our interpretation must be *within-county*
- So, "comparing a county in year A where its arrest probability is 1 (100 percentage points) higher than it is in year B, we expect the number of crimes per person to drop by .03"
- Or if we think we've causally identified it (and want to work on a more realistic scale), "raising the arrest probability by 1 percentage point in a county reduces the number of crimes per person in that county by .0003".
- We're basically "controlling for county" (and will do that explicitly in a moment)
- So your interpretation should think of it in that way - *holding county constant* i.e. *comparing two observations with the same value of county* i.e. *comparing a county to itself at a different point in time*

---

# Concept Checks

- Why does subtracting the within-individual mean of each variable "control for individual"?
- In a sentence, interpret the slope coefficient in the estimated model $(Y_{it} - \bar{Y}_i) = 2 + 3(X_{it} - \bar{X}_i)$ where $Y$ is "blood pressure", $X$ is "stress at work", and $i$ is an individual person

---

# The Least Squares Dummy Variable Approach

- De-meaning the data isn't the only way to do it!
- You can also use the least squares dummy variable (another word for "binary variable") method
- We just treat "individual" like the categorical variable it is and add it as a control! Again, the regression approach is exactly the same as with any categorical control, but the *research design* reason for doing it is different

---

# Let's do it!

```{r, echo = TRUE}
lsdv <- feols(crmrte ~ prbarr + factor(county), data = crime4)
etable(orig_data, de_mean, lsdv, keep = c('prbarr', 'demeaned_prob'))
```

---

# The same!

- The result is the same, as it should be
- Except for that $R^2$ - What is that "within R2"?
- Because de-meaning takes out the part explained by the fixed effects ( $\alpha_i$ ) *before* running the regression, while LSDV does it *in* the regression
- So the .94 is the portion of `crmrte` explained by `prbarr` *and* `county`, whereas the .21 is the "within - $R^2$ " - the portion of *the within variation* that's explained by `prbarr`
- Neither is wrong (and the .94 isn't "better"), they're just measuring different things

---

# Why LSDV?

- A benefit of the LSDV approach is that it calculates the fixed effects $\alpha_i$ for you
- We left those out of the table with the `coefs` argument of `export_summs` (we rarely want them) but here they are:

```{r}
lsdv
```

- Interpretation is exactly the same as with a categorical variable - we have an omitted county, and these show the difference relative to that omitted county

---

# Why LSDV?

- This also makes clear another element of what's happening! Just like with a categorical var, the line is moving *up and down* to meet the counties
- Graphically, de-meaning moves all the points together in the middle to draw a line, while LSDV moves the line up and down to meet the points

```{r, dev = 'CairoPNG'}
crime4 %>%
  ungroup() %>%
  mutate(pred = predict(lsdv)) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  )) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt) + 
  geom_line(aes(y = pred, group = county), color = 'blue') +
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate',
       caption = 'One outlier eliminated in County 7.') + 
  #scale_x_continuous(limits = c(.15, 2.5)) + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple'))
```

---

# Why Not LSDV?

- LSDV is computationally expensive
- If there are a lot of individuals, or big data, or if you have many sets of fixed effects (yes you can do more than just individual - we'll get to that next time!), it can be very slow
- Most professionally made fixed-effects commands use de-meaning, but then adjust the standard errors properly
- (They also leave the fixed effects coefficients off the regression table by default)

---

# Going Professional

- Applied researchers rarely do either of these, and rather will use a command specifically designed for fixed effects
- Like good ol' `feols()`! (what did you think the "fe" part stood for?)
- Note there are also functions in **fixest** that do fixed effects in non-linear models like logit, probit, or poisson regression (`feglm()` and `fepois()`)
- Plus, it clusters the standard errors by the first fixed effect by default, which we usually want!

---

# Going Professional

```{r, echo = TRUE}
library(fixest)
pro <- feols(crmrte ~ prbarr | county, data = crime4)
etable(de_mean, pro)
```

---

# Limits to Fixed Effects

- Okay! At this point we have the concept behind fixed effects, can execute them, and know what they're good for
- What aren't they good for?

1. They don't control for anything that has within variation
2. They control away *everything* that's between-only, so we can't see the effect of anything that's between-only ("effect of geography on crime rate?" Nope!)
3. Anything with only a *little* within variation will have most of its variation washed out too ("effect of population density on crime rate?" probably not)
4. The estimate pays the most attention to individuals with *lots of variation in treatment*

- 2 and 3 can be addressed by using "random effects" instead but we aren't covering that in this class (see the The Effect chapter on Fixed Effects for more)

---

# Concept Checks

- Why can't we use individual-person fixed effects to study the impact of race on traffic stops?
- The within $R^2$ from is .3, and the overall $R^2$ is .5. Interpret these two numbers in sentences
- In a sentence, interpret the slope coefficient in the estimated model $(Y_{it} - \bar{Y}_i) = 1 + .5(X_{it} - \bar{X}_i)$ where $Y$ is "school funding per child" and $X$ is "population growth", and $i$ is city

---

# Swirl

- Open up the Fixed Effects Swirl and let's do it!

